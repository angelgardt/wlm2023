[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "L // WLM 2023",
    "section": "",
    "text": "Вступление\nКнига содержит конспекты лекций курса WLM 2023.",
    "crumbs": [
      "Вступление"
    ]
  },
  {
    "objectID": "l1.html",
    "href": "l1.html",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "",
    "text": "1.1 Установка R и RStudio\nЧтобы стать счастливым пользователем R, надо установить на свой комп две программы:\nПричем во избежание возможных проблем, надо поставить программы именно в этом порядке — сначала R, а потом RStudio, иначе IDE может на найти R и будет ругаться.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#установка-r-и-rstudio",
    "href": "l1.html#установка-r-и-rstudio",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "",
    "text": "собственно R\n\nна Win\nна Mac\nна Linux\n\nIDE RStudio1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#r-как-язык-программирования.-команды",
    "href": "l1.html#r-как-язык-программирования.-команды",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.2 R как язык программирования. Команды",
    "text": "1.2 R как язык программирования. Команды\nВ R нет команд и практически нет ключевых слов, распространенных в других языках программирования. Практически все операции обернуты в функции. Остаются только математические и логические операторы, которые, на самом деле, тоже функции.\n\n1.2.1 Математические операции\nВсе в наличии:\n\n2 + 3 # сложение\n\n[1] 5\n\n4 - 1 # вычитание\n\n[1] 3\n\n5 * 12 # умножение\n\n[1] 60\n\n5 ^ 8 # возведение в степень\n\n[1] 390625\n\n4 / 7 # деление\n\n[1] 0.5714286\n\n5 %/% 3 # целочисленное деление\n\n[1] 1\n\n5 %% 3 # остаток от деления\n\n[1] 2\n\n\nСкобки также существуют и привычно работают:\n\n6 / 3 + 2 * 4\n\n[1] 10\n\n6 / (3 + 2) * 4\n\n[1] 4.8\n\n6 / ((3 + 2) * 4)\n\n[1] 0.3\n\n\n\n\n1.2.2 Математические функции\nМожно посчитать корень:\n\nsqrt(16)\n\n[1] 4\n\n\nИли логарифм:\n\nlog(10)\n\n[1] 2.302585\n\nlog(8, base = 2)\n\n[1] 3\n\nlog(8, 2)\n\n[1] 3\n\n\nИли что-то на тригонометрическом:\n\nsin(5); cos(5); tan(5)\n\n[1] -0.9589243\n\n\n[1] 0.2836622\n\n\n[1] -3.380515\n\n\nКстати, можно и вот так — это к тому, что математические операторы тоже являются функциями:\n\n`+`(2, 3)\n\n[1] 5\n\n`^`(4, 5)\n\n[1] 1024\n\n`/`(8, 3)\n\n[1] 2.666667\n\n\n\n\n1.2.3 Логические операции\nК логическим операциями можно отнести операции сравнения:\n\n5 &gt; 4 # больше\n\n[1] TRUE\n\n6 &lt; 2 # меньше\n\n[1] FALSE\n\n5 &gt;= 5 # больше или равно\n\n[1] TRUE\n\n6 &lt;= 3 # меньше или равно\n\n[1] FALSE\n\n23 == 14 # равно\n\n[1] FALSE\n\n77 != 98 # не равно\n\n[1] TRUE\n\n\nА также логические операторы И (&) и ИЛИ (|):\n\nTRUE & TRUE\n\n[1] TRUE\n\nTRUE & FALSE\n\n[1] FALSE\n\nFALSE & FALSE\n\n[1] FALSE\n\nTRUE | TRUE\n\n[1] TRUE\n\nTRUE | FALSE\n\n[1] TRUE\n\nFALSE | FALSE\n\n[1] FALSE\n\n\n\n\n1.2.4 Переменные и объекты\nРезультаты вычислений и преобразований хотелось бы сохранять, поэтому в R существует оператор присваивания &lt;-:\n\nx &lt;- 5\ny &lt;- 4 * 8\n\nМожно, конечно, написать и x = 5, но сообщество вас не поймет и будет косо смотреть… Когда мы присвоим некоторой переменной какой-либо объект, он отобразиться в окошке Environment, и с ним можно будет работать. Например, совершать разные операции:\n\nx + y\n\n[1] 37\n\nsqrt(x)\n\n[1] 2.236068\n\nlog(y, base = x)\n\n[1] 2.153383\n\n\nОбъектом в R может быть вообще все, что угодно — число, строка, вектор, матрица, датафрейм, таблица, результат моделирования, функция и т.д.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#типы-данных",
    "href": "l1.html#типы-данных",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.3 Типы данных",
    "text": "1.3 Типы данных\nТип данных — это характеристика данных, которая определяет:\n\nмножество допустимых значений, которые могут принимать данные этого типа\nдопустимые операции над данными этого типа\n\n\n1.3.1 numeric\nЭто числа с десятичной частью.\n\nclass(3.14)\n\n[1] \"numeric\"\n\ntypeof(3.14)\n\n[1] \"double\"\n\n\n\"double\" нам говорит о том, что числа с десятичной частью храняться в R с двойной точностью. И это хорошо.\n\n\n1.3.2 integer\nЭто целые числа.\n\nclass(3)\n\n[1] \"numeric\"\n\n\nПравда чтобы создать именно целое число, надо указать, что мы хотим именно целое число с помощью литерала L:\n\nclass(3L)\n\n[1] \"integer\"\n\ntypeof(3L)\n\n[1] \"integer\"\n\n\nПо умолчанию объект типа 3 воспринимается R как 3.0, поэтому тип данных будет numeric.\n\n\n1.3.3 complex\nКомплексные числа тоже существуют, и мы с ними немного познакомимся, чтобы перестать их бояться.\n\nclass(2+3i)\n\n[1] \"complex\"\n\n\n\n\n1.3.4 character\nТекст тоже надо как-то хранить.\n\ns1 &lt;- 'a'\ns2 &lt;- \"это строка\"\n\nclass(s1)\n\n[1] \"character\"\n\nclass(s2)\n\n[1] \"character\"\n\n\nКавычки не важны, если у вас не встречаются кавычки внутри кавычек. Тогда надо использовать разные:\n\ns &lt;- 'Мужчина громко зашёл в комнату и высказал решительное \"здравствуйте\"'\ns\n\n[1] \"Мужчина громко зашёл в комнату и высказал решительное \\\"здравствуйте\\\"\"\n\n\n\n\n1.3.5 factor\nБывают такие переменные, которые группируют наши данные. Например,\n\nгород проживания (Москва, Санкт-Петербург, Казань, Екатеринбург)\nуровень образования (бакалавриат, специалитет, магистратура, аспирантура)\nэкспериментальная группа (group1, group2, control)\nи др.\n\nОбычно они текстовые. Для них был придуман тип данных factor, чтобы их было дешевле хранить. Однако большинство современных пакетов сами могут решить, когда надо текст перевести в фактор.\nOrdered factor (упорядоченный фактор) — тип данных, который позволяет задать порядок групп. Например,\n\nуровень образования: bachelor &lt; master &lt; phd &lt; postdoc\nсложность экспериментальной задачи: easy &lt; medium &lt; hard\nи др.\n\nВот упорядоченный фактор нам время от времени может понадобиться — например, для создания визуалиация или в некоторых статистических моделях.\n\n\n1.3.6 Специальные литералы\n\n1.3.6.1 NA\nПропущенное значение (Not Available). Обозначает отсутствие значения там, где оно вроде бы должно быть. Причины могут быть разные:\n\nтехнические ошибки записи данных\nошибки настройки платформы — забыли сделать ответы обязательными\nорганизация исследования — ограничили время на ответ\n«честный» пропуск — дали возможность не отвечать на вопрос\nпредобработка данных — специально создали NA, чтобы далее с ними работать\nи др.\n\n\n\n1.3.6.2 NaN\nЭто не число (Not a Number).\n\n0 / 0\n\n[1] NaN\n\n\n\n\n1.3.6.3 NULL\nЭто ничто. Пустота. Используется для задания аргументов функций.\n\nggplot(data = NULL)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#структуры-данных",
    "href": "l1.html#структуры-данных",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.4 Структуры данных",
    "text": "1.4 Структуры данных\nСтруктура данных — это способ и форма объединения однотипных и/или логически связанных данных.\n\n\n\n\nПример данных\n\n\n\n\n1.4.1 Датафрейм\nВоплощение привычной нам «таблицы» в R.\n\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n\nЭто сложная структура данных. Чтобы понять всю её мощь, необходимо начать с более простых.\n\n\n1.4.2 Векторы\nВектор — это набор чисел.\n\\[\n\\pmatrix{1 & 4 & 36 & -8 & 90.1 & -14.5}\n\\]\nЕсли это утверждение вызывает у вас внутренний протест, давай проследим пусть от направленного отрезка в набору чисел.\nВозьмем направленный отрезок — вектор:\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\nИменно так мы понимали вектор в школе. Договоримся, что все векторы у нас начинаются из точки \\((0, 0)\\):\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\nЕсли теперь у нас все вектора начинаются из начал координат, то мы можем полностью описать вектор только координатами его конца. Поэтому уберем вектор:\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\nТо есть для нас теперь вектор равносилен точке на плоскости. А точка однозначно описывается двумя координатами. Получается, можно просто записать:\n\\[\n\\pmatrix{1 & 2}\n\\]\nПолучается, что это одно и то же:\n\\[\n\\pmatrix{1 & 0.5}, \\quad \\pmatrix{2 & 3}, \\quad \\pmatrix{4.2 & -3.5}\n\\]\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\nТеперь обобщим вектор на более общие случаи:\n\nВектор — это набор некоторого колчиества элементов одного типа.\n\n\nv_num &lt;- c(1, 6, -34, 7.7) # числовой вектор\nv_char &lt;- c(\"Москва\", \"Санкт-Петербург\", \"Нижний Новгород\", \"Пермь\") # текстовый вектор\nv_log &lt;- c(TRUE, FALSE, TRUE, TRUE) # логический вектор\n\n\nclass(v_num)\n\n[1] \"numeric\"\n\nv_num\n\n[1]   1.0   6.0 -34.0   7.7\n\nclass(v_char)\n\n[1] \"character\"\n\nv_char\n\n[1] \"Москва\"          \"Санкт-Петербург\" \"Нижний Новгород\" \"Пермь\"          \n\nclass(v_log)\n\n[1] \"logical\"\n\nv_log\n\n[1]  TRUE FALSE  TRUE  TRUE\n\n\n\n1.4.2.1 Индексация векторов\nИз вектора можно вытащить его элемент:\n\nv_char[2] # по номеру\n\n[1] \"Санкт-Петербург\"\n\nv_num[v_num &gt; 5] # по условию\n\n[1] 6.0 7.7\n\n\n\n\n1.4.2.2 Векторизация\nДля того, чтобы выполнить операцию на всем векторе поэлементно, не нужно перебирать его элементы.\n\nvec &lt;- 1:4\nvec - 1\n\n[1] 0 1 2 3\n\nvec^2\n\n[1]  1  4  9 16\n\nsqrt(vec)\n\n[1] 1.000000 1.414214 1.732051 2.000000\n\n\n\n\n1.4.2.3 Recycling\nЕсли мы будем, например, складывать два вектора разной длины, то более короткий зациклится.\n\nvec1 &lt;- 1:10\nvec2 &lt;- 1:2\n\nvec1\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nvec2\n\n[1] 1 2\n\nvec1 + vec2\n\n [1]  2  4  4  6  6  8  8 10 10 12",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#матрицы",
    "href": "l1.html#матрицы",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.5 Матрицы",
    "text": "1.5 Матрицы\nЕсли мы желаем приблизиться к датафрейму, то одного ряда элементов нам недостаточно — надо выходить во второе измерение! Поэтому уложим вектор в матрицу:\n\n\n\n\nВарианты преобразования вектора в матрицу\n\n\n\nИли вот еще разные варианты:\n\nv &lt;- 1:12\nm1 &lt;- matrix(v, nrow = 3)\nm1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nm2 &lt;- matrix(v, nrow = 4)\nm2\n\n     [,1] [,2] [,3]\n[1,]    1    5    9\n[2,]    2    6   10\n[3,]    3    7   11\n[4,]    4    8   12\n\nm3 &lt;- matrix(v, nrow = 3, byrow = TRUE)\nm3\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\nm4 &lt;- matrix(v, nrow = 4, byrow = TRUE)\nm4\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n[4,]   10   11   12\n\n\n\n1.5.1 Индексация матриц\nИз матрицы можно вытащить её элементы:\n\nm1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nm1[2, 3] # отдельный элемент\n\n[1] 8\n\nm1[1, ] # целую строку\n\n[1]  1  4  7 10\n\nm1[, 4] # целый столбец\n\n[1] 10 11 12\n\nm1[1:2, 2:4] # часть матрицы\n\n     [,1] [,2] [,3]\n[1,]    4    7   10\n[2,]    5    8   11",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#массивы",
    "href": "l1.html#массивы",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.6 Массивы",
    "text": "1.6 Массивы\n\nВектор — одномерный массив.\nМатрица — двумерный массив.\nМассивы — структуры, которые объединяют данные только одного типа.\n\n\nc(2, TRUE)\n\n[1] 2 1\n\nc(2, TRUE, \"word\")\n\n[1] \"2\"    \"TRUE\" \"word\"\n\n\nПри объединении разных типов данных в одном массиве происходит приведение типов (coercion) по следующей иерархии:\n\nlogical → integer → numeric → complex → character\n\nЭто нам осложняет жизнь, так как мы бы хотели объединять данные разных типов в одну структуру.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#списки",
    "href": "l1.html#списки",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.7 Списки",
    "text": "1.7 Списки\nСписки позволяют объединять массивы различных типов данных, чем делают нашу жизнь значительно приятнее.\n\n\n\n\nСхема внутренней структуры списка\n\n\n\nНапример, так:\n\nl &lt;- list(v1 = v_num,\n          v2 = v_char,\n          m1 = m1,\n          ls = list(v = v,\n                    m = m3))\nl\n\n$v1\n[1]   1.0   6.0 -34.0   7.7\n\n$v2\n[1] \"Москва\"          \"Санкт-Петербург\" \"Нижний Новгород\" \"Пермь\"          \n\n$m1\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n$ls\n$ls$v\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n$ls$m\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\n\n1.7.1 Индексация списков\n\nl[1] # по номеру элемента, возвращается список\n\n$v1\n[1]   1.0   6.0 -34.0   7.7\n\nl[[1]] # по номеру элемента, возвращается массив\n\n[1]   1.0   6.0 -34.0   7.7\n\nl$ls # по названию элемента\n\n$v\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n$m\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\nl$ls$m # можно идти многоуровнево\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#собираем-датафрейм",
    "href": "l1.html#собираем-датафрейм",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.8 Собираем датафрейм",
    "text": "1.8 Собираем датафрейм\n\nвозьмем список\nпотребуем, чтобы его элементами были векторы\nпотребуем, чтобы эти векторы были одинаковой длины\nрасположим их «вертикально»\n\n\n\n\n\nСтруктура списка и датафрейма\n\n\n\n\n1.8.1 Индексация датафрейма\nДля примера возьмем датафрейм про бриллианты:\n\ndiam\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\n\nДатафрейм наследует свойства списка и матрицы, что делает его невероятно гибким в обращении и крайне удобным в работе:\n\ndiam$carat # вытащить столбец\n\n[1] 0.23 0.21 0.23 0.29 0.31 0.24\n\ndiam[diam$price &gt; 330, ] # отобрать строки по условию\n\n# A tibble: 3 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n2  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n3  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\ndiam[, c(2:3, 7)] # вытащить столбцы по номерам\n\n# A tibble: 6 × 3\n  cut       color price\n  &lt;ord&gt;     &lt;ord&gt; &lt;int&gt;\n1 Ideal     E       326\n2 Premium   E       326\n3 Good      E       327\n4 Premium   I       334\n5 Good      J       335\n6 Very Good J       336\n\ndiam[1:4, c(\"carat\", \"price\")] # вытащить отдельные строки по номерам и столбцы по названиям\n\n# A tibble: 4 × 2\n  carat price\n  &lt;dbl&gt; &lt;int&gt;\n1  0.23   326\n2  0.21   326\n3  0.23   327\n4  0.29   334",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#функции",
    "href": "l1.html#функции",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.9 Функции",
    "text": "1.9 Функции\n\nЕсли какой-либо кусок кода повторяется более трех раз, имеет смысл обернуть его в функцию.\n\nКак стоит понимать функцию?\nФункция — это некий черный ящик, который\n\nпринимает что-либо на вход\nпроделывает с этим какие-либо операции\nи что-то возвращает\n\n\n1.9.1 Синтаксис функции\nСинтаксис создания функции выглядит так:\n\nfunction_name &lt;- function(arguments) {\n    ...\n    body\n    ...\n    return()\n}\n\nЭлементы функции:\n\nимя функции (function_name) — как мы к ней будем обращаться при вызове\nаргументы функции (arguments) — какие значения и объекты она принимает на вход\nтело функции (body) — что она делает с входными объектами\nвозвращаемое значение (return()) — что функция вернет в качестве результата работы\n\nВызов функции:\n\nfunction_name(arguments)\n\n\n\n1.9.2 Пример функции\n\ncot &lt;- function(x) {\n  result &lt;- 1 / tan(x)\n  return(result)\n}\ncot(3)\n\n[1] -7.015253\n\n\nЕсли функция простая, можно не создавать временные объекты:\n\ncot &lt;- function(x) {\n  return(1 / tan(x))\n}\ncot(3)\n\n[1] -7.015253\n\n\nЕсли функция короткая, можно даже не писать return():\n\ncot &lt;- function(x) {\n  1 / tan(x)\n}\ncot(3)\n\n[1] -7.015253\n\n\n\n\n1.9.3 Пример более полезной функции\nОсторожно, большое!\nЭто функция, которая занималась предобработкой данных в реальном проекте. Прикиньте, если бы мы такой кусок кода повторяли для каждого датасета… — жуть!\n\n\n\n\n\n\nВажно!\n\n\n\nВам не нужно сейчас подробно понимать, что написано ниже — мы все разберем по ходу курса и научимся писать такое же! Сейчас главное ухватить структуру функции — где аргументы, где тело, где возвращаемое значение. Всё! Остальное освоим по ходу дела.\n\n\n\nmr_preproc &lt;- function(d) {\n\n  require(tidyverse)\n  \n  d |&gt; select(\n    # select columns we need\n    \"Индивидуальный_код\",\n    correctAns,\n    base_pic,\n    rotated_pic,\n    resp_MR_easy.keys,\n    resp_MR_easy.corr,\n    resp_MR_easy.rt\n  ) |&gt;\n    drop_na() |&gt; # remove technical NAs (recording artefacts, not missing data)\n    mutate(task = \"MR\",\n           # add task name (mental rotation)\n           level = \"easy\",\n           # add difficulty level\n           trial = 1:16) |&gt; # number trials\n    rename(\n      \"id\" = \"Индивидуальный_код\",\n      # rename columns for handy usage\n      \"key\" = resp_MR_easy.keys,\n      \"is_correct\" = resp_MR_easy.corr,\n      \"rt\" = resp_MR_easy.rt\n    ) -&gt; MR # ready to use\n  \n  return(MR)\n \n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#условный-оператор",
    "href": "l1.html#условный-оператор",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.10 Условный оператор",
    "text": "1.10 Условный оператор\nИногда при написании функции может понадобиться обработать какие-то важные случаи.\n\nНапример, в двух запусках сбора данных столбцы были названы по-разному: если это не учесть, код будет ломаться.\n\nДля этого подойдет условный оператор.\n\n1.10.1 Структура условного оператора\n\nif (condition) {\n  ...\n  body\n  ...\n} else {\n  ...\n  body\n  ...\n}\n\n\n\n1.10.2 Пример функции с условным оператором\nДопустим, нам жизненно неободима функция, которая будет определять, является число четным или нечетным, потому что сами мы постоянно путаемся…\nВот она:\n\nodd_even &lt;- function(x) { # функция принимает на вход число\n  \n  if (x %% 2 == 0) { # проверяет, равняется ли нулю остаток от деления числа на два\n    \n    return(\"even\") # возвращает \"even\", если равняется\n    \n  } else {\n    \n    return(\"odd\") # возвращает \"odd\", если нет\n    \n  }\n  \n}\n\n\nodd_even(2)\n\n[1] \"even\"\n\nodd_even(34)\n\n[1] \"even\"\n\nodd_even(11)\n\n[1] \"odd\"\n\nodd_even(135)\n\n[1] \"odd\"\n\n\nРаботает!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#пример-функции-из-реального-проекта-с-условным-оператором",
    "href": "l1.html#пример-функции-из-реального-проекта-с-условным-оператором",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.11 Пример функции из реального проекта с условным оператором",
    "text": "1.11 Пример функции из реального проекта с условным оператором\n\n\n\n\n\n\nВажно!\n\n\n\nВам не нужно сейчас подробно понимать, что написано ниже — мы все разберем по ходу курса и научимся писать такое же! Сейчас главное ухватить структуру условного оператора — где условие, что выполняется, если условие верно, что выполняется, если условие ложно. Всё! Остальное освоим по ходу дела.\n\n\n\nms_preproc &lt;- function(d) {\n  \n  require(tidyverse)\n  \n  # Since we our participants could fill the fields in any order, \n  # here is a function which allows us to count correct inputs \n  # our subjects made.\n  \n  if (\"mouse_MSe.time\" %in% colnames(d)) { \n    ### здесь начинается условный оператор, который проверяет, есть ли такая колонка\n    ### если колонка есть, то запускается код ниже\n    \n    d |&gt; select(\n      \"Индивидуальный_код\",\n      matches(\"^noun\"),\n      matches(\"resp\\\\d\\\\.text$\"),\n      \"mouse_MSe.time\"\n    ) |&gt;\n      filter_at(vars(paste0(\"noun\", 1:3)), all_vars(!is.na(.))) |&gt;\n      filter_at(vars(paste0(\"noun\", 4:7)), all_vars(is.na(.))) |&gt;\n      mutate(task = \"MS\",\n             level = \"easy\") |&gt;\n      rename(\n        \"resp1\" = resp1.text,\n        \"resp2\" = resp2.text,\n        \"resp3\" = resp3.text,\n        \"id\" = \"Индивидуальный_код\",\n        \"rt\" = \"mouse_MSe.time\"\n      ) |&gt;\n      select(-c(paste0(\"noun\", 4:7))) -&gt; MS\n    \n  } else {\n    ### а если колонки нет, то запускается этот код\n    \n    d |&gt; select(\"Индивидуальный_код\",\n                matches(\"^noun\"),\n                matches(\"resp\\\\d\\\\.text$\")) |&gt;\n      filter_at(vars(paste0(\"noun\", 1:3)), all_vars(!is.na(.))) |&gt;\n      filter_at(vars(paste0(\"noun\", 4:7)), all_vars(is.na(.))) |&gt;\n      mutate(task = \"MS\",\n             level = \"easy\",\n             rt = NA) |&gt;\n      rename(\n        \"resp1\" = resp1.text,\n        \"resp2\" = resp2.text,\n        \"resp3\" = resp3.text,\n        \"id\" = \"Индивидуальный_код\"\n      ) |&gt;\n      select(-c(paste0(\"noun\", 4:7))) -&gt; MS\n\n  }\n  \n  return(MS)\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#вне-функций",
    "href": "l1.html#вне-функций",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.12 Вне функций",
    "text": "1.12 Вне функций\nВне функций условный оператор практически не используется, потому что для предобработки данных есть удобная функция ifelse().",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#циклы",
    "href": "l1.html#циклы",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.13 Циклы",
    "text": "1.13 Циклы\nТо, что мы написали функция, чтобы не дублировать код — это хорошо, однако эту функцию нам все равно придется запускать много раз, если нам надо этот кусок кода повторить.\nПоэтому используем цикл:\n\nfor (i in a:b) {\n  ...\n  body\n  ...\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#пример-простеньких-циклов",
    "href": "l1.html#пример-простеньких-циклов",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.14 Пример простеньких циклов",
    "text": "1.14 Пример простеньких циклов\nПросто печатаем числа от 1 до 10:\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nНу, или более сложные выражения:\n\nfor (j in 1:10) {\n  print(sqrt(j) + j^2)\n}\n\n[1] 2\n[1] 5.414214\n[1] 10.73205\n[1] 18\n[1] 27.23607\n[1] 38.44949\n[1] 51.64575\n[1] 66.82843\n[1] 84\n[1] 103.1623",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#пример-цикла-для-чтения-и-предоработки-данных",
    "href": "l1.html#пример-цикла-для-чтения-и-предоработки-данных",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.15 Пример цикла для чтения и предоработки данных",
    "text": "1.15 Пример цикла для чтения и предоработки данных\n\n\n\n\n\n\nВажно!\n\n\n\nВам не нужно сейчас подробно понимать, что написано ниже — мы все разберем по ходу курса и научимся писать такое же! Сейчас главное ухватить структуру цикла — где итератор и где тело, которое будет выполняться несколько раз. Всё! Остальное освоим по ходу дела.\n\n\n\nfor (i in 1:length(files)) { ## будем двигаться от 1 до количества файлов в папке с данными\n  \n  print(files[i]) ## печатает имя файла, чтобы видеть на каком файле сломалось, если сломается\n  \n  d &lt;- read_csv(files[i], show_col_types = FALSE) ## считывает один файл из папки\n  \n  ## запускаем функции предобработки\n  MR_data |&gt; bind_rows(mr_preproc(d) |&gt; mutate(file = files[i])) -&gt; MR_data\n  ST_data |&gt; bind_rows(st_preproc(d) |&gt; mutate(file = files[i])) -&gt; ST_data\n  MS_data |&gt; bind_rows(ms_preproc(d) |&gt; mutate(file = files[i])) -&gt; MS_data\n  NASATLX_data |&gt; bind_rows(nasatlx_preproc(d) |&gt; mutate(file = files[i])) -&gt; NASATLX_data\n  SEQUENCE_data |&gt; bind_rows(sequence_preproc(d) |&gt; mutate(file = files[i])) -&gt; SEQUENCE_data\n  \n  ## завершили цикл, идем на следующую итерацию\n\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#циклы-в-r-это-зло-они-долго-работают",
    "href": "l1.html#циклы-в-r-это-зло-они-долго-работают",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "1.16 Циклы в R — это зло! Они долго работают!",
    "text": "1.16 Циклы в R — это зло! Они долго работают!\n\nДа, циклы работают не быстро — это правда. Но, с другой стороны, мы и не терабайты данных анализируем.\n\nДопустим, у нас 50 респондентов. Цикл, подобный тому, что на предыдущем слайде, отбработает секунды за 3. Даже чай не успеете заварить.\nБезусловно, есть более изящные и быстрые инструменты, и с ними мы познакомимся на предобработке данных. Но в целом, можно и циклом обойтись.\nКонечно, если у вас огромные датасеты и вы работаете с Big Data, то прогон цикла может значительно затянуться — в этом случае разумно сразу использовать другие инструменты.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l1.html#footnotes",
    "href": "l1.html#footnotes",
    "title": "1  L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции",
    "section": "",
    "text": "По пути надо ещё не перепутать с R-Studio, которая восстанавливает данные с диска. Критическое сходство названий двух программ обязывает к повышенной внимательности при написании работ/статей/отчётов/заявок на гранты, в которых вы ссылаетесь на RStudio — иногда рецензенты весьма недоумевают, как исследователи анализировали данные с помощью ПО для восстановления данных…↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "l2.html",
    "href": "l2.html",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "",
    "text": "2.1 Форматы файлов данных",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#форматы-файлов-данных",
    "href": "l2.html#форматы-файлов-данных",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "",
    "text": "2.1.1 CSV\n\nТекстовый формат\nЗначения разделены запятыми (Comma-Separated Values)\n\nНо это не точно\n\n\nПрочитать данные этого формата можно функцией\n\nread.csv()\n\n\n\n2.1.2 TSV\n\nТекстовый формат\nЗначения разделены знаком табуляции (\\t, Tab-Separated Values)\n\nПрочитать данные этого формата можно функцией\n\nread.table()\n\n\n\n2.1.3 TXT\n\nТекстовый формат\nРазделитель может быть любой\n\nПрочитать данные этого формата можно функцией\n\nread.table()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#табличные-форматы",
    "href": "l2.html#табличные-форматы",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.2 Табличные форматы",
    "text": "2.2 Табличные форматы\n\nФайлы Excel — .xls, .xlsx\n\nДля их чтения понадобится пакет readxl, в котором есть функции\n\nreadxl::read_xls()\nreadxl::read_xlsx()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#форматы-файлов-данных-c-особой-разметкой",
    "href": "l2.html#форматы-файлов-данных-c-особой-разметкой",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.3 Форматы файлов данных c особой разметкой",
    "text": "2.3 Форматы файлов данных c особой разметкой\nВстречаются редко и обычно связаны с данными, которые достаются из интернета (парсинг страниц или выгрзуки JavaScript).\n\n2.3.1 JSON\nВыглядят так:\n\n{\n  \"first_name\": \"John\",\n  \"last_name\": \"Smith\",\n  \"is_alive\": true,\n  \"age\": 27,\n  \"address\": {\n    \"street_address\": \"21 2nd Street\",\n    \"city\": \"New York\",\n    \"state\": \"NY\",\n    \"postal_code\": \"10021-3100\"\n  },\n  \"phone_numbers\": [\n    {\n      \"type\": \"home\",\n      \"number\": \"212 555-1234\"\n    },\n    {\n      \"type\": \"office\",\n      \"number\": \"646 555-4567\"\n    }\n  ],\n  \"children\": [\n    \"Catherine\",\n    \"Thomas\",\n    \"Trevor\"\n  ],\n  \"spouse\": null\n}\n\nДля работы с ними пригодится пакет jsonlite.\n\n\n2.3.2 XML\nВыглядят так:\n\n&lt;?xml version=\"1.0\"?&gt;\n&lt;catalog&gt;\n   &lt;book id=\"bk101\"&gt;\n      &lt;author&gt;Gambardella, Matthew&lt;/author&gt;\n      &lt;title&gt;XML Developer's Guide&lt;/title&gt;\n      &lt;genre&gt;Computer&lt;/genre&gt;\n      &lt;price&gt;44.95&lt;/price&gt;\n      &lt;publish_date&gt;2000-10-01&lt;/publish_date&gt;\n      &lt;description&gt;An in-depth look at creating applications \n      with XML.&lt;/description&gt;\n   &lt;/book&gt;\n   &lt;book id=\"bk102\"&gt;\n      &lt;author&gt;Ralls, Kim&lt;/author&gt;\n      &lt;title&gt;Midnight Rain&lt;/title&gt;\n      &lt;genre&gt;Fantasy&lt;/genre&gt;\n      &lt;price&gt;5.95&lt;/price&gt;\n      &lt;publish_date&gt;2000-12-16&lt;/publish_date&gt;\n      &lt;description&gt;A former architect battles corporate zombies, \n      an evil sorceress, and her own childhood to become queen \n      of the world.&lt;/description&gt;\n   &lt;/book&gt;\n   &lt;book id=\"bk103\"&gt;\n      &lt;author&gt;Corets, Eva&lt;/author&gt;\n      &lt;title&gt;Maeve Ascendant&lt;/title&gt;\n      &lt;genre&gt;Fantasy&lt;/genre&gt;\n      &lt;price&gt;5.95&lt;/price&gt;\n      &lt;publish_date&gt;2000-11-17&lt;/publish_date&gt;\n      &lt;description&gt;After the collapse of a nanotechnology \n      society in England, the young survivors lay the \n      foundation for a new society.&lt;/description&gt;\n   &lt;/book&gt;\n&lt;/catalog&gt;\n\nДля работы с ними пригодится пакет XML.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#организация-рабочего-пространства",
    "href": "l2.html#организация-рабочего-пространства",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.4 Организация рабочего пространства",
    "text": "2.4 Организация рабочего пространства\nРабочая директория — папка, в которую по умолчанию смотрит R, когда начинает искать файлы.\nЧтобы узнать рабочую директорию, воспользуйтесь функцией\n\ngetwd()\n\nЧтобы установить какую-либо папку в качестве рабочей директории, используйте\n\nsetwd(\"/home/nglgrdt/R\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#путь-к-файлу",
    "href": "l2.html#путь-к-файлу",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.5 Путь к файлу",
    "text": "2.5 Путь к файлу\nБывает двух видов:\n\nабсолютный — /home/nglgrdt/R/wlm2023/pr1-course.R\nотносительный — wlm2023/pr1-course.R\n\nОтносительный путь вычисляется относительно текущей рабочей директории.\nПолезная вещь: .. — подняться на один уровень в иерархии папок.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#projects",
    "href": "l2.html#projects",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.6 Projects",
    "text": "2.6 Projects\n\nRStudio предоставляет возможность работать в проектах\nЭто существенно упрощает организацию файлов, относящихся к одному исследованию\nВнутри проекта можно создавать необходимые папки (для данных, скриптов, результатов анализа, визуализаций и др.)\nПри запуске проекта автоматически устанавливается рабочая директория\nУдобно использовать относительные пути к файлам\nЧтобы код воспроизводился на другом компьютере, достаточно заархивировать весь проект и отправить коллеге\n\n\n2.6.1 Как создать проект?\n\nВ правом верхнем углу нажмите на Project: (None) и выберите New Project....\n\n\n\n\n\nСоздание проекта. Шаг 1\n\n\n\n\nВ открывшемся окне выберите New Directory. Это опция создаст новую папку для проекта.\n\n\n\n\n\nСоздание проекта. Шаг 2\n\n\n\n\nДалее выберите New Project.\n\n\n\n\n\nСоздание проекта. Шаг 3\n\n\n\n\nВ поле Directory name укажите, как будет называться папка проекта. В поле Create project as subdirectory of: можно выбрать, внутри какой папки будет создана папка проекта. Нажмите Create Project.\n\n\n\n\n\nСоздание проекта. Шаг 4\n\n\n\n\nПроект создан и открыт. Теперь в нем можно работать. Чтобы узнать, в каком проекте вы сейчас находитесь, посмотрите в правый верхний угол.\n\n\n\n\n\nСоздание проекта. Шаг 5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#кодировка",
    "href": "l2.html#кодировка",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.7 Кодировка",
    "text": "2.7 Кодировка\n\nКомпьютер умеет хранить только числа, а скрипт — это текст\nДанные также часто содержат текст\nДоговорились, что буквенные символы будут храниться на железе компьютера в виде чисел, и составили таблицы соответствий между числами и буквами\nТакие таблицы были названы кодировками\nКодировок много, так как они создавались для разных алфавитов и задач\nСтандартной считается UTF-8\nПри работе с нестандартными символами (например, диакритические знаки или фонетические символы) пригодится UTF-16",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#первичное-исследование-данных",
    "href": "l2.html#первичное-исследование-данных",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.8 Первичное исследование данных",
    "text": "2.8 Первичное исследование данных\nПрочитаем данные. В данном случае использован относительный путь.\n\nlec2 &lt;- read.csv(\"data/lecture2.csv\", encoding = \"UTF-8\")\n\nВыведем первые 6 строк датасета:\n\nhead(lec2)\n\n  id var1 var2  var3  var4 var5\n1  1  Abc    5  TRUE cond1 12.8\n2  2  Def   16 FALSE cond1 14.2\n3  3  Ghi   94 FALSE cond2 32.5\n4  4  Jkl   28 FALSE cond2  9.4\n5  5  Mno   11  TRUE cond3  6.3\n6  6  Pqr  100  TRUE cond3 11.7\n\n\nВыведем последние 3 строки датасета:\n\ntail(lec2, n = 3)\n\n  id var1 var2  var3  var4 var5\n5  5  Mno   11  TRUE cond3  6.3\n6  6  Pqr  100  TRUE cond3 11.7\n7  7  Stu   96 FALSE cond1 95.5\n\n\nПосмотрим на структуру датасета:\n\nstr(lec2)\n\n'data.frame':   7 obs. of  6 variables:\n $ id  : int  1 2 3 4 5 6 7\n $ var1: chr  \"Abc\" \"Def\" \"Ghi\" \"Jkl\" ...\n $ var2: int  5 16 94 28 11 100 96\n $ var3: logi  TRUE FALSE FALSE FALSE TRUE TRUE ...\n $ var4: chr  \"cond1\" \"cond1\" \"cond2\" \"cond2\" ...\n $ var5: num  12.8 14.2 32.5 9.4 6.3 11.7 95.5\n\n\nПосмотрим описательные статистики по переменным:\n\nsummary(lec2)\n\n       id          var1                var2          var3        \n Min.   :1.0   Length:7           Min.   :  5.0   Mode :logical  \n 1st Qu.:2.5   Class :character   1st Qu.: 13.5   FALSE:4        \n Median :4.0   Mode  :character   Median : 28.0   TRUE :3        \n Mean   :4.0                      Mean   : 50.0                  \n 3rd Qu.:5.5                      3rd Qu.: 95.0                  \n Max.   :7.0                      Max.   :100.0                  \n     var4                var5      \n Length:7           Min.   : 6.30  \n Class :character   1st Qu.:10.55  \n Mode  :character   Median :12.80  \n                    Mean   :26.06  \n                    3rd Qu.:23.35  \n                    Max.   :95.50  \n\n\nСделаем частотную таблицу по категориальной переменной:\n\ntable(lec2$var4)\n\n\ncond1 cond2 cond3 \n    3     2     2 \n\n\nПосмотрим уникальные значений по переменной-индентификатору:\n\nunique(lec2$id)\n\n[1] 1 2 3 4 5 6 7\n\n\nОтсортируем количественную переменную по возрастанию:\n\nsort(lec2$var5)\n\n[1]  6.3  9.4 11.7 12.8 14.2 32.5 95.5\n\n\nПроверим, есть ли пропущенные значения — выполним функцию is.na() на каждом столце датасета:\n\napply(lec2, 2, is.na)\n\n        id  var1  var2  var3  var4  var5\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE FALSE FALSE FALSE\n[5,] FALSE FALSE FALSE FALSE FALSE FALSE\n[6,] FALSE FALSE FALSE FALSE FALSE FALSE\n[7,] FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nУпрощенным вариантом функции apply() является sapply() — она сразу запускает какую-либо функцию по столбцам датасета. В данном случае использована анонимная функция, которая посчитает количество пропущенных значений в каждом столбце.\n\nsapply(lec2, function(x) sum(is.na(x)))\n\n  id var1 var2 var3 var4 var5 \n   0    0    0    0    0    0",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#зачем-нам-предобрабатывать-данные",
    "href": "l2.html#зачем-нам-предобрабатывать-данные",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.9 Зачем нам предобрабатывать данные?",
    "text": "2.9 Зачем нам предобрабатывать данные?\n\nДанные разнообразны в зависимости от того\n\nкакая у нас исследовательская область\nкакой у нас исследуемый феномен\nс каким оборудованием мы работаем\n…\n\nДля того, чтобы мы могли работать с любыми данными, независимо от того, откуда они к нам пришли, нам нужно привести их к некому стандартному виду",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#концепция-tidy-data",
    "href": "l2.html#концепция-tidy-data",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.10 Концепция Tidy Data",
    "text": "2.10 Концепция Tidy Data\n\nВ каждом столбце содержится одна переменная\nВ каждой строке содержится одно наблюдение\nВ каждой ячейке содержится одно значение\n\n\n\n\n\nПринципы концепции Tidy Data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#семейство-пакетов-tidyverse",
    "href": "l2.html#семейство-пакетов-tidyverse",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.11 Семейство пакетов tidyverse",
    "text": "2.11 Семейство пакетов tidyverse\nУстановить пакет:\n\ninstall.packages(\"tidyverse\")\n\nПодключить пакет к текущей сессии R:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#импорт-данных.-tibble",
    "href": "l2.html#импорт-данных.-tibble",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.12 Импорт данных. tibble",
    "text": "2.12 Импорт данных. tibble\nФункции импорта данных из tidyverse атоматически читают данные в tibble. С точки зрения пользователя tibble — это тот же самый датафрейм. Есть некоторые отличия в выводе в консоль, с которыми мы ближе познакомимся на практике.\nДля чтения CSV-файлов есть следующие функции:\n\nread_csv()\nread_csv2()\n\nДля чтения данных с другими разделителями есть функция\n\nread_delim()\n\nЗагрузим данные:\n\nds &lt;- read_csv(\"data/lecture2.csv\")\n\nRows: 7 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): var1, var4\ndbl (3): id, var2, var5\nlgl (1): var3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#pipe",
    "href": "l2.html#pipe",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.13 Pipe",
    "text": "2.13 Pipe\nПрежде чем начать разговор о предобработке, познакомимся с одним очень полезным оператором из tidyverse — это пайп.\nСуществует два вида пайпа:\n\nмаггритеровский %&gt;%\n\nтребуется подключение пакета tidyverse\n\nнативный |&gt;\n\nнаходится в базовом R, включается через настройки\n\nTools &gt; Global Options &gt; Code &gt; Use native pipe operator\n\n\n\nС точки зрения пользователя практически не отличаются друг от друга. И тот, и другой вводится комбинацией Ctrl + Shift + M. Выбрать, какой именно будет использоваться, можно в настройках (Tools &gt; Global Options &gt; Code &gt; Use native pipe operator.)\n\n\n\n\nВыбор используемого пайпа в настройках\n\n\n\nПайп передает то, что слева от него, в функцию, которая справа от него, в качестве первого аргумента.\n\nsum(1:3)\n\n[1] 6\n\n1:3 %&gt;% sum()\n\n[1] 6\n\n\n\nround(pi, 2)\n\n[1] 3.14\n\npi %&gt;% round(2)\n\n[1] 3.14\n\n\nПозволяет выстраивать цепочки последовательных преобразований:\n\nsqrt(abs(log(abs(round(sin(1 / cos(3)), 2)), 3)))\n\n[1] 0.3846181\n\n3 %&gt;% cos() %&gt;% \n  `/`(1, .) %&gt;% \n  sin() %&gt;% \n  round(2) %&gt;% \n  abs() %&gt;% \n  log(3) %&gt;% \n  abs() %&gt;% \n  sqrt()\n\n[1] 0.3846181",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#переименование-переменных",
    "href": "l2.html#переименование-переменных",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.14 Переименование переменных",
    "text": "2.14 Переименование переменных\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\n\n\nds %&gt;% \n  rename(city = var1,\n         age = var2)\n\n# A tibble: 7 × 6\n     id city    age var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\n\n\nds %&gt;% \n  set_names(vars(id, city, age, student, condition, score))\n\n# A tibble: 7 × 6\n  `~id` `~city` `~age` `~student` `~condition` `~score`\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;lgl&gt;      &lt;chr&gt;           &lt;dbl&gt;\n1     1 Abc          5 TRUE       cond1            12.8\n2     2 Def         16 FALSE      cond1            14.2\n3     3 Ghi         94 FALSE      cond2            32.5\n4     4 Jkl         28 FALSE      cond2             9.4\n5     5 Mno         11 TRUE       cond3             6.3\n6     6 Pqr        100 TRUE       cond3            11.7\n7     7 Stu         96 FALSE      cond1            95.5\n\n\n\nds %&gt;% \n  set_names(vars(id, city, age, student, condition, score)) %&gt;% \n  rename_all(str_remove, \"~\")\n\n# A tibble: 7 × 6\n     id city    age student condition score\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1     1 Abc       5 TRUE    cond1      12.8\n2     2 Def      16 FALSE   cond1      14.2\n3     3 Ghi      94 FALSE   cond2      32.5\n4     4 Jkl      28 FALSE   cond2       9.4\n5     5 Mno      11 TRUE    cond3       6.3\n6     6 Pqr     100 TRUE    cond3      11.7\n7     7 Stu      96 FALSE   cond1      95.5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#фильтрация-данных.-строки",
    "href": "l2.html#фильтрация-данных.-строки",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.15 Фильтрация данных. Строки",
    "text": "2.15 Фильтрация данных. Строки\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  filter(var5 &gt; 10)\n\n# A tibble: 5 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     6 Pqr     100 TRUE  cond3  11.7\n5     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  filter(var3)\n\n# A tibble: 3 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     5 Mno      11 TRUE  cond3   6.3\n3     6 Pqr     100 TRUE  cond3  11.7\n\nds %&gt;% slice(3:5)\n\n# A tibble: 3 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     3 Ghi      94 FALSE cond2  32.5\n2     4 Jkl      28 FALSE cond2   9.4\n3     5 Mno      11 TRUE  cond3   6.3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#фильтрация-данных.-столбцы",
    "href": "l2.html#фильтрация-данных.-столбцы",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.16 Фильтрация данных. Столбцы",
    "text": "2.16 Фильтрация данных. Столбцы\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  select(id, var1, var3, var4)\n\n# A tibble: 7 × 4\n     id var1  var3  var4 \n  &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt;\n1     1 Abc   TRUE  cond1\n2     2 Def   FALSE cond1\n3     3 Ghi   FALSE cond2\n4     4 Jkl   FALSE cond2\n5     5 Mno   TRUE  cond3\n6     6 Pqr   TRUE  cond3\n7     7 Stu   FALSE cond1\n\nds %&gt;% \n  select(starts_with(\"var\"))\n\n# A tibble: 7 × 5\n  var1   var2 var3  var4   var5\n  &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1 Abc       5 TRUE  cond1  12.8\n2 Def      16 FALSE cond1  14.2\n3 Ghi      94 FALSE cond2  32.5\n4 Jkl      28 FALSE cond2   9.4\n5 Mno      11 TRUE  cond3   6.3\n6 Pqr     100 TRUE  cond3  11.7\n7 Stu      96 FALSE cond1  95.5",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#сортировка-данных",
    "href": "l2.html#сортировка-данных",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.17 Сортировка данных",
    "text": "2.17 Сортировка данных\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  arrange(var5)\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     5 Mno      11 TRUE  cond3   6.3\n2     4 Jkl      28 FALSE cond2   9.4\n3     6 Pqr     100 TRUE  cond3  11.7\n4     1 Abc       5 TRUE  cond1  12.8\n5     2 Def      16 FALSE cond1  14.2\n6     3 Ghi      94 FALSE cond2  32.5\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  arrange(desc(var5))\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     7 Stu      96 FALSE cond1  95.5\n2     3 Ghi      94 FALSE cond2  32.5\n3     2 Def      16 FALSE cond1  14.2\n4     1 Abc       5 TRUE  cond1  12.8\n5     6 Pqr     100 TRUE  cond3  11.7\n6     4 Jkl      28 FALSE cond2   9.4\n7     5 Mno      11 TRUE  cond3   6.3\n\nds %&gt;% \n  distinct(var4, var3)\n\n# A tibble: 4 × 2\n  var4  var3 \n  &lt;chr&gt; &lt;lgl&gt;\n1 cond1 TRUE \n2 cond1 FALSE\n3 cond2 FALSE\n4 cond3 TRUE",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#создание-и-изменение-переменных",
    "href": "l2.html#создание-и-изменение-переменных",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.18 Создание и изменение переменных",
    "text": "2.18 Создание и изменение переменных\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  mutate(banch = 1)\n\n# A tibble: 7 × 7\n     id var1   var2 var3  var4   var5 banch\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8     1\n2     2 Def      16 FALSE cond1  14.2     1\n3     3 Ghi      94 FALSE cond2  32.5     1\n4     4 Jkl      28 FALSE cond2   9.4     1\n5     5 Mno      11 TRUE  cond3   6.3     1\n6     6 Pqr     100 TRUE  cond3  11.7     1\n7     7 Stu      96 FALSE cond1  95.5     1\n\nds %&gt;% \n  mutate(banch = 1,\n         var5_cat = ifelse(var5 &gt; mean(var5), \"high\", \"low\"),\n         var4 = recode(var4,\n                       \"cond1\" = \"easy\",\n                       \"cond2\" = \"medium\",\n                       \"cond3\" = \"hard\"))\n\n# A tibble: 7 × 8\n     id var1   var2 var3  var4    var5 banch var5_cat\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   \n1     1 Abc       5 TRUE  easy    12.8     1 low     \n2     2 Def      16 FALSE easy    14.2     1 low     \n3     3 Ghi      94 FALSE medium  32.5     1 high    \n4     4 Jkl      28 FALSE medium   9.4     1 low     \n5     5 Mno      11 TRUE  hard     6.3     1 low     \n6     6 Pqr     100 TRUE  hard    11.7     1 low     \n7     7 Stu      96 FALSE easy    95.5     1 high",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#группировка-и-аггрегация-данных",
    "href": "l2.html#группировка-и-аггрегация-данных",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.19 Группировка и аггрегация данных",
    "text": "2.19 Группировка и аггрегация данных\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  summarise(v5_mean = mean(var5),\n            v2_median = median(var2))\n\n# A tibble: 1 × 2\n  v5_mean v2_median\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    26.1        28\n\nds %&gt;% \n  group_by(var4) %&gt;% \n  summarise(n = n(),\n            v5_mean = mean(var5),\n            v2_median = median(var2))\n\n# A tibble: 3 × 4\n  var4      n v5_mean v2_median\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 cond1     3    40.8      16  \n2 cond2     2    21.0      61  \n3 cond3     2     9        55.5\n\nds %&gt;% \n  summarise(n = n(),\n            v5_mean = mean(var5),\n            v2_median = median(var2),\n            .by = var4)\n\n# A tibble: 3 × 4\n  var4      n v5_mean v2_median\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 cond1     3    40.8      16  \n2 cond2     2    21.0      61  \n3 cond3     2     9        55.5\n\n\n\nungroup()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#объединение-датасетов.-строки-и-столбцы",
    "href": "l2.html#объединение-датасетов.-строки-и-столбцы",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.20 Объединение датасетов. Строки и столбцы",
    "text": "2.20 Объединение датасетов. Строки и столбцы\n\na\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     3 Ghi      94\n4     4 Jkl      28\n\nb\n\n# A tibble: 4 × 3\n     id var3  var4 \n  &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;\n1     1 TRUE  cond1\n2     2 FALSE cond1\n3     3 FALSE cond2\n4     4 FALSE cond2\n\nc\n\n# A tibble: 3 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     5 Mno      11\n2     6 Pqr     100\n3     7 Stu      96\n\na %&gt;% \n  bind_cols(b)\n\nNew names:\n• `id` -&gt; `id...1`\n• `id` -&gt; `id...4`\n\n\n# A tibble: 4 × 6\n  id...1 var1   var2 id...4 var3  var4 \n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt;\n1      1 Abc       5      1 TRUE  cond1\n2      2 Def      16      2 FALSE cond1\n3      3 Ghi      94      3 FALSE cond2\n4      4 Jkl      28      4 FALSE cond2\n\na %&gt;% \n  bind_rows(c)\n\n# A tibble: 7 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     3 Ghi      94\n4     4 Jkl      28\n5     5 Mno      11\n6     6 Pqr     100\n7     7 Stu      96",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#объединение-датасетов.-ключ",
    "href": "l2.html#объединение-датасетов.-ключ",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.21 Объединение датасетов. Ключ",
    "text": "2.21 Объединение датасетов. Ключ\n\n\n\n\nСхема датасетов\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#inner_join",
    "href": "l2.html#inner_join",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.22 inner_join()",
    "text": "2.22 inner_join()\n\n\n\n\nСхема inner_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% inner_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     2 Def      16 cond1  14.2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#left_join",
    "href": "l2.html#left_join",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.23 left_join()",
    "text": "2.23 left_join()\n\n\n\n\nСхема left_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% left_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 4 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     2 Def      16 cond1  14.2\n3     5 Mno      11 &lt;NA&gt;   NA  \n4     7 Stu      96 &lt;NA&gt;   NA",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#right_join",
    "href": "l2.html#right_join",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.24 right_join()",
    "text": "2.24 right_join()\n\n\n\n\nСхема right_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% right_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 4 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     2 Def      16 cond1  14.2\n3     3 &lt;NA&gt;     NA cond2  32.5\n4     4 &lt;NA&gt;     NA cond2   9.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#full_join",
    "href": "l2.html#full_join",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.25 full_join()",
    "text": "2.25 full_join()\n\n\n\n\nСхема full_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% full_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 6 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     2 Def      16 cond1  14.2\n3     5 Mno      11 &lt;NA&gt;   NA  \n4     7 Stu      96 &lt;NA&gt;   NA  \n5     3 &lt;NA&gt;     NA cond2  32.5\n6     4 &lt;NA&gt;     NA cond2   9.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#semi_join",
    "href": "l2.html#semi_join",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.26 semi_join()",
    "text": "2.26 semi_join()\n\n\n\n\nСхема semi_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% semi_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n\ny %&gt;% semi_join(x)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#anti_join",
    "href": "l2.html#anti_join",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.27 anti_join()",
    "text": "2.27 anti_join()\n\n\n\n\nСхема anti_join()\n\n\n\n\n\nx\n\n# A tibble: 4 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     2 Def      16\n3     5 Mno      11\n4     7 Stu      96\n\ny\n\n# A tibble: 4 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     3 cond2  32.5\n4     4 cond2   9.4\n\nx %&gt;% anti_join(y)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     5 Mno      11\n2     7 Stu      96\n\ny %&gt;% anti_join(x)\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 2 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     3 cond2  32.5\n2     4 cond2   9.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#дублирование-ключа",
    "href": "l2.html#дублирование-ключа",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.28 Дублирование ключа",
    "text": "2.28 Дублирование ключа\n\n\n\n\nСхема работы .._join() при наличии дублей в колонке ключа\n\n\n\n\n\nx\n\n# A tibble: 5 × 3\n     id var1   var2\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5\n2     1 Abc       7\n3     2 Def      16\n4     5 Mno      11\n5     7 Stu      96\n\ny\n\n# A tibble: 5 × 3\n     id var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     2 cond2   2  \n4     3 cond2  32.5\n5     4 cond2   9.4\n\nx %&gt;% inner_join(y)\n\nJoining with `by = join_by(id)`\n\n\nWarning in inner_join(., y): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 4 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     1 Abc       7 cond1  12.8\n3     2 Def      16 cond1  14.2\n4     2 Def      16 cond2   2  \n\nx %&gt;% left_join(y)\n\nJoining with `by = join_by(id)`\n\n\nWarning in left_join(., y): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 6 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     1 Abc       7 cond1  12.8\n3     2 Def      16 cond1  14.2\n4     2 Def      16 cond2   2  \n5     5 Mno      11 &lt;NA&gt;   NA  \n6     7 Stu      96 &lt;NA&gt;   NA  \n\nx %&gt;% right_join(y)\n\nJoining with `by = join_by(id)`\n\n\nWarning in right_join(., y): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 6 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     1 Abc       7 cond1  12.8\n3     2 Def      16 cond1  14.2\n4     2 Def      16 cond2   2  \n5     3 &lt;NA&gt;     NA cond2  32.5\n6     4 &lt;NA&gt;     NA cond2   9.4\n\nx %&gt;% full_join(y)\n\nJoining with `by = join_by(id)`\n\n\nWarning in full_join(., y): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 3 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n# A tibble: 8 × 5\n     id var1   var2 var3   var4\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 cond1  12.8\n2     1 Abc       7 cond1  12.8\n3     2 Def      16 cond1  14.2\n4     2 Def      16 cond2   2  \n5     5 Mno      11 &lt;NA&gt;   NA  \n6     7 Stu      96 &lt;NA&gt;   NA  \n7     3 &lt;NA&gt;     NA cond2  32.5\n8     4 &lt;NA&gt;     NA cond2   9.4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#способы-объединения-по-ключу-на-диаграммах-венна",
    "href": "l2.html#способы-объединения-по-ключу-на-диаграммах-венна",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.29 Способы объединения по ключу на диаграммах Венна",
    "text": "2.29 Способы объединения по ключу на диаграммах Венна\n\n\n\n\nСхема работы .._join() на диаграммах Венна",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#широкий-и-длинный-форматы-данных",
    "href": "l2.html#широкий-и-длинный-форматы-данных",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.30 Широкий и длинный форматы данных",
    "text": "2.30 Широкий и длинный форматы данных\n\n\n\n\nПредставление одних и тех же данных в длинном и широком форматах\n\n\n\n\nlong\n\n# A tibble: 6 × 3\n     id var1   var2\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     2 cond1  14.2\n3     1 cond2  32.5\n4     2 cond2   9.4\n5     1 cond3   6.3\n6     2 cond3  11.7\n\nlong %&gt;% \n  pivot_wider(names_from = var1,\n              values_from = var2) -&gt; wide\nwide\n\n# A tibble: 2 × 4\n     id cond1 cond2 cond3\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  12.8  32.5   6.3\n2     2  14.2   9.4  11.7\n\nwide %&gt;% \n  pivot_longer(cols = -id) # names_to, values_to\n\n# A tibble: 6 × 3\n     id name  value\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 cond1  12.8\n2     1 cond2  32.5\n3     1 cond3   6.3\n4     2 cond1  14.2\n5     2 cond2   9.4\n6     2 cond3  11.7",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#строки",
    "href": "l2.html#строки",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.31 Строки",
    "text": "2.31 Строки\nДва специализированных пакета:\n\nstringr (является частью tidyverse)\nstringi (устанавливается отдельно)\n\n\n# install.packages(\"stringi\")\nlibrary(stringi)\n\n\n2.31.1 Как создать строку?\n\ns1 &lt;- \"сложившаяся структура организации влечет за собой процесс внедрения и модернизации новых предложений\"\ns1\n\n[1] \"сложившаяся структура организации влечет за собой процесс внедрения и модернизации новых предложений\"\n\n\n\ns2 &lt;- 'С другой стороны постоянный количественный \"рост\" и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации'\ns2\n\n[1] \"С другой стороны постоянный количественный \\\"рост\\\" и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"\n\n\n\ns3 &lt;- \"С другой стороны постоянный количественный \"рост\" и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"\ns3\n\nError in parse(text = input): &lt;text&gt;:1:52: unexpected symbol\n1: s3 &lt;- \"С другой стороны постоянный количественный \"рост\n                                                       ^\n\n\n\ns4 &lt;- \"С другой стороны постоянный количественный 'рост' и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"\ns4\n\n[1] \"С другой стороны постоянный количественный 'рост' и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"\n\n\n\ns5 &lt;- \"С другой стороны постоянный количественный «рост» и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"\ns5\n\n[1] \"С другой стороны постоянный количественный «рост» и сфера нашей активности позволяет выполнять важные задания по разработке соответствующий условий активизации\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#генерация-случайных-строк",
    "href": "l2.html#генерация-случайных-строк",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.32 Генерация случайных строк",
    "text": "2.32 Генерация случайных строк\n\nstri_rand_strings(n = 10, length = 5)\n\n [1] \"eZ1eh\" \"eyznU\" \"YTRXy\" \"Uz5d0\" \"aTh8T\" \"B5DRC\" \"2yzFP\" \"FDTuK\" \"tW43M\"\n[10] \"RMT92\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#операции-над-строками",
    "href": "l2.html#операции-над-строками",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.33 Операции над строками",
    "text": "2.33 Операции над строками\n\n2.33.1 Конкатенация строк\n\npaste('first', 'second', 'third')\n\n[1] \"first second third\"\n\n\n\npaste('first', 'second', 'third', sep = \"_\")\n\n[1] \"first_second_third\"\n\n\n\npaste0('first', 'second', 'third')\n\n[1] \"firstsecondthird\"\n\n\n\nstr_c('first', 'second', 'third')\n\n[1] \"firstsecondthird\"\n\n\n\nstr_c('first', 'second', 'third', sep = \"|\")\n\n[1] \"first|second|third\"\n\n\n\nstri_c('first', 'second', 'third')\n\n[1] \"firstsecondthird\"\n\n\n\nstri_c('first', 'second', 'third', sep = \" & \")\n\n[1] \"first & second & third\"\n\n\n\nds_1\n\n# A tibble: 150 × 4\n      id scale  item score\n   &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n 1     1 A         1     4\n 2     1 A         2     1\n 3     1 A         3     5\n 4     1 A         4     1\n 5     1 A         5     5\n 6     1 B         1     1\n 7     1 B         2     2\n 8     1 B         3     5\n 9     1 B         4     3\n10     1 B         5     5\n# ℹ 140 more rows\n\nds_1 %&gt;% \n  unite(scale_item, scale, item, sep = \"_\") -&gt; ds_2\nds_2\n\n# A tibble: 150 × 3\n      id scale_item score\n   &lt;int&gt; &lt;chr&gt;      &lt;int&gt;\n 1     1 A_1            4\n 2     1 A_2            1\n 3     1 A_3            5\n 4     1 A_4            1\n 5     1 A_5            5\n 6     1 B_1            1\n 7     1 B_2            2\n 8     1 B_3            5\n 9     1 B_4            3\n10     1 B_5            5\n# ℹ 140 more rows\n\n\n\n\n2.33.2 Разделение строк\n\nstr_split(\"first second third\", pattern = \" \")\n\n[[1]]\n[1] \"first\"  \"second\" \"third\" \n\n\n\nds_2 %&gt;% \n  separate(scale_item, into = c(\"scale\", \"item\"))\n\n# A tibble: 150 × 4\n      id scale item  score\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1     1 A     1         4\n 2     1 A     2         1\n 3     1 A     3         5\n 4     1 A     4         1\n 5     1 A     5         5\n 6     1 B     1         1\n 7     1 B     2         2\n 8     1 B     3         5\n 9     1 B     4         3\n10     1 B     5         5\n# ℹ 140 more rows\n\n\n\n\n2.33.3 Сортировка строк\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\n\n\nsort(unsorted_s)\n\n [1] \"Ccw\" \"Cka\" \"Ejt\" \"Eph\" \"Gbz\" \"Hfj\" \"Ixs\" \"Iyc\" \"Jgf\" \"Jhe\" \"Koi\" \"Nqr\"\n[13] \"Nvd\" \"Odl\" \"Ruu\" \"Snm\" \"Swx\" \"Szq\" \"Tmp\" \"Vin\" \"Vlo\" \"Yey\" \"Ysv\" \"Ytg\"\n[25] \"Zab\" \"Zrk\"\n\n\n\nstr_sort(unsorted_s)\n\n [1] \"Ccw\" \"Cka\" \"Ejt\" \"Eph\" \"Gbz\" \"Hfj\" \"Ixs\" \"Iyc\" \"Jgf\" \"Jhe\" \"Koi\" \"Nqr\"\n[13] \"Nvd\" \"Odl\" \"Ruu\" \"Snm\" \"Swx\" \"Szq\" \"Tmp\" \"Vin\" \"Vlo\" \"Yey\" \"Ysv\" \"Ytg\"\n[25] \"Zab\" \"Zrk\"\n\n\n\n# по умолчанию\nstr_sort(c(\"э\", \"а\", \"у\", \"i\"), locale = 'en')\n\n[1] \"i\" \"а\" \"у\" \"э\"\n\n\n\n# для русского языка\nstr_sort(c(\"э\", \"а\", \"у\", \"i\"), locale = 'ru')\n\n[1] \"а\" \"у\" \"э\" \"i\"\n\n\n\n\n2.33.4 Изменение строк\n\n2.33.4.1 Выделение подстроки\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\nstr_sub(unsorted_s, start = 1, end = 2)\n\n [1] \"Od\" \"Sn\" \"Nq\" \"Ck\" \"Jg\" \"Ru\" \"Vl\" \"Ko\" \"Ej\" \"Tm\" \"Nv\" \"Vi\" \"Ys\" \"Zr\" \"Ep\"\n[16] \"Sw\" \"Ye\" \"Yt\" \"Ix\" \"Cc\" \"Hf\" \"Za\" \"Gb\" \"Jh\" \"Iy\" \"Sz\"\n\n\n\n\n2.33.4.2 Замена подстроки\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\nstr_replace(unsorted_s, pattern = \"O\", replacement = \"Ь\")\n\n [1] \"Ьdl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\n\n\n\n2.33.4.3 Удаление подстроки\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\nstr_remove(unsorted_s, \"S\")\n\n [1] \"Odl\" \"nm\"  \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"wx\"  \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"zq\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#операции-над-строками-1",
    "href": "l2.html#операции-над-строками-1",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.34 Операции над строками",
    "text": "2.34 Операции над строками\n\n2.34.1 Разделение строк\n\nstr_split(\"first second third\", pattern = \" \")\n\n[[1]]\n[1] \"first\"  \"second\" \"third\" \n\n\n\nds_2 %&gt;% \n  separate(scale_item, into = c(\"scale\", \"item\"))\n\n# A tibble: 150 × 4\n      id scale item  score\n   &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n 1     1 A     1         3\n 2     1 A     2         1\n 3     1 A     3         1\n 4     1 A     4         1\n 5     1 A     5         5\n 6     1 B     1         4\n 7     1 B     2         3\n 8     1 B     3         3\n 9     1 B     4         3\n10     1 B     5         3\n# ℹ 140 more rows",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#сортировка-строк",
    "href": "l2.html#сортировка-строк",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.35 Сортировка строк",
    "text": "2.35 Сортировка строк\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\n\n\nsort(unsorted_s)\n\n [1] \"Ccw\" \"Cka\" \"Ejt\" \"Eph\" \"Gbz\" \"Hfj\" \"Ixs\" \"Iyc\" \"Jgf\" \"Jhe\" \"Koi\" \"Nqr\"\n[13] \"Nvd\" \"Odl\" \"Ruu\" \"Snm\" \"Swx\" \"Szq\" \"Tmp\" \"Vin\" \"Vlo\" \"Yey\" \"Ysv\" \"Ytg\"\n[25] \"Zab\" \"Zrk\"\n\n\n\nstr_sort(unsorted_s)\n\n [1] \"Ccw\" \"Cka\" \"Ejt\" \"Eph\" \"Gbz\" \"Hfj\" \"Ixs\" \"Iyc\" \"Jgf\" \"Jhe\" \"Koi\" \"Nqr\"\n[13] \"Nvd\" \"Odl\" \"Ruu\" \"Snm\" \"Swx\" \"Szq\" \"Tmp\" \"Vin\" \"Vlo\" \"Yey\" \"Ysv\" \"Ytg\"\n[25] \"Zab\" \"Zrk\"\n\n\n\n# по умолчанию\nstr_sort(c(\"э\", \"а\", \"у\", \"i\"), locale = 'en')\n\n[1] \"i\" \"а\" \"у\" \"э\"\n\n\n\n# для русского языка\nstr_sort(c(\"э\", \"а\", \"у\", \"i\"), locale = 'ru')\n\n[1] \"а\" \"у\" \"э\" \"i\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#изменение-строк",
    "href": "l2.html#изменение-строк",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.36 Изменение строк",
    "text": "2.36 Изменение строк\n\n2.36.1 Выделение подстроки\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\nstr_sub(unsorted_s, start = 1, end = 2)\n\n [1] \"Od\" \"Sn\" \"Nq\" \"Ck\" \"Jg\" \"Ru\" \"Vl\" \"Ko\" \"Ej\" \"Tm\" \"Nv\" \"Vi\" \"Ys\" \"Zr\" \"Ep\"\n[16] \"Sw\" \"Ye\" \"Yt\" \"Ix\" \"Cc\" \"Hf\" \"Za\" \"Gb\" \"Jh\" \"Iy\" \"Sz\"\n\n\n\n\n2.36.2 Замена подстроки\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\nstr_replace(unsorted_s, pattern = \"O\", replacement = \"Ь\")\n\n [1] \"Ьdl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\n\n\n\n2.36.3 Удаление подстроки\n\nunsorted_s\n\n [1] \"Odl\" \"Snm\" \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"Swx\" \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"Szq\"\n\nstr_remove(unsorted_s, \"S\")\n\n [1] \"Odl\" \"nm\"  \"Nqr\" \"Cka\" \"Jgf\" \"Ruu\" \"Vlo\" \"Koi\" \"Ejt\" \"Tmp\" \"Nvd\" \"Vin\"\n[13] \"Ysv\" \"Zrk\" \"Eph\" \"wx\"  \"Yey\" \"Ytg\" \"Ixs\" \"Ccw\" \"Hfj\" \"Zab\" \"Gbz\" \"Jhe\"\n[25] \"Iyc\" \"zq\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#регулярные-выражения",
    "href": "l2.html#регулярные-выражения",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.34 Регулярные выражения",
    "text": "2.34 Регулярные выражения\n\ndates &lt;- c('21.92.2001', '01.04.1994', '5-3-2011', '6/04/1999')\ndates\n\n[1] \"21.92.2001\" \"01.04.1994\" \"5-3-2011\"   \"6/04/1999\" \n\n\n\n2.34.1 Метасимволы\n\nstr_view(dates, pattern = \".\")\n\n[1] │ &lt;2&gt;&lt;1&gt;&lt;.&gt;&lt;9&gt;&lt;2&gt;&lt;.&gt;&lt;2&gt;&lt;0&gt;&lt;0&gt;&lt;1&gt;\n[2] │ &lt;0&gt;&lt;1&gt;&lt;.&gt;&lt;0&gt;&lt;4&gt;&lt;.&gt;&lt;1&gt;&lt;9&gt;&lt;9&gt;&lt;4&gt;\n[3] │ &lt;5&gt;&lt;-&gt;&lt;3&gt;&lt;-&gt;&lt;2&gt;&lt;0&gt;&lt;1&gt;&lt;1&gt;\n[4] │ &lt;6&gt;&lt;/&gt;&lt;0&gt;&lt;4&gt;&lt;/&gt;&lt;1&gt;&lt;9&gt;&lt;9&gt;&lt;9&gt;\n\n\n\nstr_view(dates, pattern = \"\\\\.\") # экранирование метасимволов\n\n[1] │ 21&lt;.&gt;92&lt;.&gt;2001\n[2] │ 01&lt;.&gt;04&lt;.&gt;1994\n\n\n\nstr_view(dates, pattern = \"^0\")\n\n[2] │ &lt;0&gt;1.04.1994\n\n\n\nstr_view(dates, pattern = \"9$\")\n\n[4] │ 6/04/199&lt;9&gt;\n\n\n\n\n2.34.2 Классы знаков\n\nstr_view(dates, pattern = '\\\\d') # ищем цифры\n\n[1] │ &lt;2&gt;&lt;1&gt;.&lt;9&gt;&lt;2&gt;.&lt;2&gt;&lt;0&gt;&lt;0&gt;&lt;1&gt;\n[2] │ &lt;0&gt;&lt;1&gt;.&lt;0&gt;&lt;4&gt;.&lt;1&gt;&lt;9&gt;&lt;9&gt;&lt;4&gt;\n[3] │ &lt;5&gt;-&lt;3&gt;-&lt;2&gt;&lt;0&gt;&lt;1&gt;&lt;1&gt;\n[4] │ &lt;6&gt;/&lt;0&gt;&lt;4&gt;/&lt;1&gt;&lt;9&gt;&lt;9&gt;&lt;9&gt;\n\n\n\nstr_view(dates, pattern = '\\\\D') # ищем не-цифры\n\n[1] │ 21&lt;.&gt;92&lt;.&gt;2001\n[2] │ 01&lt;.&gt;04&lt;.&gt;1994\n[3] │ 5&lt;-&gt;3&lt;-&gt;2011\n[4] │ 6&lt;/&gt;04&lt;/&gt;1999\n\n\n\nstr_view('успешный успех', '\\\\s') # пробелы\n\n[1] │ успешный&lt; &gt;успех\n\n\n\nstr_view('успешный успех', '\\\\S') # не-пробелы\n\n[1] │ &lt;у&gt;&lt;с&gt;&lt;п&gt;&lt;е&gt;&lt;ш&gt;&lt;н&gt;&lt;ы&gt;&lt;й&gt; &lt;у&gt;&lt;с&gt;&lt;п&gt;&lt;е&gt;&lt;х&gt;\n\n\n\nstr_view('верно ведь, что здесь что-то есть', '\\\\w') # не пробелы и не знаки препинания\n\n[1] │ &lt;в&gt;&lt;е&gt;&lt;р&gt;&lt;н&gt;&lt;о&gt; &lt;в&gt;&lt;е&gt;&lt;д&gt;&lt;ь&gt;, &lt;ч&gt;&lt;т&gt;&lt;о&gt; &lt;з&gt;&lt;д&gt;&lt;е&gt;&lt;с&gt;&lt;ь&gt; &lt;ч&gt;&lt;т&gt;&lt;о&gt;-&lt;т&gt;&lt;о&gt; &lt;е&gt;&lt;с&gt;&lt;т&gt;&lt;ь&gt;\n\n\n\nstr_view('верно ведь, что здесь что-то есть', '\\\\W') # пробелы и знаки препинания\n\n[1] │ верно&lt; &gt;ведь&lt;,&gt;&lt; &gt;что&lt; &gt;здесь&lt; &gt;что&lt;-&gt;то&lt; &gt;есть\n\n\n\n\n2.34.3 Квантификация\n\n? — ноль или один раз\n* — ноль или более раз\n+ — один или более раз\n{n} — n раз\n\n\nstr_view(dates, '\\\\d{2}')\n\n[1] │ &lt;21&gt;.&lt;92&gt;.&lt;20&gt;&lt;01&gt;\n[2] │ &lt;01&gt;.&lt;04&gt;.&lt;19&gt;&lt;94&gt;\n[3] │ 5-3-&lt;20&gt;&lt;11&gt;\n[4] │ 6/&lt;04&gt;/&lt;19&gt;&lt;99&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#регулярные-выражения-1",
    "href": "l2.html#регулярные-выражения-1",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.38 Регулярные выражения",
    "text": "2.38 Регулярные выражения\n\n2.38.1 Классы знаков\n\nstr_view_all(dates, pattern = '\\\\d') # ищем цифры\n\n[1] │ &lt;2&gt;&lt;1&gt;.&lt;9&gt;&lt;2&gt;.&lt;2&gt;&lt;0&gt;&lt;0&gt;&lt;1&gt;\n[2] │ &lt;0&gt;&lt;1&gt;.&lt;0&gt;&lt;4&gt;.&lt;1&gt;&lt;9&gt;&lt;9&gt;&lt;4&gt;\n[3] │ &lt;5&gt;-&lt;3&gt;-&lt;2&gt;&lt;0&gt;&lt;1&gt;&lt;1&gt;\n[4] │ &lt;6&gt;/&lt;0&gt;&lt;4&gt;/&lt;1&gt;&lt;9&gt;&lt;9&gt;&lt;9&gt;\n\n\n\nstr_view_all(dates, pattern = '\\\\D') # ищем не-цифры\n\n[1] │ 21&lt;.&gt;92&lt;.&gt;2001\n[2] │ 01&lt;.&gt;04&lt;.&gt;1994\n[3] │ 5&lt;-&gt;3&lt;-&gt;2011\n[4] │ 6&lt;/&gt;04&lt;/&gt;1999\n\n\n\nstr_view_all('успешный успех', '\\\\s') # пробелы\n\n[1] │ успешный&lt; &gt;успех\n\n\n\nstr_view_all('успешный успех', '\\\\S') # не-пробелы\n\n[1] │ &lt;у&gt;&lt;с&gt;&lt;п&gt;&lt;е&gt;&lt;ш&gt;&lt;н&gt;&lt;ы&gt;&lt;й&gt; &lt;у&gt;&lt;с&gt;&lt;п&gt;&lt;е&gt;&lt;х&gt;\n\n\n\nstr_view_all('верно ведь, что здесь что-то есть', '\\\\w') # не пробелы и не знаки препинания\n\n[1] │ &lt;в&gt;&lt;е&gt;&lt;р&gt;&lt;н&gt;&lt;о&gt; &lt;в&gt;&lt;е&gt;&lt;д&gt;&lt;ь&gt;, &lt;ч&gt;&lt;т&gt;&lt;о&gt; &lt;з&gt;&lt;д&gt;&lt;е&gt;&lt;с&gt;&lt;ь&gt; &lt;ч&gt;&lt;т&gt;&lt;о&gt;-&lt;т&gt;&lt;о&gt; &lt;е&gt;&lt;с&gt;&lt;т&gt;&lt;ь&gt;\n\n\n\nstr_view_all('верно ведь, что здесь что-то есть', '\\\\W') # пробелы и знаки препинания\n\n[1] │ верно&lt; &gt;ведь&lt;,&gt;&lt; &gt;что&lt; &gt;здесь&lt; &gt;что&lt;-&gt;то&lt; &gt;есть",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#регулярные-выражения-2",
    "href": "l2.html#регулярные-выражения-2",
    "title": "2  L2 // Предобработка данных. Дата и время Визуализация данных",
    "section": "2.39 Регулярные выражения",
    "text": "2.39 Регулярные выражения\n\n2.39.1 Квантификация\n\n? — ноль или один раз\n* — ноль или более раз\n+ — один или более раз\n{n} — n раз\n\n\nstr_view_all(dates, '\\\\d{2}')\n\n[1] │ &lt;21&gt;.&lt;92&gt;.&lt;20&gt;&lt;01&gt;\n[2] │ &lt;01&gt;.&lt;04&gt;.&lt;19&gt;&lt;94&gt;\n[3] │ 5-3-&lt;20&gt;&lt;11&gt;\n[4] │ 6/&lt;04&gt;/&lt;19&gt;&lt;99&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#дата-и-время",
    "href": "l2.html#дата-и-время",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.35 Дата и время",
    "text": "2.35 Дата и время\n\n2.35.1 Почему это особый тип данных?\n\nГод ≠ 365 дней: високосные года\nСутки ≠ 24 часа: переход на зимнее и летнее время\nМинута ≠ 60 секунд: компенсация замедления вращения земли (30 июня 23:59:60 или 31 декабря 23:59:60)\n\nВсе это автоматически обрабатывает lubridate.\n\ntoday()\n\n[1] \"2025-02-13\"\n\n\n\nnow()\n\n[1] \"2025-02-13 16:01:45 MSK\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#форматы-даты-и-времени",
    "href": "l2.html#форматы-даты-и-времени",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.36 Форматы даты и времени",
    "text": "2.36 Форматы даты и времени\n\nyyyy-mm-dd — является международным стандартом\n\nв таком формате с датой можно работать как со строкой, что оказывается удобно для баз данных\n\ndd/mm/yy, dd/mm/yyyy, dd.mm.yyyy — используется в Европе\nmm/dd/yy, mm/dd/yyyy — используется в США\nUnix-timestamp (число) — количество секунд с 01.01.1970, используется в базах данных",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#операции-над-датами",
    "href": "l2.html#операции-над-датами",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.37 Операции над датами",
    "text": "2.37 Операции над датами\n\n## текущий timestamp\nas.numeric(now())\n\n[1] 1739451706\n\n\n\nyear(now())\n\n[1] 2025\n\nmonth(now())\n\n[1] 2\n\nday(now())\n\n[1] 13\n\nhour(now())\n\n[1] 16\n\nminute(now())\n\n[1] 1\n\nsecond(now())\n\n[1] 45.7651\n\ndifftime(ymd_hm(\"2023-01-21, 21:00\"), ymd_hm(\"2023-01-21, 18:10\"), units = \"mins\")\n\nTime difference of 170 mins",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#зачем-нужна-визуализация",
    "href": "l2.html#зачем-нужна-визуализация",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.38 Зачем нужна визуализация?",
    "text": "2.38 Зачем нужна визуализация?\n\n2.38.1 Квартет Анскомба\n\n\nspc_tbl_ [44 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ id     : num [1:44] 1 1 1 1 2 2 2 2 3 3 ...\n $ dataset: num [1:44] 1 2 3 4 1 2 3 4 1 2 ...\n $ x      : num [1:44] 10 10 10 8 8 8 8 8 13 13 ...\n $ y      : num [1:44] 8.04 9.14 7.46 6.58 6.95 8.14 6.77 5.76 7.58 8.74 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   id = col_double(),\n  ..   dataset = col_double(),\n  ..   x = col_double(),\n  ..   y = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n# A tibble: 4 × 7\n  dataset     n mean_x mean_y  sd_x  sd_y   cor\n    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1       1    11      9    7.5  3.32  2.03  0.82\n2       2    11      9    7.5  3.32  2.03  0.82\n3       3    11      9    7.5  3.32  2.03  0.82\n4       4    11      9    7.5  3.32  2.03  0.82\n\n\nНо если это нарисовать, получим такое:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n2.38.2 Datasaurus\n\n\nspc_tbl_ [1,846 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ dataset: chr [1:1846] \"dino\" \"dino\" \"dino\" \"dino\" ...\n $ x      : num [1:1846] 55.4 51.5 46.2 42.8 40.8 ...\n $ y      : num [1:1846] 97.2 96 94.5 91.4 88.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   dataset = col_character(),\n  ..   x = col_double(),\n  ..   y = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n# A tibble: 13 × 7\n   dataset        n mean_x mean_y  sd_x  sd_y   cor\n   &lt;chr&gt;      &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 away         142   54.3   47.8  16.8  26.9  -0.1\n 2 bullseye     142   54.3   47.8  16.8  26.9  -0.1\n 3 circle       142   54.3   47.8  16.8  26.9  -0.1\n 4 dino         142   54.3   47.8  16.8  26.9  -0.1\n 5 dots         142   54.3   47.8  16.8  26.9  -0.1\n 6 h_lines      142   54.3   47.8  16.8  26.9  -0.1\n 7 high_lines   142   54.3   47.8  16.8  26.9  -0.1\n 8 slant_down   142   54.3   47.8  16.8  26.9  -0.1\n 9 slant_up     142   54.3   47.8  16.8  26.9  -0.1\n10 star         142   54.3   47.8  16.8  26.9  -0.1\n11 v_lines      142   54.3   47.8  16.8  26.9  -0.1\n12 wide_lines   142   54.3   47.8  16.8  26.9  -0.1\n13 x_shape      142   54.3   47.8  16.8  26.9  -0.1\n\n\nОднако если это визуализировать, то будет вот что:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#виды-графиков",
    "href": "l2.html#виды-графиков",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.39 Виды графиков",
    "text": "2.39 Виды графиков\n\n2.39.1 Столбчатая диаграмма (Bar plot, Bar graph)\n\n\n\n\n\n\n\n\n\n\n\n2.39.2 Лучевая диаграмма (Sunburts)\n\n\n\n\n\n\n\n\n\n\n\n2.39.3 Круговая диаграмма (Pie chart)\n\n\n\n\n\n\n\n\n\n\n\n2.39.4 Линейная диаграмма (Line graph, Line plot)\n\n\n\n\n\n\n\n\n\n\n\n2.39.5 Гистограмма (Histogram)\n\n\n\n\n\n\n\n\n\n\n\n2.39.6 График плотности распределения (Density plot)\n\n\n\n\n\n\n\n\n\n\n\n2.39.7 Dot plot\n\n\n\n\n\n\n\n\n\n\n\n2.39.8 Ящик с усами (Boxplot)\n\n\n\n\n\n\n\n\n\n\n\n2.39.9 Violin plot\n\n\n\n\n\n\n\n\n\n\n\n2.39.10 График интервальных оценок (Error bar)\n\n\n\n\n\n\n\n\n\n\n\n2.39.11 Диаграмма рассеяния (Scatter plot)\n\n\n\n\n\n\n\n\n\n\n\n2.39.12 Пузырьковая диаграмма (Bubble plot)\n\n\n\n\n\n\n\n\n\n\n\n2.39.13 Корреляционная матрица (Corrplot)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#good-bad-practices-в-визуализации",
    "href": "l2.html#good-bad-practices-в-визуализации",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.40 Good & Bad practices в визуализации",
    "text": "2.40 Good & Bad practices в визуализации\n\n2.40.1 Хорошо\n\nПодписать оси так, чтобы было понятно, что они обозначают\nИспользовать контрастную палитру цветов (или обойтись черно-белой)\nПроверить, как видят график люди с цветовыми аномалиями зрения\nВыстроить из визуализаций историю\n\n\n\n2.40.2 Плохо\n\nИспользовать разные диапазоны шкал или разный масштаб на графиках, которые необходимо сравнивать\nСтроить столбчатую диаграмму не от нуля\nСтроить визуализацию в 3D на бумаге\nСтроить круговую диаграму\nСтроить круговую диаграмму в 3D\n\nБольше примеров странных визуализаций тут.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#философия-a-layered-grammar-of-graphics",
    "href": "l2.html#философия-a-layered-grammar-of-graphics",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.41 Философия A Layered Grammar of Graphics",
    "text": "2.41 Философия A Layered Grammar of Graphics\n\nГрафик состоит из нескольких независимых друг от друга элементов\n\nосновы графика (фон и оси)\nсистемы координат\nспособов отображения данных (геометрических объектов)\nшкал\nфасетов\n\nОднотипные элементы графика располагаются на отдельных слоях\nВычисления отделены от визуализации\nДизайн отделен от содержания\n\nЭти принципы легли в основу пакета ggplot2.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l2.html#итоги",
    "href": "l2.html#итоги",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.42 Итоги",
    "text": "2.42 Итоги\n\nФорматы файлов данных бывают разные и их нужно по-разному загружать в R\nRStudio Projects — это хорошо и помогает воспроизводимости\nПредобработка данных — это определенный набор операций, которые в tidyverse достаточно интуитивно реализованы\nСтроки и даты требуют к себе особого внимания\nВизуализаций много, они разнообразны и каждая по-своему хороша",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  },
  {
    "objectID": "l3.html",
    "href": "l3.html",
    "title": "3  L3 // Математика для анализа данных",
    "section": "",
    "text": "3.1 Дискретная математика",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>L3 // Математика для анализа данных</span>"
    ]
  },
  {
    "objectID": "l3.html#дискретная-математика",
    "href": "l3.html#дискретная-математика",
    "title": "3  L3 // Математика для анализа данных",
    "section": "",
    "text": "3.1.1 Алгебра логики\nЛогика исследует закономерности мышления, но делает это отлично от того, как этим занимается, например, психология. Формальная логика абстрагируется от связей мышления с какими-либо сторонами сознания и сосредотачивается на логической непротиворечивости и последовательности мышления. Таким образом, формальная логика — это наука об общих структурах правильного мышления в его языковой форме (Зегет 1985).\nЛогика — это нормативная наука, то есть она определяет, как оно должно быть, в то время как, например, психология исследует как оно есть и как и почему логические законы нарушаются.\n\n3.1.1.1 Высказывания\nЛогика как наука имеет дело, прежде всего, с высказываниями. Высказывание отражает определённую объективную1 связь между предметами. Высказывание истинно, если в нём содержится адеквантное отражение этой связи — в ином случае высказывание ложно. В общем случае высказывание существует с форме повествовательного предложения.\nНапример, высказывание «Земля вращается вокруг Солнца» отражает объективное отношение, поэтому оно истинно, а высказывание «страницы этой книги зелёного цвета» не адекватно отражает существующее положение дел, поэтому оно ложно.\nПрежде всего, нас будут интересовать атомарные высказывания. Это элементарные высказывания, которые невозможно разделить на составляющие — более мелкие высказывания. Например, «четыре — это целое число» — это атомарное высказывание.\nАтомарные высказывания могут быть либо истинны, либо ложны. Почему? Потому что мы находимся в рамках двузначной логики2. Атомарные высказывания могут быть обозначены пропозициональными переменными. Так же как и числа в математике могут быть заменены буквеными обозначениями для абстрагирования от значения числа, так же и высказывания заменяются переменными для абстрагирования от содержания высказывания.\nДля обозначения пропозициональных переменных используются латинские буквы. А так как само высказывания имеет определённое значение истинности (истина и ложь), то и переменная, которой мы обозначаем это высказывание, также будет обладать этим же значением истинности. Всё аналогично математике.\n\n\n3.1.1.2 Логические операции\nС атомарными высказываниями можно выполнять различные логические операции.\n\n3.1.1.2.1 Инверсия\nСамая простая операция — инверсия, или отрицание. Оно обозначается с помощью оператора \\(\\neg\\). Это унарная операция, то есть она применяется к одной переменной. При отрицании значение истинности высказывания изменяется на противоположное, поэтому мы можно составить следующую таблицу истинности для отрицания:\n\n\n\n\\(p\\)\n\\(\\neg p\\)\n\n\n\n\nистина\nложь\n\n\nложь\nистина\n\n\n\nВ данном случае с помощью переменной \\(p\\) обозначено некоторое атомарное высказывание.\nОсобо стоит отметить, что при отрицании отрицается всё высказывание целиком, а не какой-то отдельный его элемент. То есть формально правильным вариантом отрицания высказывания «все лебеди белые» будет следующий — «неверно, что все лебеди белые».\nГрафически инверсия отображается так:\n\n\n\n\nЛогическое отрицание\n\n\n\n\nИз атомарных высказываний можно составлять сложные высказывания при помощи логических операторов. Например, высказывание «если четыре делится на два, то четыре — чётное число» является сложным, посколько состоит из двух атомарных — «четыре делится на два» и «четыре — чётное число» — соединённых союзом «если…, то…».\nДалее мы пристумаем с знакомству с бинарными операторами, то есть такими, которые функционируют на двух аргументах.\n\n\n3.1.1.2.2 Конъюнкция\nКонъюнкция (логическое умножение, логические И) представляет собой такое высказывание, которое наиболее точно передается следующей конструкцией естественного языка — «как \\(p\\), так и \\(q\\)».\n\\(p\\) и \\(q\\) в данном случае пропозициональные переменные, которые заменяют конкретные высказывания. Конъюнкция истинна тогда и только тогда, когда обе пропозициональные переменные, входящие в её состав, имеют значение истинности «истина». В любом ином случае конъюнкция ложна.\nКонъюнкция обозначается символом \\(\\wedge\\) и имеет следующую таблицу истинности:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\wedge q\\)\n\n\n\n\nистина\nистина\nистина\n\n\nистина\nложь\nложь\n\n\nложь\nистина\nложь\n\n\nложь\nложь\nложь\n\n\n\nГрафически конъюнкция отображается так:\n\n\n\n\nКонъюнкция (логическое И)\n\n\n\n\n\n3.1.1.2.3 Дизъюнкция\nДизъюнкция (логическое сложение, логические ИЛИ) представляет собой такое высказывание, которое наиболее точно передается следующей конструкцией естественного языка — «или \\(p\\), или \\(q\\), или и то и другое», поэтому дизъюнкция истинна тогда, когда хотя бы одна пропозициональная переменная, входящая в её состав, имеет значении истинности «истина». В случае, если оба высказывания ложны, дизъюнкция будет ложна.\nДизъюнкция обозначается символом \\(\\vee\\) и имеет следующую таблицу истинности:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\vee q\\)\n\n\n\n\nистина\nистина\nистина\n\n\nистина\nложь\nистина\n\n\nложь\nистина\nистина\n\n\nложь\nложь\nложь\n\n\n\nГрафически дизъюнкция отображается так:\n\n\n\n\nДизъюнкция (логическое ИЛИ)\n\n\n\n\n\n3.1.1.2.4 Разделительная дизъюнкция\nРазделительная дизъюнкция (исключающее ИЛИ) — это такое высказывание, которое наиболее полно описывается следующим выражением естественного языка — «либо \\(p\\), либо \\(q\\)». На её графическом представлении хорошо видно, чем она отличается от обычной дизъюнкции — она исключает ту часть пространства, где верны оба высказывания:\n\n\n\n\nРазделительная дизъюнкция (исключающее ИЛИ)\n\n\n\nДля обозначения разделительно дизъюнкции есть много различных операторов, но мы будем записывать её так — \\(p \\,\\text{XOR}\\,q\\). По иллюстрации можно восстановить таблицу истинности для этого оператора:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\,\\text{XOR}\\,q\\)\n\n\n\n\nистина\nистина\nложь\n\n\nистина\nложь\nистина\n\n\nложь\nистина\nистина\n\n\nложь\nложь\nложь\n\n\n\nТаким образом, видно, что исключающее ИЛИ истинно тогда, когда значения истинности пропозициональных переменных, входящих в сложное высказывание, различны.\n\n\n\n3.1.1.3 Условные высказывания\n\n3.1.1.3.1 Импликация\nСложное высказывание, описываемое конструкцией естественного языка «если \\(p\\), то \\(q\\)» в формальной логике носит название импликации. Она отражает следование одного утверждения из другого и обозначается следующим образом — \\(p \\rightarrow q\\). Высказывание \\(p\\) называется антецедентом импликации, а \\(q\\) — консеквентом импликации.\nИмпликация имеет следующую таблицу истинности:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\rightarrow q\\)\n\n\n\n\nистина\nистина\nистина\n\n\nистина\nложь\nложь\n\n\nложь\nистина\nистина\n\n\nложь\nложь\nистина\n\n\n\nКак видно из таблицы, импликация ложна только тогда, когда её антецедент истинен, а консеквент — ложен. Но что более интересно, так это то, что, согласно таблице, из ложного утверждения может следовать любое. Это факт мы вспомним, когда будем обсуждать тестирование статистических гипотез.\nИмпликация утвержает то же самое, что и следующее сложное высказывание — \\(\\neg (p \\wedge \\neg q)\\). Отсюда можно получить графическое изображение импликации:\n\n\n\n\nИмпликация \\(p \\rightarrow q\\)\n\n\n\n\n\n3.1.1.3.2 Репликация\nРепликация похожа на импликацию, но действует как бы в обратном направлении, что отражено в её обозначении — \\(p \\leftarrow q\\). Эта конструкция читается как «\\(p\\) реплицирует \\(q\\)» и является эквивалентом естественноязыкового «только если \\(p\\), то \\(q\\)». Соответствующим образом изменяется и таблица истиности:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\leftarrow q\\)\n\n\n\n\nистина\nистина\nистина\n\n\nистина\nложь\nистина\n\n\nложь\nистина\nложь\n\n\nложь\nложь\nистина\n\n\n\nИдея здесь в том, что если мы получили в результате истину, то лжи в начале быть не могло. Графическое изображение репликации выглядит так:\n\n\n\n\nРепликация \\(p \\leftarrow q\\)\n\n\n\n\n\n3.1.1.3.3 Эквиваленция\nЕсли мы соединим с помощью конъюнкции импликацию и репликацию, то есть запишем вот такое высказывание — \\((p \\rightarrow q) \\wedge (p \\leftarrow q)\\) — то получим эквиваленцию. По своей сути она является логическим отражением языковой конструкции «\\(q\\) только тогда, когда \\(p\\)», поэтому она обозначается вот так — \\(p \\leftrightarrow q\\) — и её таблица истинности выглядит соответствующим образом:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\leftrightarrow q\\)\n\n\n\n\nистина\nистина\nистина\n\n\nистина\nложь\nложь\n\n\nложь\nистина\nложь\n\n\nложь\nложь\nистина\n\n\n\nИз неё выводится графическое представление эквиваленции:\n\n\n\n\nЭквиваленция \\(p \\leftrightarrow q\\)\n\n\n\n\n\n\n\n3.1.2 Элементы теории множеств\nНа базе теории множеств стоит вся современная математика. Мы ознакомимся только c некоторыми концепциями этого раздела математики, но вообще полезно с ним познакомиться гораздо глубже.\n\n3.1.2.1 Множество\nПонятие множества неопределимо. По крайней мере силами самой теории множеств. Но мы будем понимать под множеством совокупность, или набор, некоторых (в общем случае любых) объектов. Это могут быть числа, буквы, точки и любые другие объекты. Объекты, входящие в состав множества, называются элементами этого множества.\nМножества обозначают заглавными латинскими буквами (например, \\(A\\)), а его элементы — прописными латинскими буквами (например, \\(a_1\\), \\(a_2\\) и т. д.).\nМножества удобно изображать кружочками. Примерно так:\n\n\n\n\nМножество \\(А\\), содержащее элементы \\(a_1\\), \\(a_2\\), \\(a_3\\), \\(a_4\\), \\(a_5\\)\n\n\n\nЕсли элемент входит в данное множество, то мы говорим, что этот элемент принадлежит данному множеству, и записываем это следующим образом:\n\\[ a_i \\in A \\]\nСимвол \\(\\in\\) читается как «принадлежит».\nЕсли мы хотим задать множество через перечисление элементов, то можно это сделать с помощью фигурных скобок вот так:\n\\[ B = \\{ 0,1,2,3,4,5 \\} \\]\nВ данном случае множество \\(B\\) содержит 6 элементов — числа от нуля до пяти.\nПриведём примеры множеств.\n\nМножество букв русского алфавита: \\(L = \\{ а, б, в, г, д, \\dots, э, ю, я \\}\\)\nМножество всех натуральных чисел: \\(\\mathbb{N}= \\{ 0, 1, 2, 3, \\dots \\}\\)\nМножество всех целых чисел: \\(\\mathbb{Z}= \\{0, 1, -1, 2, -2, 3, -3, \\dots \\}\\)\n\nТакже из числовых множеств мы можем вспомнить рациональные числа \\(\\mathbb{Q}\\), действительные (вещественные) числа \\(\\mathbb{R}\\) и комплексные числа \\(\\mathbb{C}\\).\nМы можем взять и рассмотреть не все элементы какого-то множества, а какую-то их часть. Например, взять элементы \\(a_1\\) и \\(a_2\\) и объединить их в множество поменьше.\n\n\n\n\nМножество \\(А_1\\), состоящее из элементов \\(a_1\\) и \\(a_2\\), является подмножеством множества \\(A\\)\n\n\n\nМы получим множество \\(A_1 = \\{a_1, a_2\\}\\), которое является подмножеством множества \\(A\\). Иначе говоря, множество \\(A_1\\) включается во множество \\(A\\):\n\\[\nA_1 \\subset A\n\\]\nВ частности, множество натуральных чисел включается во множество целых \\(\\mathbb{N}\\subset \\mathbb{Z}\\). А если продолжить эту цепочку, то можно получить следующую иерархию числовых множеств:\n\\[\n\\mathbb{N}\\subset \\mathbb{Z}\\subset \\mathbb{Q}\\subset \\mathbb{R}\\subset \\mathbb{C}\n\\]\nТривиальными подмножествами любого множества является пустое множество \\(\\varnothing\\) и само это множество. Пусть \\(M\\) — любое множество. Тогда можно записать два справедливых утверждения:\n\\[\n\\varnothing \\subset M\n\\]\n\\[\nM \\subset M\n\\]\n\n\n3.1.2.2 Операции над множествами\n\n3.1.2.2.1 Объединение\nНад множествами можно производить определённые операции. Во-первых, множества можно складывать, или объединять:\n\\[\nA + B = A \\cup B = \\{ x : x \\in A \\vee x \\in B \\}\n\\]\nТогда в новом множестве окажутся все элементы обоих исходных множеств.\n\n\n\n\nОбъединение множеств \\(A\\) и \\(B\\)\n\n\n\n\n\n3.1.2.2.2 Пересечение\nВо-вторых, множества можно умножать, или находить их пересечение:\n\\[\nA \\cdot B = A \\cap B = \\{ x : x \\in A \\wedge x \\in B \\}\n\\]\nТогда в новом множестве окажутся те элементы, которые принадлежат обоим множествам сразу.\n\n\n\n\nПересечение множеств \\(A\\) и \\(B\\)\n\n\n\n\n\n3.1.2.2.3 Разность множеств\nВ-третьих, можно искать разность множеств:\n\\[\nA ∖ B = \\{ x : x \\in A \\wedge x \\notin B \\}\n\\]\n\n\n\n\nРазность множеств \\(A\\) и \\(B\\)\n\n\n\nВ частности, если мы будем искать разность между универсумом \\(U\\) — множеством, которое содержит вообще все возможные элементы — и множеством \\(A\\), мы получим дополнение множества \\(A\\):\n\\[\nU ∖ A = A^c = \\bar A = \\{ x : x \\notin A \\}\n\\]\n\n\n\n\nДополнение множества \\(A\\)\n\n\n\n\n\n3.1.2.2.4 Симметрическая разность\nА ещё можно вычитать множества друг из друга, то есть искать их симметрическую разность:\n\\[\nA \\, \\Delta \\, B = ( A ∖ B ) \\cup ( B ∖ A ) = \\{ x : x \\in A \\text{XOR} x \\in B \\}\n\\]\n\n\n\n\nСимметрическая разность множеств \\(A\\) и \\(B\\)\n\n\n\n\n\n3.1.2.2.5 Декартово произведение\nНу, и самое красивое — декартово произведение двух множеств. Пусть у нас есть два множества \\(A\\) и \\(B\\). Тогда их декартово произведение представляет собой множество всех возможных упорядоченных пар \\((a,b), a \\in A, b \\in B\\).\n\\[\nA \\times B = \\{ (a,b) : a \\in A, b \\in B \\}\n\\]\n\n\n\n\nДекартово произведение множеств \\(A\\) и \\(B\\)\n\n\n\nУпорядоченность подразумевает, что если мы будем перемножать \\(A \\times B\\), то будут получаться пары \\((a,b)\\), а если \\(B \\times A\\), то пары \\((b,a)\\).\nК слову, вспомните координатную плоскость — это ни что иное, как декартово произведение двух координатных прямых: \\(\\mathbb{R}\\times \\mathbb{R}= \\mathbb{R}^2 = \\{ (x, y) : x \\in \\mathbb{R}, y \\in \\mathbb{R}\\}\\).\n\n\n\n3.1.2.3 Отображения\nМы можем сопоставлять элементы много множества элементам другого. Тогда мы получим отображение. Например, мы можем взять множество букв латинского алфавита и сопоставить каждому элементу этого множества число, которое будет соответствовать позиции буквы в алфавите. Пусть у нас есть множество букв латинского алфавита \\(L\\):\n\\[\nL = \\{ \\text{a}, \\text{b}, \\text{c}, \\text{d},\\dots, \\text{x}, \\text{y}, \\text{z} \\}\n\\]\nи множество натуральных чисел от 1 до 26 \\(N\\):\n\\[\nN = \\{ 1,2,3, \\dots,24,25,26 \\}\n\\]\nТогда мы можем задать такое отображение \\(F\\)\n\\[\nF : L \\rightarrow N,\n\\]\nгде каждой букве будет соответствовать её порядковый номер в алфавите.\nМы обозначили отображение буквой \\(F\\), которой в математике часто обозначают функции — это не случайно. Не вдаваясь в детали, можно сказать, что термины «отображение» и «функция» — синонимы. Ведь по сути что делает [математическая] функция? Сопоставляет между собой значения числовых множеств. То есть отображает одно числовое множество в другое. Множество \\(L\\) в примере выше будет областью определения функции (domain), а множество \\(N\\) — множеством её значений (range)3.\nНапример, простая функция \\(y = x\\), \\(x \\in \\mathbb{R}\\), \\(y \\in \\mathbb{R}\\) отображает множество вещественных чисел в само себя — \\(F : \\mathbb{R}\\rightarrow \\mathbb{R}\\). Функция модуля \\(y = |x|\\) отображает множество вещественных чисел во множество положительных вещественных чисел — \\(F : \\mathbb{R}\\rightarrow \\mathbb{R}_{+}\\). И так далее.\nВ случае с буквами мы могли бы задать функцию \\(f(l) = n\\), \\(l \\in L\\), \\(n \\in N\\) которая возвращала бы следующие результаты:\n\\[\nf(\\text{a}) = 1, \\quad f(\\text{b}) = 2, \\quad f(\\text{z}) = 26\n\\]\n\n\n\n\nОтображение \\(F : L \\rightarrow N\\)\n\n\n\n\n\n3.1.2.4 Мощность множества\nМножества могут содержать разное количество элементов. Характеристика, описывающая, сколько элементов содержит данное множество, называется мощностью множества.\nВо-первых, множества могут быть конечными и бесконечными.\n\nЕсли множество конечно, то его мощность равна количеству его элементов.\n\nНапример, множество очков, которое может выпаcть на стандартном игральном кубике — это \\(S_1 = \\{1,2,3,4,5,6\\}\\). Его мощность равна 6 — \\(|S_1| = 6\\).\nМножество значений пятибалльной шкалы Ликерта — это \\(S_2 = \\{1, 2, 3, 4, 5\\}\\). Его мощность равна 5 — \\(|S_2| = 5\\).\n\nЕсли множество бесконечно, то надо понять, насколько оно бесконечно.\n\nБесконечности бывают разного размера. В детали мы погружать не будем, однако отметим, что есть два вида бесконечностей.\n\nЕсли можно построить отображение, в котором каждом элементу некоторого множества \\(S\\) будет сопоставлено единственное натуральное число4, то такое множество называется счётным.\n\nЭто значит, что элементов во множестве \\(S\\) бесконечное количество — так как количество натуральных чисел бесконечно — однако при неограниченном количестве времени их все-таки можно пересчитать.\nМощность такого множества обозначается \\(\\aleph_0\\), то есть \\(|\\mathbb{N}| = \\aleph_0\\).\n\nЕсли количество элементов множества больше количества натуральных чисел, то такое множество обладает мощностью континуума \\(\\aleph_1\\).\n\nЭто множество будет равномощно множеству вещественных чисел \\(\\mathbb{R}\\).\n\n\nВозможно, это звучит весьма контринтуитивно — как одна бесконечность может быть больше другой? — однако это так: вещественных чисел больше, чем натуральных.\n\n\n\n3.1.3 Элементы комбинаторики\n\n3.1.3.1 Перестановки\nПредставим такую задачу: на черной пятнице мы накупили книг по анализу данных, и нам необходимо расставить их на полке. Всего у нас есть пять книг. Сколькими способами мы это сможем сделать?\nПодойдём к вопросу технически: возьмем и начнём расставлять. На первое место мы можем поставить любую из пяти книг, то есть вариантов заполнить первое место на полке — пять штук. Когда первое место заполнено, то вариантов заполнить второе место остаётся четыре. Всего возможных вариантов заполнения первых двух мест получается \\(5 \\times 4\\).\nЗаполняем далее: на третье место претендуют три оставшиеся книги, то есть вариантов, которыми мы можем расставить три книги — \\(5 \\times 4 \\times 3\\).\nСледуя далее этой логике мы получим, что всего возможных вариантов расставить все пять книг на полке будет\n\\[\n5 \\times 4 \\times 3 \\times 2 \\times 1 = 5!\n\\]\nМы получили формулу числа перестановок из \\(n\\) элементов:\n\\[\nP_n = n!\n\\]\nТо есть любые \\(n\\) объектов можно расставить на \\(n\\) мест \\(n!\\) способами.\n\n\n3.1.3.2 Размещения\n\n3.1.3.2.1 Без повторений\nТеперь представим, что у нас очень маленькая полка, и на ней умещается всего три книги. Сколькими способами мы можем заполнить такую полку, если всего в нашем распоряжении пять книг?\nВсего перестановок из пяти книг \\(5!\\), однако в силу того, что на полку умещаются только три первые книги из каждой перестановки, отличных друг от друга вариантов теперь будет меньше. Во сколько раз? В число раз, равно количеству перестановок из тех книг, которые на полку не помещаются. В нашем случае — \\((5 - 3)!\\).\nТо есть мы можем заполнить нашу полку \\(\\dfrac{5!}{(5-3)!}\\) способами.\nМы получили формулу для подсчета числа размещений (без повторений) из \\(n\\) элементов по \\(k\\) местам:\n\\[\nA_n^k = \\frac{n!}{(n-k)!}\n\\]\n\n\n3.1.3.2.2 С повторениями\nКаждой книги у нас по одному экземпляру, поэтому выше мы говорили о размещениях без повторений. Для полноты картины посмотрим на размещения с повторениями, хотя они встречаются в практике реже.\nТеперь у нас неограниченное количество копий каждой из книг, поэтому при размещении трех из пяти книг на полке все три могут оказаться одной и той же, или две одинаковые, а одна отличается и т.д. Суть — книги могут повторяться.\nПоскольку теперь, независимо от того, сколько книг мы уже поставили, у нас все равно осталось столько же, то есть 5, так как есть копии, каждый раз мы будем выбирать из пяти книг. Таких выборов нам нужно будет сделать три, так как три места на полке. Итого, заполнить нашу полку мы сможем \\(5^3\\) способами.\nМы получили формулу для подсчета числа размещений с повторениями из \\(n\\) элементов по \\(k\\) местам:\n\\[\n\\overline{A_n^k} = n^k\n\\]\n\n\n\n3.1.3.3 Сочетания\n\n3.1.3.3.1 Без повторений\nА теперь задача такова: нам не важно в каком порядке будут стоять книги на полке — нам нужно просто поставить три какие-то книги. Сколько возможно вариантов выбрать три книги из пяти?\nТак как мы теперь не учитываем порядок книг, то возможных вариантов будет в \\(3!\\) раз меньше, чем число размещений. Почему? Так как все перестановки этих трёх книг для нас теперь идентичны. Итого, всего вариантов выбрать три книги из пяти \\(\\dfrac{5!}{3!(5-3)!}\\).\nМы получили формулу для подсчета числа сочетаний из \\(n\\) элементов по \\(k\\):\n\\[\nС_n^k = \\frac{n!}{k!(n-k)!}\n\\]\nПоследняя формула нам пригодится далее при обсуждении схемы испытаний Бернулли.\n\n\n3.1.3.3.2 С повторениями\nВновь для полноты картины посмотрим на случай с повторениями. У нас вновь есть неограниченное количество копий каждой книги, однако порядок выставления на полку нам не важен — как и было в сочетаниях. Количество сочетаний в этом случае будет равно\n\\[\n\\overline{С_n^k} = \\frac{(n+k-1)!}{k!(n-1)!}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>L3 // Математика для анализа данных</span>"
    ]
  },
  {
    "objectID": "l3.html#элементы-математического-анализа",
    "href": "l3.html#элементы-математического-анализа",
    "title": "3  L3 // Математика для анализа данных",
    "section": "3.2 Элементы математического анализа",
    "text": "3.2 Элементы математического анализа\nИз всего матана нам надо уловить два основных концепта — производную и интеграл. Этим и займёмся, захватив попутно немного пределов.\n\n3.2.1 Последовательности\nЧисловая последовательность — это последовательность чисел [внезапно]. В общем случае — любых. Она обозначается \\((x_n)^\\infty_{n=1}\\), где \\(x_n\\) — это некоторый элемент последовательности, а верхний и нижний индексы обозначают границы изменения индекса \\(n\\). Например, \\(\\langle 1,-1,1,-1,\\dots \\rangle\\) — это числовая последовательность, которую можно обозначить \\(\\big( (-1)^n \\big)^\\infty_{n=1}\\).\nПоследовательность возникает на некотором множестве чисел. Если на таком множестве определено отношение порядка, то есть элементы этого множества можно сравнивать на «больше-меньше-равно», то можно сформировать монотонную последовательность. Это такая последовательность, которая не возрастает — то есть стоит на месте или убывает — или не убывает — то есть стоит на месте или возрастает. Более того, если существует такой объект (число), к которому элементы последовательности приближаются с ростом номера, то он является…\n\n\n3.2.2 Предел последовательности\n…пределом этой последовательности.\nРазберемся на примере. Пусть у нас есть вот такая простенькая последовательность:\n\\[\n\\Big( \\frac{1}{n} \\Big)^{\\infty}_{n=1} = \\Big \\langle 1, \\frac{1}{2}, \\frac{1}{3}, \\dots \\Big \\rangle\n\\]\nДостаточно очевидно, что каждый следующий её элемент, меньше предыдущего. Отрицательными элементы данной последовательности быть не могут, поэтому кажется, что всё идет к тому, что где-то там последовательность упрётся в ноль.\nФормально число \\(a\\) называется пределом последовательности \\(\\{x_n\\}\\), если для любого положительного числа \\(\\varepsilon\\) существует номер \\(N_\\varepsilon\\), такой что для любого \\(n &gt; N_\\varepsilon\\), выполняется неравенство \\(|x_n - a| &lt; \\varepsilon\\), или на математическом:\n\\[\n\\lim_{n \\rightarrow \\infty} x_n = a \\Leftrightarrow \\forall \\varepsilon &gt; 0 \\exists N_\\varepsilon \\in \\mathbb{N}: n \\geq N_\\varepsilon, |x_n - a| &lt; \\varepsilon\n\\]\nТо есть, в случае нашей последовательности мы можем отступить на сколь угодно малое число \\(\\varepsilon\\) от нуля, и, начиная с какого-то номера, все элементы нашей последовательности окажутся в интервале \\((\\varepsilon, 0)\\). Поэтому \\(\\lim_{n \\rightarrow 0} \\frac{1}{n} = 0\\).\n\n\n3.2.3 Функции\nФункции (они же отображения, как мы выяснили выше) устанавливают соответствие между элементами двух множеств. Чаще всего мы имеет дело с числовыми функциями, то есть такими, которые ставят одни числа в соответствие другим. У любой функции есть область определения (множество \\(X\\)) и область значений (множество \\(Y\\)). Сама же функция представляет собой множество упорядоченных пар\n\\[\n(x,y) \\in X \\times Y,\n\\] таких что\n\nпары существуют для всех элементов \\(X\\), и\nесли первые элементы пар равны, то равны и их вторые элементы.\n\nПримерами функций могут быть \\(y = x^2\\), \\(y = kx + b\\), \\(y = |x|\\) и другие.\n\n\n\n\n\nПримеры функций\n\n\n\n\n\n3.2.3.1 Дискретные и непрерывные функции\nВ зависимости от того, какова область определения функции, то есть множество \\(X\\), функции могут быть дискретными и непрерывными. Например, если функция определена на множестве целых чисел \\(\\mathbb{Z}\\), то она будет дискретная, так как между, например, \\(1\\) и \\(2\\) будет пусто.\n\n\n\n\n\nПример графика дискретной функции\n\n\n\n\nЕсли функция определена на множестве \\(\\mathbb{R}\\), то она будет непрерывной. Например, функция \\(f(x) = x^2\\) является непрерывной, как и функции \\(f(x) = \\sqrt{x}\\) и \\(f(x) = \\ln(x)\\). Если функция непрерывная, то она дифференцируема5.\n\n\n\n3.2.4 Производная\nА раз они дифференцируемы, то мы можем взять производную!\nПроизводная — очень полезная вещь. Во-первых, она показывает тангенс угла наклона касательной в данной точке, а во-вторых скорость и направление изменения функции в данной точке. На самом деле, и первое, и второе рассказывает нам примерно об одном и том же.\nДавайте издалека. Как нам узнать, куда двигается функция в данной точке?\nПусть дана функция \\(f(x) = 2x^3 + 3x^2-4x+6\\).\n\n\n\n\n\n\n\n\n\nВыберем точку \\(x_0\\), в которой мы хотим определить, куда и с какой скоростью движется наша функция. В этой точке функция имеет значение \\(y_0\\):\n\n\n\n\n\n\n\n\n\nШагнём на некоторую дистанцию \\(\\Delta x\\) вправо (по направлению оси \\(x\\)). Назовём эту дистанцию приращением аргумента. В точке \\(x_0 + \\Delta x\\) фунция будет иметь какое-то значение \\(y_0 + \\Delta y\\), где \\(\\Delta y\\) — приращение функции.\n\n\n\n\n\n\n\n\n\nНаша функция движется из точки \\((x_0,y_0)\\) в точку \\((x_0 + \\Delta x, y_0 + \\Delta y)\\). Имеем следующий треугольник — приблизим картинку:\n\n\n\n\n\n\n\n\n\nНас интересует угол \\(\\alpha\\) — именно он задает скорость и направление изменения функции. Если мы узнаем, каков угол \\(\\alpha\\) — а точнее \\(\\tan \\alpha\\), потому что так проще — то узнаем, куда движется функция.\n\\[\n\\tan \\alpha = \\frac{\\Delta y}{\\Delta x}\n\\]\nНу, хорошо. Но мы шагали достаточно далеко от точки, которая нас интесует. Если мы будем постепенно уменьшать шаг, то получим последовательность\n\\[\n\\langle \\tan \\alpha_1, \\tan \\alpha_2, \\tan \\alpha_3, ... \\rangle\n\\]\nУ этой последовательности есть предел, и если мы его рассчитаем, то как раз и получим значение производной в данной точке.\n\\[\nf'(x_0) = \\lim_{\\Delta x \\rightarrow 0} \\frac{\\Delta f(x)}{\\Delta x} = \\frac{df}{dx}(x_0)\n\\]\nОчень маленькое приращение обозначается \\(dx\\) (или \\(df\\) , если это приращение функции). Вот мы и получили производную.\nМожно построить график производной. Это тоже будет функция.\nВажное свойство этой функции, которое нам понадобится в дальнейшем, заключается в том, что там, где график производной пересекает ось \\(x\\) — то есть там, где производная равна нулю — на исходной функции случаются точки смены монотонности — точки минимума и максимума.\n\n\n\n\n\n\n\n\n\n\n\n3.2.5 Частная производная\nМы хорошо знакомы с функциями одной переменной, где некий \\(y\\) зависит от некоего \\(x\\) и более ни от чего не зависит. Однако в общем случае никто нам не может помешать задать следующую функцию:\n\\[\nf(x, y) = 2x^2 + y^3\n\\]\nТеперь у нас две переменные — \\(x\\) и \\(y\\) — и от них обеих зависит значение функции. Это даже можно изобразить:\n\n\n\n\n\n\nВ общем-то наличие второй переменной практически ничего не меняет, однако появляется важная фича — мы можем смотреть, как изменяется значение функции при изменении каждой переменной в отдельности. Это позволяют делать частные производные.\nЧастные производные в целом берутся так же, как и обычные, только мы предполагаем, что все другие переменные — то есть те, по которым мы не берём производную в данный момент — это константы. Таким образом, мы как бы фиксируем другие переменные и получаем скорость изменения функции по какой-либо одной переменной.\nРассмотрим на примере нашей функции \\(f(x, y) = 2x^2 + y^3\\). Пусть мы хотим взять производную по \\(x\\). Тогда мы предполагаем следующее:\n\\[\ny = \\text{const}\\Rightarrow y^3 = \\text{const}= c\n\\]\nФункция примет следующий вид:\n\\[\nf(x, y) = 2x^2 + c,\n\\]\nа производная по \\(x\\) будет вычисляться следующим образом:\n\\[\n\\frac{\\partial f(x, y)}{\\partial x} = (2x^2 + c)' = (2x^2)' = 4x\n\\]\n\\(\\frac{\\partial f(x, y)}{\\partial x}\\) — это обозначение частной производной, аналогично тому, как мы обозначали производную через \\(\\frac{dy}{dx}\\).\nАналогично можно найти частную производную по \\(y\\):\n\\[\nx = \\text{const}\\Rightarrow 2x^2 = \\text{const}= c\n\\]\n\\[\n\\frac{\\partial f(x, y)}{\\partial y} = (c + y^3)' = (y^3)' = 3x^2\n\\]\nЗнакомство с вычислением частной производной понадобится нам, чтобы понять, как внутри устроена линейная регрессия и ухватить идею градиентного спуска.\n\n\n3.2.6 Интеграл\nИнтеграл — штука мощная, но нам он понадобится только с одной стороны. Нам надо будет искать площадь под кривой. Этим и займемся.\nПусть у нас есть функция \\(y = \\sqrt{x}\\). Нам надо найти площадь под её графиком на отрезке от \\(0\\) до \\(3\\).\n\n\n\n\n\n\n\n\n\nМы можем разбить этот отрезок на части размером \\(\\Delta x\\), а саму площадь на соответствующие прямоугольники. Это нам позволит оценить площадь. На рисунке ниже \\(\\Delta x = 0.25\\).\n\n\n\n\n\n\n\n\n\nПолучается, площадь можно оценить, сложив площади всех прямоугольников:\n\\[\nS \\approx \\sum_{i=1}^n y_i \\Delta x\n\\]\nЯсно, что чем у́же прямоугольники у нас будут, тем точнее мы будем знать площадь. Ниже представлены рисунки для случая \\(\\Delta x = 0.1\\) и \\(\\Delta x = 0.05\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nЧтобы вычислить площадь точно, снова воспользуемся пределами, и определим с их помощью определенный интеграл:\n\\[\n\\lim_{\\Delta x \\rightarrow 0} \\sum_{i = 1}^n y_i \\Delta x = \\int_a^b \\sqrt{x}\\,dx\n\\]\nОпределенный он потому, что мы знаем, площадь в каких границах нас интересует. Определённый интеграл — это число.\n\\[\n\\int_0^3 \\sqrt{x} \\, dx \\approx 3.46\n\\]\nА раз есть определённый интеграл, значит есть и неопределённый. Рассмотрим следующую визуализацию:\n\n\n\n\nПостроение неопределенного интеграла\n\n\n\nМы смотрим, как изменяется площадь под графиком некоторой функции \\(f(x)\\) по мере нашего движения по оси \\(x\\) (нижний график), и строим график, по оси \\(y\\) которого расположена площадь под исходным графиком, левее данного \\(x\\). Этот график и отображает неопределенный интеграл, второе название которого первообразная. Неопределённый интеграл (первообразная) — это такая функция \\(F(x)\\), производная которой \\(F'(x)\\) равняется \\(f(x)\\), то есть исходной функции. Таким образом, справедливо равенство\n\\[\nF'(x) = f(x)\n\\]\nА также справедливо соответствие между определённым и неопредленным интегралом:\n\\[\n\\int_a^b f(x) dx = F(b) - F(a)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>L3 // Математика для анализа данных</span>"
    ]
  },
  {
    "objectID": "l3.html#элементы-линейной-алгебры",
    "href": "l3.html#элементы-линейной-алгебры",
    "title": "3  L3 // Математика для анализа данных",
    "section": "3.3 Элементы линейной алгебры",
    "text": "3.3 Элементы линейной алгебры\n\n«Увы, невозможно объяснить, что такое матрица. Ты должен увидеть это сам.» Морфеус (Матрица, 1999)\n\n\n3.3.1 Системы линейных уравнений\nЛинейная алгебра занимается решением систем линейных уравнений. Да в общем-то и все. В самом общем виде система линейных уравнений выглядит так:\n\\[\n\\cases{\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1m}x_m = b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2m}x_m = b_2 \\\\\n\\cdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nm}x_m = b_n \\\\\n}\n\\]\nВ данном случае это система из \\(n\\) уравнений с \\(m\\) неизвесными. Хотя такая запись математически предельно верна, выглядит она достаточно громоздко, и ею весьма трудно пользоваться. Поэтому обычно системы линейных уравнений записывают в матричном виде:\n\\[\n\\mathbf{A}\\mathbf{x} = \\mathbf{b}\n\\]\nСогласитесь, читать такое значительно проще. Здесь \\(\\mathbf{A}\\) — это матрица коэффициентов системы, \\(\\mathbf{x}\\) — вектор неизвестных, а \\(\\mathbf{b}\\) — вектор свободных членов системы. Внутри они устроены вот так:\n\\[\n\\mathbf{A} = \\pmatrix{\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n}\n\\quad\n\\mathbf{x} = \\pmatrix{x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_m}\n\\quad\n\\mathbf{b} = \\pmatrix{b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n}\n\\]\nРазберемся с этими структурами подробнее.\n\n\n3.3.2 Векторы\nКак мы уже знаем, вектор — это набор чисел. В зависимости от того, как он расположен, он может быть вектором-строкой (\\(\\mathbf{r}\\)) или вектором-столбцом (\\(\\mathbf{c}\\)):\n\\[\n\\mathbf{r} = \\pmatrix{r_1 & r_2 & r_3 & \\dots & r_n}\n\\] \\[\n\\mathbf{c} = \\pmatrix{c_1 \\\\ c_2 \\\\ c_3 \\\\ \\vdots \\\\ c_m}\n\\]\nГеометрически элементы вектора (числа) являются его координатами в некотором \\(n\\)-мерном пространстве. О переходе от вектора как направленного отрезка к вектору как набору чисел мы говорили в первой лекции.\nПосмотрим, что можно делать с векторами на примере векторов \\(\\mathbf{v}\\) и \\(\\mathbf{w}\\), в которых есть по \\(n\\) элементов.\n\\[\n\\mathbf{v} = \\pmatrix{v_1 & v_2 & v_3 & \\dots & v_n}\n\\]\n\\[\n\\mathbf{w} = \\pmatrix{w_1 & w_2 & w_3 & \\dots & w_n}\n\\]\n\n3.3.2.1 Сложение векторов\nВекторы одинакоых размерностей можно складывать друг с другом:\n\\[\n\\mathbf{v} + \\mathbf{w} = \\pmatrix{v_1 + w_1 & v_2 + w_2 & v_3 + w_3 & \\dots & v_n + w_n}\n\\]\n\n\n3.3.2.2 Умножение вектора на число\nВектор можно умножать на произвольное вещественное число:\n\\[\n\\alpha \\cdot \\mathbf{v} = \\pmatrix{\\alpha \\cdot v_1 & \\alpha \\cdot v_2 & \\alpha \\cdot v_3 & \\dots & \\alpha \\cdot v_n}, \\quad \\forall \\alpha \\in \\mathbb{R}\n\\]\n\n\n3.3.2.3 Скалярное произведение векторов\nВекторы одинаковых размерностей можно скалярно умножить друг с другом. Скалярное произведение векторов определено как сумма произведений их соответствующих координат:\n\\[\n\\mathbf{v} \\cdot \\mathbf{w} = v_1 w_1 + v_2 w_2 + v_3 w_3 + \\dots + v_n w_n\n\\]\nТаким образом, в результате скалярного произведения векторов получается одно число.\n\n\n\n3.3.3 Матрицы\nКак мы уже знаем, матрица — это двумерный массив чисел. Своего рода табличка. Соответственно, в матрице есть строки и столбцы. Это значит, что одну и ту же матрицу можно записать по-разному. Например, матрицу \\(\\mathbf{A}\\), с которой мы встречались выше, можно записать:\n\nуказав все элементы:\n\n\\[\n\\mathbf{A} = \\pmatrix{\na_{11} & a_{12} & \\cdots & a_{1m} \\\\\na_{21} & a_{22} & \\cdots & a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nm}\n}\n\\]\n\nзадав её через векторы-столбцы:\n\n\\[\n\\mathbf{A} = \\pmatrix{\n\\mathbf{a_1} & \\mathbf{a_2} & \\cdots & \\mathbf{a_m} \\\\\n}\n\\]\n\nзадав её через векторы-строки:\n\n\\[\n\\mathbf{A} = \\pmatrix{\n\\mathbf{a_1} \\\\ \\mathbf{a_2} \\\\ \\cdots \\\\ \\mathbf{a_n}\n}\n\\]\nВсе три способа записи обозначают один и тот же объект и используются в зависимости от того, какой способ рассмотрения матрицы в данный момент удобнее.\nМатрица характеризуется, прежде всего, размером. В размере сначала указывается количество строк, затем — количество столбцов. Рассматриваемая нами матрица \\(\\mathbf{A}\\) имеет размер \\(n \\times m\\), что можно записать как \\(\\mathbf{A}_{n \\times m}\\).\n\nЕсли количество строк и столбцов в матрице совпадает, она называется квадратной.\nВ матрице есть главная диагональ — слева сверху вправо вниз — и побочная диагональ — справа сверху влево вниз.\nКвадратная матрица, все элементы которой, кроме стоящих на главной диагонали, равны нулю, называется диагональной матрицей.\nЕсть две замечательные матрицы: единичная \\(\\mathbf{I}\\) (или \\(\\mathbf{E}\\)) и нулевая \\(\\mathbf{O}\\).\n\n\\[\n\\mathbf{I} = \\mathbf{E} = \\pmatrix{\n1 & 0 & \\dots & 0 \\\\\n0 & 1 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1 \\\\\n}\n\\quad\n\\mathbf{O} = \\pmatrix{\n0 & 0 & \\dots & 0 \\\\\n0 & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 0 \\\\\n}\n\\]\n\n3.3.3.1 Сложение матриц\nДве матрицы одинакового размера можно складывать друг с другом:\n\\[\n\\mathbf{A}_{n \\times m} + \\mathbf{B}_{n \\times m} = \\pmatrix {\na_{11} + b_{11} & a_{12} + b_{12} & \\dots & a_{1m} + b_{1m} \\\\\na_{21} + b_{21} & a_{22} + b_{22} & \\dots & a_{2m} + b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} + b_{n1} & a_{n2} + b_{n2} & \\dots & a_{nm} + b_{nm} \\\\\n}\n\\]\nСвойства сложения матриц:\n\nКоммутативность: \\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\)\nАссоциативность: \\((\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\)\nСуществование нулевого элемента: \\(\\mathbf{A} + \\mathbf{O} = \\mathbf{A}\\)\nСуществование противоположного элемента: \\(\\mathbf{A} + (-\\mathbf{A}) = \\mathbf{O}\\)\n\n\n\n3.3.3.2 Умножение матрицы на число\nЛюбую матрицу можно умножить на вещественное число:\n\\[\n\\alpha \\cdot \\mathbf{A} = \\pmatrix{\n\\alpha \\cdot a_{11} & \\alpha \\cdot a_{12} & \\alpha \\cdot \\cdots & a_{1m} \\\\\n\\alpha \\cdot a_{21} & \\alpha \\cdot a_{22} & \\cdots & \\alpha \\cdot a_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\alpha \\cdot a_{n1} & \\alpha \\cdot a_{n2} & \\cdots & \\alpha \\cdot a_{nm}\n} \\quad \\forall \\alpha \\in \\mathbb{R}\n\\]\n\n\n3.3.3.3 Матричное умножение\nШтош, приступим.\n\nМатрицы можно матрично перемножить друг с другом, если у них совпадают внутренние размерности.\nРезультатом перемножения \\(\\mathbf{A}_{n \\times k} \\times \\mathbf{B}_{k \\times m}\\) является матрица \\(\\mathbf{C}_{n \\times m}\\).\nЭлемент \\(c_{ij}\\) матрицы \\(\\mathbf{C}\\) равен скалярному произведению \\(i\\)-го вектора-строки матрицы \\(\\mathbf{A}\\) и \\(j\\)-го вектора-столбца матрицы \\(\\mathbf{B}\\).\n\nВизуально:\n\n\n\n\nПринцип матричного умножения\n\n\n\nТеперь попробуем это расписать. Пусть есть две матрицы \\(\\mathbf{A}_{n \\times k} \\times \\mathbf{B}_{k \\times m}\\), которые выглядят следующим образом:\n\\[\n\\mathbf{A} =\n\\pmatrix{\\mathbf{a}_1 \\\\ \\mathbf{a}_2 \\\\ \\vdots \\\\ \\mathbf{a}_n} =\n\\pmatrix{\na_{11} & a_{12} & \\dots & a_{1k} \\\\\na_{21} & a_{22} & \\dots & a_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nk}\n}\n\\]\n\\[\n\\mathbf{B} =\n\\pmatrix{\\mathbf{b}_1 & \\mathbf{b}_2 & \\dots & \\mathbf{b}_n} =\n\\pmatrix{\nb_{11} & b_{12} & \\dots & b_{1m} \\\\\nb_{21} & b_{22} & \\dots & b_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nb_{k1} & b_{k2} & \\dots & b_{km}\n}\n\\]\n\\[\n\\mathbf{A} \\times \\mathbf{B} = \\pmatrix{\n\\mathbf{a}_1 \\cdot \\mathbf{b}_1 & \\mathbf{a}_1 \\cdot \\mathbf{b}_2 & \\dots & \\mathbf{a}_1 \\cdot \\mathbf{b}_m \\\\\n\\mathbf{a}_2 \\cdot \\mathbf{b}_1 & \\mathbf{a}_2 \\cdot \\mathbf{b}_2 & \\dots & \\mathbf{a}_2 \\cdot \\mathbf{b}_m \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbf{a}_n \\cdot \\mathbf{b}_1 & \\mathbf{a}_n \\cdot \\mathbf{b}_2 & \\dots & \\mathbf{a}_n \\cdot \\mathbf{b}_m \\\\\n} =\n\\]\n\\[\n= \\pmatrix{\n(a_{11}b_{11} + a_{12}b_{21} + \\dots + a_{1k}b_{k1}) & (a_{11}b_{12} + a_{12}b_{22} + \\dots + a_{1k}b_{k2}) & \\dots & (a_{11}b_{1m} + a_{12}b_{2m} + \\dots + a_{1k}b_{km}) \\\\\n(a_{21}b_{11} + a_{22}b_{21} + \\dots + a_{2k}b_{k1}) & (a_{21}b_{12} + a_{12}b_{22} + \\dots + a_{2k}b_{k2}) & \\dots & (a_{21}b_{1m} + a_{22}b_{2m} + \\dots + a_{2k}b_{km}) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(a_{n1}b_{11} + a_{n2}b_{21} + \\dots + a_{nk}b_{k1}) & (a_{n1}b_{12} + a_{n2}b_{22} + \\dots + a_{nk}b_{k2}) & \\dots & (a_{n1}b_{1m} + a_{n2}b_{2m} + \\dots + a_{nk}b_{km}) \\\\\n} =\n\\]\n\\[\n= \\pmatrix{\nc_{11} & c_{12} & \\dots & c_{1m} \\\\\nc_{21} & c_{22} & \\dots & c_{2m} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nc_{n1} & c_{n2} & \\dots & c_{nm}\n} = \\mathbf{C}\n\\]\n\\[\nc_{ij} = \\sum_{t=1}^k a_{it}b_{tj}\n\\]\nСвойства матричного умножения:\n\nАссоциативность: \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\) и \\(\\alpha(\\mathbf{A}\\mathbf{B}) = (\\alpha\\mathbf{A})\\mathbf{B} = \\mathbf{A}(\\alpha\\mathbf{B})\\)\nДистрибутивность: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{B}\\mathbf{C}\\) и \\((\\mathbf{A} + \\mathbf{B})\\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}\\)\nОтсутствие коммутативности: в общем случае \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)\nУмножение на единичный элемент: \\(\\mathbf{I}\\mathbf{A} = \\mathbf{A}\\), \\(\\mathbf{A}\\mathbf{I} = \\mathbf{A}\\)\nУмножение на нулевой элемент: \\(\\mathbf{O}\\mathbf{A} = \\mathbf{O}\\), \\(\\mathbf{A}\\mathbf{O} = \\mathbf{O}\\)\nУмножение на обратный элемент — выполняется только для некоторых квадратных матриц (см. ниже): \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\)\n\n\n\n3.3.3.4 Транспонирование матрицы\nЭто очень простая операция — строки и столбы матрицы меняются местами:\n\\[\n\\mathbf{A} =\n\\pmatrix{\\mathbf{a}_1 & \\mathbf{a}_2 & \\dots & \\mathbf{a}_n} =\n\\pmatrix{\na_{11} & a_{12} & \\dots & a_{1k} \\\\\na_{21} & a_{22} & \\dots & a_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\dots & a_{nk}\n}\n\\]\n\\[\n\\mathbf{A}^T =\n\\pmatrix{\\mathbf{a}_1 \\\\ \\mathbf{a}_2 \\\\ \\vdots \\\\ \\mathbf{a}_n} =\n\\pmatrix{\na_{11} & a_{21} & \\dots & a_{n1} \\\\\na_{12} & a_{22} & \\dots & a_{n2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1k} & a_{2k} & \\dots & a_{nk}\n}\n\\]\nЕсли размер исходной матрицы был \\(n \\times k\\), то размер транспонированной матрицы будет \\(k \\times n\\).\n\n\n3.3.3.5 Определитель и обратная матрица\nОпределитель (детерминант) матрицы \\(\\det \\mathbf{A}\\), \\(|\\mathbf{A}|\\), \\(\\Delta \\mathbf{A}\\) — это величина, которая может быть вычислена и поставлена в соответствие квадратной матрице. Он «определяет» свойства матрицы, в том числе одно из ключевых — её обратимость.\nВычисление детерминанта матрицы в общем случае является достаточно сложной задачей и требует введения дополнительных определений, поэтому ограничимся интерпретацией его значений:\n\nЕсли матрица не является квадратной, то детерминант не определен.\nЕсли детерминант матрицы не равен нулю, то:\n\nсистема линейных уравнений, задаваемая данной матрицей имеет единственное решение\nдля данной матрицы существует обратная матрица, обладающая следующим свойством — \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\\).\n\nЕсли детерминант матрицы равен нулю, то система линейных уравнений имеет несколько решений.\n\nОдной из причин может быть линейная зависимость между столбцами или строками матрицы — то есть ситуация, при который один из столбцов (одна из строк) линейно выражается через другой (другую).\n\n\n\n\n3.3.3.6 След матрицы\nСлед матрицы — это сумма элементов главной диагонали (квадратной) матрицы.\n\\[\n\\text{tr}(\\mathbf{A}) = \\sum_i a_{ii}\n\\]\n\n\n\n\nЗегет, В. 1985. Элементарная Логика. Москва: Высшая школа.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>L3 // Математика для анализа данных</span>"
    ]
  },
  {
    "objectID": "l3.html#footnotes",
    "href": "l3.html#footnotes",
    "title": "3  L3 // Математика для анализа данных",
    "section": "",
    "text": "Обсуждение критериев объективности мы оставим за рамками этого курса и постулируем, что мы их как-то хотя бы интуитивно пониманием. Для некоторой концептуальной рамки обозначим следующее: мы говорим об объективной связи между предметами, если (1) определённым предметам (или индивидам) присущи определённые признаки и если (2) определённым признакам свойственны определённые признаки (Зегет 1985).↩︎\nЕсть и другие подходы к определению истинности высказываний — таковы различные виды многозначной логики. Но они не-необходимы нам для целей курса, поэтому оставим их за бортом.↩︎\nВ общем случае множеством значений может быть какое-либо подмножество этого множества. Так, мы вполне можем написать, что \\(F: L \\rightarrow \\mathbb{N}\\) — это будет корректное утверждение. Однако множество значений функции теперь является подмножеством множества натуральных чисел.↩︎\nТакое отображение называется биекцией.↩︎\nЭто не вполне правда. Непрерывность является необходимым, но недостаточным условием для дифференцируемости.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>L3 // Математика для анализа данных</span>"
    ]
  },
  {
    "objectID": "l4.html",
    "href": "l4.html",
    "title": "4  L4 // Теория измерений",
    "section": "",
    "text": "4.1 Измерения\nНачнем с наиболее общего определения измерения.\nИзмерение — процедура приписывания определенным психологическим объектам определенных чисел на определенной шкале.\nТем не менее, такое общее определение измерения позволяет нам говорить, что измерить мы может всё, что угодно:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>L4 // Теория измерений</span>"
    ]
  },
  {
    "objectID": "l4.html#l4_measures",
    "href": "l4.html#l4_measures",
    "title": "4  L4 // Теория измерений",
    "section": "",
    "text": "Note\n\n\n\nЗдесь мы будем говорить об измерениях в социальных науках. Они имеют определенную спефицику по сравнению, скажем, с физическими измерениями.\n\n\n\n\n\nТехнически звучит не сложно.\nНо часто приходится доказывать, что мы реально что-то померили и сделали это адекватно.\n\n\n\nрост\nвозраст\nпол\nнациональность\nколичество детей в семье\nрейтинг студентов\nкурс / уровень обучения\nгеографические координаты (долгота и широта)\nтемпература\nдата\nIQ\nнарциссизм / макиавеллизм / психопатия\nвремя реакции\nточность ответов испытуемого в эксперименте\nи т.д.\n\n\n4.1.1 Какие существуют измерения в разных областях психологии?\nПодходы к измерениям можно поделить по отраслям (областям) психологической науки1:\n\nНейронаука и психофизиология\n\nРегистрируются физиологические (= физические) процессы\nПроцессы [как правило] являются реакциями на физическую стимуляцию\nПредполагается, что эти физиологические процессы являются коррелятами некоторых психических процессов\n\\(\\Phi \\rightarrow \\Phi (\\sim \\Psi)\\)\n\nПсихофизика\n\nИзмеряются субъективные феномены (реакции, ощущения, пороги)\nСубъективные реакции происходят на физическую стимуляцию\n\\(\\Psi \\rightarrow \\Phi\\)\n\nПоведенческие исследования\n\nИзмеряются поведенческие реакции с помощью объективных метрик (времени реакции, точности кликов, последовательности поиска)\nПоведенческие реакции [как правило] обусловлены физической стимуляцией\nПредполагается, что за поведенческими реакциями стоят некоторые психические процессы\n\\(B \\rightarrow \\Psi\\)\n\nПсихометрика\n\nИзмеряются субъективные феномены, не связанные напрямую с физиологическим процессами\nЧасто (= всегда) изучаются гипотетические конструкты\nИзмерение происходит с помощью субъективных методик\n\\(\\Psi \\rightarrow \\Psi\\)\n\n\nИз подобного методологического безобразия происходят две важных мысли:\n\nВ любой области психологической науки нам необходимо с теоретических позиций обосновать связь измеряемых в ходе исследования переменных с изучаемыми психическими феноменами.\nПри любом измерении нам необходимо выбрать адекватный способ числового отражения изучаемых феноменов, чтобы мы могли использовать статистические методы анализа.\n\nТаким образом, если мы уточним в свете последней важной мысли определение измерения, то оно будет звучать так:\nИзмерение — это процедура приписывания психологическим объектам чисел таким образом, чтобы отношения между числами соответствовали отношениям между психологическими объектами2.\nДействительно, с числами можно делать всё, что угодно, что не запрещено математикой — а не запрещено ею много чего, однако не любые математические операции имеют смысл по отношению к исходными психологическим объектам. Из этих ограничений возникают шкалы.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>L4 // Теория измерений</span>"
    ]
  },
  {
    "objectID": "l4.html#l4_scales",
    "href": "l4.html#l4_scales",
    "title": "4  L4 // Теория измерений",
    "section": "4.2 Шкалы",
    "text": "4.2 Шкалы\nМожно найти много технических определений шкалы, одно из которых звучит весьма красиво:\n\nШкала — это числовая структура, изоморфная эмпирической структуре.\n\nНо такое определение мало что проясняет относительно содержания шкалы. Для нас удобнее будет менее точное, но более осязаемое понимание:\n\nШкала — это набор чисел с ограничениями на допустимые по отношению к ним операции.\n\nВ таком понимании ещё С. Стивенсом в 1946 году были предложены четыре вида шкал — это классификацией мы пользуемся до сих пор:\n\nНоминальная шкала (шкала наименований, nominal scale)\nПорядковая шкала (ранговая шкала, ordinal scale)\nИнтервальная шкала (шкала разностей, interval scale)\nАбсолютная шкала (шкала отношений, ratio scale)\n\nШкалы отличаются друг от друга по математическому содержанию используемых на них чисел, допустимым на них математическим (и логическим) операциям и преобразованиям, наличию и характеру нуля, типу шкалы и типу данных.\n\n4.2.1 Признаки и переменные\nВ ходе исследования мы измеряем различные признаки изучаемых объектов. Попытаемся эти признаки как-то систематизировать. В терминах данных признаки — это переменные, поэтому далее мы будем чаще употреблять именно этот термин — переменная — имея в виду то, что мы намерили, изучая интересующий нас признак изучаемого объекта. Во многом признак и переменная — это синонимы, только первый термин больше из теории измерений, а второй из статистики и анализа данных. Измерение же от отдельного человека / объекта выборки называется наблюдение. В общем-то с этим мы уже сталкивались, когда обсуждали данные.\nИтак, типы переменных:\n\nКоличественные переменные — те, которые принимают числовые значения. Они могут быть:\n\nнепрерывными — принимают любые значения (рост, возраст, время реакции и др.)\nдискретными — могут принимать только определенные значения (количество детей в семье, число отчисленных студентов, количество пачек гречи, которое человек скупил на карантине и др.)\n\n\nЧисло, приписываемое количественному признаку (переменной) ведёт себя как привычное нам математическое число в том смысле, что выражает некоторое количество — сантиметров, лет, секунд, детей, студентов, пачек гречи…\n\nНоминальные (категориальные) переменные — используются для разделения наших наблюдений на группы (пол, национальность, курс обучения, используемая операционная ситема компьютера и др.)\n\nЗаписаны эти переменные обычно текстом (скажем, пол — male и female или операционная система — Win, MacOS, Linux). Однако, например, курс обучения можно записать по-разному: текстом — freshman, sophomore, junior, senior — и числом — 1, 2, 3, 4. Однако в данном случае цифры не несут никакого математического смысла — это просто лейблы, с помощью которых мы различаем группы наблюдений. Ведь и пол мы можем записать с помощью чисел — пусть male = 0, female = 1. Ведь не будем же мы складывать-вычитать девушек и парней?\nВнимательный читатель мог заметить, что курс обучения это не совсем категориальная переменная, ведь «второкурсник» в каком-то смысле «больше», чем «первокурсник». Но мы не можем сказать «на сколько» или «во сколько» больше! Что же делать?\n\nНельзя сказать, что «второкурсник» выражает большую выраженность признака «год обучения», чем «первокурсник».\nВместе с тем «второкурсник» дольше учился и освоил больше дисциплин, чем «первокурсник». При этом «третьекурсник» учился дольше «второкурсника». То есть существует порядок категорий.\nТакая переменная называется ранговой.\n\nДругой пример ранговой переменной — это студенческий рейтинг. Что делает рейтинг? Упорядочивает студентов. Можно ли сказать, что четвертый в рейтинге студент в два раза менее успешен, чем второй? Нет — тот же GPA может отличаться на десятые или сотые доли.\nИтого, переменные:\n\nколичественные\n\nнепрерывные\nдискретные\n\nранговые\nноминальные\n\n\n4.2.1.1 Виды шкал\nОт того, в какой шкале измерена переменная, которую мы исследуем, будет зависеть:\n\nкакие графики мы сможем нарисовать\nкакие статистики на ней имеют смысл\nкакие статистические модели дадут адекватный результат\n\nВ общем, почти весь анализ определяется тем, с какой шкалой мы работаем, поэтому разберем каждую шкалу подробнее.\n\n\n4.2.1.2 Номинальная шкала\n\nНаименее мощная шкала\nНеметрическая — расстояния между делениями не определены\nТип данных — категориальные\nДопустимые операции\n\nсравнение на (не)равенство\n\nНоль — отсутствует\nДопустимые преобразования — любое, сохраняющее взаимно однозначное соответствие\n\nВ этой шкале, что весьма ожидаемо, измеряются номинальные переменные. Даже если на этой шкале используются числа для задания категорий, они не несут никакого математического смысла, что следует из допустимых операций данной шкалы.\nСтоит отдельно оговорить, что значит преобразование, сохраняющее взаимно однозначное соответствие. Пусть у нас есть самая типичная социально-демографическая номинальная переменная исследований — пол. И пусть он у нас закодирован как male и female. Мы можем преобразовать эту переменную как угодно. Единственное условие, которое у нас есть — это возможность опознать мужчин и женщин по присвоенным лейблам. Так, мы можем использовать числа 1 и 0 или 618 и 1040, задать текстовые лейблы m и f или м и ж, или даже выдумать что-то ещё типа gfbc и rtsu. Последний вариант технически совершенно не удобен, но устройства шкалы его вполне допускает, так как сохранено взаимно-однозначное соответствие между реальным объектами и используемыми лейблами.\n\n\n4.2.1.3 Порядковая шкала\n\nНеметрическая — расстояния между делениями не равны между собой\nТип данных — категориальные / ранговые\nДопустимые операции\n\nсравнение на (не)равенство\nсравнение на больше-меньше\n\nНоль — отсутствует\nДопустимые преобразования — любое монотонное\n\nНа этой шкале появляется порядок значений, а значит и операция сравнения на больше-меньше. Нет делений — вернее, даже если есть, то они разного размера — поэтому складывать и вычитать ещё нельзя.\nПреобразование на этой шкале должно сохранять порядок её значений, так как это ключевая характеристика данной шкалы. Так, в принципе мы можем извлечь квадратный корень из переменной уровень обучения (бакалавриат, специалитет, магистратура, аспирантура), которая закодирована как 1, 2, 3, 4 — получится 1, 1.4, 1.7, 2. Это нам усложнит жизнь, несомненно, однако шкалу не сломает — порядок элементов сохранен.\n\n\n\n\n\n\nО термине «качественные данные».\n\n\n\nПочему-то номинальную и ранговую шкалы в литературе часто называют «качественными». Видимо, потому что качественные данные обычно рассматриваются как оппозиция количественным.\nЭто в некоторой мере справедливо, поскольку есть два типа исследований — качественные и количественные. Они различаются методологией и используемыми методиками и, как следствие, собираемыми данными.\nВ рамках качественных исследований чаще всего собираются тексты, поэтому во многом качественные данные по факту обычно текстовые. Количественные данные — это, как правило, таблицы с цифрами из любой из четырёх шкал. Безусловно, анализ качественных и количественных данных также существенно различается.\nИтого, кажется, называть «качественными» номинальную и ранговые шкалы — странно, потому что качественные данные — это неструктурированный текст. Лучше их именовать категориальными. Правда, например, рейтинг студентов (ранговая шкала) тоже не совсем категориальные данные… ай, ладно — будут ранговые!\n\n\n\n\n4.2.1.4 Интервальная шкала\n\nМетрическая — расстояния между делениями одинаковые\nТип данных — количественные\nДопустимые операции\n\nсравнение на (не)равенство\nсравнение на больше-меньше\nсложение и вычитание\n\nНоль — относительный\nДопустимые преобразования — любое линейное\n\nНа этой шкале появляется возможность складывать и вычитать, так как есть точка отсчета — ноль — и деления становятся одинакового размера. Правда выбран этот ноль случайно, поэтому он не отражает полное отсутствие признака у изучаемого объекта. По этой причине операции умножения и деления на этой шкале невозможны.\nКоличество возможных преобразований также сокращается — теперь при преобразовании шкалы нам важно сохранить равенство интервалов. Этому требованию соответствуют линейные преобразования, так как они выполняют условия линейности:\n\\[\nf(x+y) = f(x) + f(y)\n\\] \\[\nf(\\alpha x) = \\alpha f(x)\n\\]\nТак, квадратный корень из значений шкалы извлечь уже не получится, потому что равенство интервалов нарушится.\n\n\n4.2.1.5 Абсолютная шкала\n\nСамая мощная шкала\nМетрическая — расстояния между делениями одинаковые\nТип данных — количественные\nДопустимые операции\n\nсравнение на (не)равенство\nсравнение на больше-меньше\nсложение и вычитание\nумножение и деление\n\nНоль — абсолютный\nДопустимые преобразования — любое преобразование подобия\n\nНаличие абсолютного нуля на данной шкале позволяет производить с её значениями все математические операции. Однако это же существенно ограничивает набор допустимых преобразований — теперь нам важно сохранять этот самый абсолютный ноль, поэтому невозможно прибавить или вычесть какие-либо число из всех значений шкалы (сдвинуть её вправо или влево, вверх или вниз). Получается, можно только умножить или разделить шкалу на некоторое значение, что и является преобразованием подобия.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>L4 // Теория измерений</span>"
    ]
  },
  {
    "objectID": "l4.html#психометрические-измерения",
    "href": "l4.html#психометрические-измерения",
    "title": "4  L4 // Теория измерений",
    "section": "4.3 Психометрические измерения",
    "text": "4.3 Психометрические измерения\nВыше мы уже выяснили, что психометрические измерения особо выделяются среди всех других измерений в психологических науках. Еще раз обозначим, почему:\n\nИзмеряются ненаблюдаемые (латентные) конструкты с помощью субъективных шкал\n\nне знаем достоверно, существуют ли наши конструкты\n\nПоследствия\n\nВ измерениях всегда есть существенная доля ошибки → нужно знать надёжность (точность) измерения\nМы не всегда уверены, действительно ли мы измеряем то, что хотим → нужно обосновать валидность измерения\n\nОдного вопроса обычно недостаточно, чтобы задать содержание конструкта или добиться нужной точности измерения\n\n\n\n\n4.3.1 Операциональная классификация методик\nВстает вопрос, а как мы вообще можем измерять в психометрике что-либо?\nОперациональная классификация распределяет методики в зависимости от того, насколько результат их процедуры зависит от субъективного опыта респондента и самого диагноста.\n\nВыше черты — прямые методы\n\nОтветы респондента используются для интерпретации напрямую и обычно количественно\n\nНиже черты — непрямые методы\n\nОтветы респондента интерпретируются не напрямую и во многом методами качественного анализа\n\n\nПолучается следующая картина:\n\nПриборные психофизиологические методики\nАппаратурные поведенческие методики\nОбъективные тесты с выбором ответа (тесты способностей или тесты знаний)\nТесты-опросники (прямой субъективный самоотчет)\nСубъективное шкалирование\n\n———————————————————————————\n\nПроективные методики\nНаблюдение\nКонтент-анализ\nПсихологическая беседа\nРолевая игра\nОбучающий эксперимент\n\nЗдесь, конечно, методики взяты шире, чем обычно используются в психометрике. Чаще всего, сталкиваясь с психометрикой, мы имеет дело с объективными тестами3 и тестами-опросниками.\nОтдельный вопрос касается тестов-опросников — почему прямой субъективный самоотчет вообще работает? Здесь есть две ключевых идеи:\n\nВновь С. Стивенс нам сказал, что люди могут прямо оценивать интенсивность стимулов, приписывая им числа. Тем самым, он легитимизировал прямой субъективный самоотчет.\nКогда мы говорим об исследовательской работе, мы проводим психометрические измерения и диагностику в ситуации, которая предполагает свободное дальнейшее поведение4, то есть респонденты сами решают, как им обойтись с результатами тестирования — поэтому искажения предполагаются минимальными.\n\n\n\n4.3.2 Психометрика в двух словах\nТак или иначе, мы оказывается в потрясающей ситуации:\n\nизмеряем непонятно что, непонятно чем и непонятно как\n\n\nС одной стороны, кажется, что ситуация довольно безвыходная, однако психометрики придумали множество инструментов, чтобы совладать с подобным стечением обстоятельств. С другой стороны, если вы не занимаетесь психометрикой непосредственно и оказались к ней критически близко в силу необходимости использования психометрических инструментов в собственном исследовании, стоит понимать, что психометрика — это обычная научная область, со своими особенностями, проблемами и кризисами, которая она пытается решать, как и любая другая. И это окей.\n\n\n4.3.3 Концепт. Конструкт. Операционализация\nЧтобы начать конструировать психометрический инструмент, прежде всего необходимо определить, что мы собираемся измерять, поэтому первым шагом является определение измеряемого конструкта.\nКонструкт — это прямым образом ненаблюдаемая переменная, характеризующая различия в поведении людей в специфической группе ситуаций (см. Messick (1993), Barrett (2005)).\n\nхарактеристика, навык, способность человека, которую мы хотим оценить или измерить\nбазируется на одной или нескольких теориях\nне может быть измерен непосредственно, но с помощью различных индикаторов или переменных\nможет быть простым и сложным\n\nНапример, простыми конструктами будут:\n\nПринадлежность к политической партии\nСтаж\nУмение умножать\nДетский эгоцентризм (по Пиаже)\nЗнание букв\n\nСложными же могут выступить:\n\nУдовлетворенность работой\nМатематическая грамотность\nУчебная мотивация\nКоммуникация\nНавык достижения цели\n\nЗа конструктом может стоять ещё более обобщенная идея, задающая теоретическу рамку, в которой определяется конструкт — концепт.\nКонцепт — это обобщенная идея, разделяемая многими людьми (сообществом), которая может быть представлена в рамках той или иной теории или подхода.\nСуществуют некоторые различия в том, как смотрят на измеряемые конструкты в образовательном и психологическом тестировании:\n\nПсихологическое тестирование\n\nИзмерение какого-либо латентного конструкта\n\nопределение конструкта, основанное на теориях и / или исследованиях\nоперационализация конструкта\nобласти содержания\n\n\nОбразовательное тестирование\n\nОбразовательные результаты (освоение программы, курса, года, и т. д.)\n\nсоответствует ФГОС\nсоответствует учебному плану / программе\nотражает цели обучения\n\n\n\nПосле определения конструкта наступет этап операционализации:\n\nопределение конструкта в терминах операций, необходимых для его измерения (Machery, 2007)\nпроцесс и документ, описывающий переход от теоретического, абстрактного понятия к наблюдаемому поведению, измеряемому в тесте\n\nОперационализация подразумевает разработку:\n\nсубконструктов и их взаимосвязей,\nгрупп ситуаций, в которых они проявляются,\nспособов сбора информации об их проявлении (Mislevy, Almond, Lukas, 2003; Brennan, 2006)\n\n\n\n4.3.4 Тестовые задания. Области содержания конструкта. Структура опросника\nКогда операционализация разработана, наступает этап разработки тестовых заданий. Мы не будем останавливаться здесь на принципах их разработки — это отдельная большая область с массой нюансов и деталей. Остановимся на двух важных вещах.\n\nПри разработке пунктов опросника должны быть учтены области содержания конструкта — ситуации и контексты, в которых он может проявляться\n\nЧем шире конструкт, тем в большем количестве контекстов он может проявляться и тем больше областей содержания должен охватывать опросник\n\nВ структуру психометрического инструмента может входит несколько субшкал или несколько субтестов\n\nЕсли опросник состоит из нескольких субшкал, то его можно использоваться только как единый психометрический инструмент — не допускается использование отдельных субшкал опросника при сборе данных.\nЕсли опросник состоит из нескольких субтестов, то возможно использование отдельных субтестов при сборе данных.\n\n\n\n\n4.3.5 Виды шкал в психометрических инструментах\nВ классификации шкал психометрических инструментов частично дублируется классификация измерительных шкал, однако добавляются и новые основания.\n\nПо характеру отношений\n\nнеметрические (номинальная и порядковая)\nметрические (разностей и отношений)\n\nПо числовому соответствию\n\nдискретные\nнепрерывные (континуальные)\n\nПо наличию и/или смыслу полюсов\n\nбиполярные\nуниполярные\n\nПо материалу\n\nграфические\nтекстовые\nчисловые\n…\n\n\nВид используемой шкалы зависит от целевой аудитории теста (дети, взрослые, клиническая выборка и др.), диагностической ситуации, особенностей измеряемого конструкта, теоретических основания и т.д.\nКлассическая психометрическая шкала — это шкала Ликерта. Она обладает следующими характеристиками:\n\nбиполярная\nзаданы текстом все альтернативы\nравные интервалы [визуально]\nгоризонтальная\nданы целые числа [от 1 до 5]\n\n\n\n4.3.6 Психометрические характеристики шкалы\n\nНадежность — это мера свободы результатов от ошибки измерения (standard error of measurement, SEM).\nНадежность нельзя рассчитать напрямую — можно только аппроксимировать\nРазными методами — ни один из них не является полностью верным\nНо нам приходится с этим как-то жить\n\nМетоды расчета надежности:\n\nCronbach’s \\(\\alpha\\) — надежность-внутренняя согласованность\nМетод расщепленных половин\nРетестовая надежность\n\n\n\n4.3.7 Психометрические характеристики пунктов\n\n4.3.7.1 Трудность задания\n\nДля дихотомического случая\n\n\\[\nb_j = \\frac{s_{1j} \\cdot p_{1j} + s_{2j} \\cdot p_{2j}}{N_j} = \\frac{0 \\cdot p_{1j} + 1 \\cdot p_{2j}}{N_j} = \\frac{p_{2j}}{N_j}\n\\]\n\nДля политомического случая\n\n\\[\nb_j = \\frac{\\sum_{k=1}^{K_j}(s_{kj} \\cdot p_{kj})}{N_j \\cdot s_{Kj}}\n\\]\nИнтерпретация значений трудности:\n\nЧем выше показатель трудности, тем легче справиться с заданием\nСлишком трудные — [0.00, 0.05] — и слишком легкие — [0.95, 1.00] — задания плохо дифференцируют выборку\nНачинать кодировку ответов лучше с 0 — так проще жить и интерпретировать результаты\n\n\n\n4.3.7.2 Дискриминативность задания\n\nТест направлен на измерение некоторого конструкта\nСуммарный тестовый балл отражает выраженность конструкта\nЧем сильнее коррелирует балл по заданию с баллом по тесту, тем лучше задание различает респондентов\nЛучше использовать скорректированную меру — корреляцию балла по заданию с суммой баллов по всем другим заданиям\n\n\n\n\n4.3.8 Валидность\nВалидность — это соответствие результатов тестирования заявленной цели тестирования, в частности, тому психическому свойству (или свойствам), которое измеряется.\n\nВ широком смысле — сведения о поведении и психических явлениях, находящихся в причинной зависимости от диагностируемого свойства.\nАналогично можно говорить о валидности тестового задания.\n\n\nГлавная цель разработки психометрического инструмента — сбор как можно большего количества разных свидетельств валидности.\n\n\n\nСбор разных свидетельств валидности обеспечивает обоснованный вывод о том, что по результатам теста можно выносить соответствующие суждения о тестируемых.\n\n\n\n4.3.8.1 Виды валидности\n\n4.3.8.1.1 Концептуальная валидность\n\nобоснование тестовой методики с позиций соответствия авторским (теоретическим) представлениям об особенностях диагностируемых свойств\nмера соответствия содержания заданий теста авторской концепции этих свойств.\n\n\n\n4.3.8.1.2 Конструктная валидность\n\nопределяет область теоретической структуры психологических явлений, измеряемых тестом\nтест, базирующийся на развитой, логически-связной теории, обеспеченной высоко-операционализированными понятиями, обладает конструктной валидностью\n\nВиды конструктной валидности:\n\nвнутренняя валидность / надежность–внутренняя согласованность\nдифференциальная валидность\nконвергентная валидность\nдискриминантная валидность\n\n\n\n4.3.8.1.3 Внутренняя валидность\n\nподчиненность пунктов (заданий, вопросов) теста основному направлению теста как целого\nориентированность пунктов на изучение одних и тех же конструктов\nанализ осуществляется путем коррелирования ответов на каждое задание с общим результатом теста\nдинамика изучаемого конструкта\n\n\n\n4.3.8.1.4 Дифференциальная валидность\n\nвнутренние взаимоотношения между диагностируемыми факторами\nтесты интересов\n\nобычно умеренного коррелируют с показателем общей академической успеваемости\nно связаны с успеваемостью по отдельным дисциплинам\nособенно важна как показатель диагностической ценности методик в профотборе\n\n\n\n\n4.3.8.1.5 Эмпирическая валидность\n\nсовокупность характеристик валидности теста, полученных экспериментально-статистическим способом\n\nкритериальная\n\nтекущая / диагностическая / конкурентная\nретроспективная\nпрогностическая\n\nконвергентная\nдискриминантная\nконцессуальная\n\n\n\n\n4.3.8.1.6 Конвергентная валидность\n\nстепень соответствия баллов двух тестовых методик, направленных на измерение одного и того же или концептуально-родственных конструктов\nзначимая корреляция между тестами\n\n\n\n4.3.8.1.7 Дискриминантная валидность\n\nстепень, в которой тест не измеряет тот конструкт, для измерения которого он не предназначен\nотсутствие значимой корреляции между тестовыми показателями, отражающими концептуально независимые свойства\nчастный случай — отсутствие корреляции с переменными приводящими к фальсификации или мотивационным искажениям результата\n\nнапример, социальная желательность\n\n\n\n\n4.3.8.1.8 Критериальная валидность\n\nотражает соответствие результатов тестирования определенным значениям критериальной переменной или вероятности критериального события\nнезависимые от результатов теста непосредственные меры исследуемого качества\n\nуровень достижения в чем-либо\nстепень развития способности\nвыраженность определенного свойства личности\nпоказатели социально- или производственно-значимых результатов деятельности\n\n\nТекущая (конкурентная)\n\nкритериальное событие происходит сейчас, в момент исследования\n\nРетроспективная\n\nкритериальное событие уже произошло\n\nПрогностическая\n\nкритериальное событие будет потом\nнужен квазиэксперимент\n\n\n\n4.3.8.1.9 Концессуальная валидность\n\nустановлении связи (корреляции) тестовых данных с данными, полученными от внешних экспертов\nэксперты хорошо знакомы с тестируемыми\n\n\n\n4.3.8.1.10 Очевидная валидность (face validity)\n\nнасколько сам тест и его задания кажутся респондентам подходящими для цели тестирования\nвысокая очевидная валидность присуща также кейс-тестам\nочень часто не совпадает с научной концепцией валидности\nвысокая очевидная валидность часто является весьма желательной\nфактор, побуждающий респондента к сотрудничеству, серьезному и ответственному отношению к выполнению заданий и к восприятию результатов оценки\n\n\n\n4.3.8.1.11 Содержательная валидность\n\nстепень соответствия содержания заданий теста той реальной деятельности, в которой проявляется измеряемое психическое свойство\n\nучебные тесты, тесты профессиональных достижений\n\nмного разнородных факторов — личностные особенности, знания, умения и навыки, специальные способности — нужна адекватная модель тестируемой деятельности\n\nподбор заданий, охватывающие главные аспекты изучаемого феномена в правильной пропорции к реальной деятельности в целом\n\nавторское обоснование пригодности теста в самом содержании тестовых заданий\n\n\n\n4.3.8.1.12 Факторная валидность\n\nподтверждение теоретической структуры конструкта, разработанной в ходе операционализации, эмпирическими данными, собранными в ходе количественной апробации методики\nпроводится с помощью [конфирматорного] факторного анализа\n\n\n\n\n\nBarrett, P. 2005. “What If There Were No Psychometrics?: Constructs, Complexity, and Measurement.” Journal of Personality Assessment 85 (2): 134–40.\n\n\nMessick, S. 1993. “Foundations of Validity: Meaning and Consequences in Psychological Assessment.” Ets Research Report Series 1993 (2): i–18.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>L4 // Теория измерений</span>"
    ]
  },
  {
    "objectID": "l4.html#footnotes",
    "href": "l4.html#footnotes",
    "title": "4  L4 // Теория измерений",
    "section": "",
    "text": "По лекции Евгения Осина.↩︎\nТаково определение измерения в репрезентационной теории С. Стивенса.↩︎\nНасколько объективные тесты действительно объективны — это очень хороший вопрос.↩︎\nДружинин, 1990↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>L4 // Теория измерений</span>"
    ]
  },
  {
    "objectID": "l5.html",
    "href": "l5.html",
    "title": "5  L5 // Введение в статистику. Случайный эксперимент и случайные величины",
    "section": "",
    "text": "5.1 Введение в математическую статистику\nСтатистика — это междисциплинарная область знаний, а также практической деятельности, изучающая массовые явления, а также приципы и методы работы с данными, характеризующими эти явления.\nМассовые явления затрагивают огромные массы людей. Огромность масс, конечно, различна. Скажем, базовые перцептивные закономерности, связанные с тем, как устроена зрительная система, охватывают всех людей. Уровень удовлетворенности жизнью россиян охватывает только население России. Городские блага москвичей — только для жителей Москвы. Учебная мотивация студентов департамента психологии НИУ ВШЭ — это только про людей с психологических бакалариата и магистратур НИУ ВШЭ.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L5 // Введение в статистику. Случайный эксперимент и случайные величины</span>"
    ]
  },
  {
    "objectID": "l5.html#введение-в-математическую-статистику",
    "href": "l5.html#введение-в-математическую-статистику",
    "title": "5  L5 // Введение в статистику. Случайный эксперимент и случайные величины",
    "section": "",
    "text": "5.1.1 Генеральная совокупность\nГенеральная совокупность (population) — множество всех [существующих] исследуемых объектов и сведений о них.\nОбъем генеральной совокупности (\\(N\\)) — число единиц, образующих генеральную совокупность.\nГенеральная совокупность недоступна для изучения в полном объеме, так как \\(N\\) имеет порядок сотен, тысяч или даже миллионов… Поэтому в рамках исследований мы всегда работаем с выборкой.\n\n\n5.1.2 Выборка\nВыборка, или выборочная совокупность (sample) — множество объектов генеральной совокупности, объемом \\(n\\) (\\(n \\ll N\\)).\n\n5.1.2.1 Репрезентативность выборки\nСтатистика даёт нам теоретический и математический аппарат, который позволяет делать выводы о генеральной совокупности по выборке. Однако если мы криво собрали данные, то никакая математика нас не спасет от некорректных выводов.\n\nGarbage in, garbage out.\n\nРепрезентативность — степень соответствия характеристик выборки характеристикам генеральной совокупности.\nНапример, мы хотим исследовать связь учебной мотивации и академической успеваемости бакалавров психологии. Если мы соберем данные только со своих однокурсников, будет нехорошо, так как в нашу выборку не попали (1) другие курсы психологического бакалавриата нашего вуза, (2) бакалавры-психологи других вузов Москвы и (3) бакалавры-психологи вузов других городов России.\nИ так работает всегда.\nНу, почти. Есть соблазн проводить исследования на студентах-психологах, потому что они достаточно близко и их можно загнать на эксперименты за баллы. Более-менее сносно это может работать на каких-то базовых когнитивных феноменах из восприятия и памяти. Обычно у нас нет оснований предполагать, что восприятие и память работают по-разному у людей разного возраста и разных социальных страт. Но вот уже с мышлением возникают проблемы.\nПочему выборка должна быть репрезентативной?\nПотому что если мы делаем нормально, то хотим обобщать результаты нашего исследования, полученные на выборке, на генеральную совокупность. Если выборка нерепрезентативна, то мы не можем этого сделать. Зачем в таком случае проводить исследование — решительно неясно.\n\n\n5.1.2.2 Как набрать репрезентативную выборку\n\nОсознать, кто наша генеральная совокупность, так как для каждой генеральной совокупности репрезентативная выборка будет своя.\nПонять, есть ли какая-то группировка, важная для нашего исследования, в нашей генеральной совокупности — социальная страта, специальность образования, сфера работы, пол / гендер, возрастные группы, регион проживания, семейное положение, что-либо ещё.\nРассчитать достаточный объём выборки. Это не самая простая задача и о ней мы будем говорить отдельно. Пока отметим в назывном порядке, что на объем выборки будут влиять дизайн исследования, изменчивость признака, уровень значимости и размер эффекта [что бы это ни значило].\nОбеспечить случайное попадание респондентов в выборку.\n\nЗдесь надо остановиться подробнее. Если у нас есть ресурсы набрать много человеков в выборку (скажем, раза в 2–3 больше, чем достаточный объем выборки), то можно просто случайным образом откуда-то доставать людей — и всё будет хорошо. Закон больших чисел и центральная предельная теорема говорят, что наша выборка будет репрезентативной. Пока примем это как данность, позже поговорим об это подробнее.\n\n\nДля интересующихся\n\nЗакон больших чисел\n\nС увеличением числа случайных величин их среднее арифметическое стремится к среднему арифметическому математических ожиданий и перестает быть случайным. Общий смысл закона больших чисел — совместное действие большого числа случайных факторов приводит к результату, почти не зависящему от случая.\n\nТаким образом, закон больших чисел гарантирует устойчивость для средних значений некоторых случайных событий при достаточно длинной серии экспериментов.\nЦентральная предельная теорема\n\nРаспределение случайной величины, которая получена в результате сложения большого числа независимых случайных величин (ни одна из которых не доминирует, не вносит в сумму определяющего вклада и имеет дисперсию значительно меньше по сравнению с дисперсией суммы) имеет распределение, близкое к нормальному.\n\nИз ЦПТ следует, что ошибки выборки также подчиняются нормальному распределению.\n\n\nЕсли мы всё же не можем набрать много человеков, то надо набрать выборку достаточного объема и проверить репрезентативна ли она — отражает ли выборка те группировки объектов, которые есть в генеральной совокупности.\nИдеальная выборка — это когда каждый человек имеет равную вероятность попасть в число респонтентов / испытуемых. Полностью случайный отбор трудно достижим — это очень дорого и логистически сложно — но к нему нужно стремиться. Сам метод сбора данных может деформировать выборку (например, онлайн опросы отсекают пенсионеров), поэтому думать о сборе данных необходимо уже на этапе планирования исследования.\n\n\n5.1.2.3 Способы формирования репрезентативной выборки\n\nЗдесь представлены три классических способа формирования выборки. В конкретном исследовании мы можем использовать и какие-либо другие способы формирования выборки, однако нам нужно будет обосновать, почему в нашем случае тот или иной способ позволяет собрать репрезентативную выборку.\n\n\n5.1.2.3.1 Простая случайная выборка (simple random sample)\n\nЭлементы генеральной совокупности случайным образом попадают в выборку\nС увеличением объема простая случайная выборка будет все больше напоминать генеральную совокупность по своим характеристикам.\n\nПредставим, что на этой картинке изображена вся генеральная совокупность:\n\n\n\n\n\n\n\n\n\nЕсли мы наберем простую случайную выборку из этой генеральной совокупности, она будет выглядеть так (черные точки):\n\n\n\n\n\n\n\n\n\nКак можно заметить, в выборку попали объекты из всех частей нашей генеральной совокупности — говорит о том, что выборка репрезентативна.\n\n\n5.1.2.3.2 Стратифицированная выборка (stratified sample)\n\nГенеральная совокупность разбивается на несколько обособленных различных по своей природе групп (страт). Например, по полу или уровню образования\nИз каждой группы случайным образом выбираются несколько объектов, которые попадают в выборку.\n\nПусть в нашей генеральной совокупности есть четыре страты:\n\n\n\n\n\n\n\n\n\nТогда мы можем разделить её на четыре «генеральных совокупности» соответственно:\n\n\n\n\n\n\n\n\n\nИз каждой такой «генеральной совокупности» будем извлекать случайную выборку:\n\n\n\n\n\n\n\n\n\n\n\n5.1.2.3.3 Групповая выборка (cluster sample)\n\nГенеральная совокупность разбивается на несколько обособленных, но одинаковых групп (кластеров). Например, население города группируется по району проживания\nВыбираются случайным образом несколько групп\nИз каждой группы случайным образом выбираются несколько объектов, которые попадают в выборку.\n\nПусть мы разделили нашу генеральную совокупность на 8 кластеров:\n\n\n\n\n\n\n\n\n\nКластеры у нас примерно одинаковые по характеристикам между собой — по крайне мере, мы так предполагаем. Выберем случайно четыре кластера, которые примут участие в исследовании:\n\n\n\n\n\n\n\n\n\nТеперь из этих кластеров наберем выборку (допустим, по 20 наблюдений из кластера или пропорционально объему отобранных кластеров) случайным образом (объекты, попавшие в итоговую выборку отмечены черным контуром, не попавшие — серым):\n\n\n\n\n\n\n\n\n\nТакой подход к формированию выборки позволяет экономить драгоценные ресурсы при проведении исследования.\n\nПодробнее про разные способы сбора выборки можно почитать тут.\n\n\n\n\n\n5.1.3 Характетистики объектов выборки и генеральной совокупности\nОбъекты генеральной совокупности обладают определенными признаками, которые мы и хотели бы изучать. Признаки количественно выражены в определенных показателях. Например,\n\n\n\n\n\n\n\nПризнак\nПоказатель\n\n\n\n\nРабочая память\nОбъем рабочей памяти\n\n\nНейротизм\nУровень нейротизма по BFI\n\n\nДоход\nСовокупный годовой доход после уплаты налогов\n\n\nКогнитивная нагрузка\nУровень когнитивной нагрузки по ЭЭГ-коррелятам\n\n\nДоверие к ИИ\nУровень доверия к ИИ по опроснику TAIA\n\n\nИндивидуализм/коллективизм\nИндекс индивидуализма/коллективизма по модели Хофстеде\n\n\n\nПризнаки могут быть очень разными и измеряться могут с помощью разных показателей.\nНезависимо от того, как измеряется признак, генеральная совокупность характеризуется параметром.\nПараметр (\\(\\theta\\)) — относительно постоянная [от одной совокупности к другой] величина, харакретизующая генеральную совокупность по некоторому показателю.\nНу, то есть в принципе существует средний уровень нейротизма по BFI студента-психолога или индекс индивидуализма/коллективизма для конкретной культуры. Проблема в том, что величина параметра, который мы изучаем, неизвестна. И никогда не будет известна.\nНо почему?\n\nМы не можем изучать всю генеральную совокупность — слишком много объектов\nНаши измерения всегда содержат ошибку — мы даже длину линейкой точно не можем измерить, что уж о психологических измерениях говорить\n\nПоэтому величину параметра мы можем только предсказать с определённой статистической точностью. Измеряя что-либо на выборке, мы получаем выборочную характеристику, или оценку (\\(\\hat \\theta\\)) — эмпирический (измеримый) аналог параметра.\n\n\n5.1.4 Характеристики статистических данных\nИтак, теперь задумаемся о том, что мы собираем на выборке некоторые данные. К каким их особенностям приведут все моменты, описанные выше?\n\nМы не можем работать с генеральной совокупностью, поэтому набираем выборку — выборки между собой имеют право различаться\nКаждый респондент или испытуемый обладает своими особенностями — мы не знаем, что мы получим в результате конкретного измерения на конкретном изучаемом объекте1\nЛюбое наше измерение содержит ошибку — ни один измерительный инструмент не является совершенным\n\nВсё это приводит нас к двум ключевым характеристикам статистически данных — неопределенности и вариативности.\n\nНеопределённость нам говорит о том, что мы не знаем, что именно мы получим в результате наших измерений для конкретной выборки.\n\nОтсюда чуть позже возникнут случайные величины.\n\nВариативность означает, что наши данные будут различатся от выборки к выборке и от респондента к респонденту\n\nОтсюда возникнут статистические критерии, в которых эта характеристика данных будет учтена в ходе тестирования гипотез.\n\n\n\n\n5.1.5 Зачем нужна статистика?\nМы в какой-то малоприятной ситуации… Мы пытаемся измерить то, что в определенном смысле невозможно измерить, при этом достаточно точно, чтобы потом это можно было сравнивать или строить какие-то модели. Задача выглядит заведомо провальной…\nОднако именно в этот момент на помощь нам приходит статистика. Не в гордом одиночестве, конечно. Она проводит с собой теорию измерений, психометрику, теорию обнаружения сигнала и др. Всё это работает в нашей психологической науке в комлексе.\nСтатистика даёт нам теоретический и математический инструментарий, чтобы мы могли делать какие-либо выводы по нашим собранным данным. К сожалению, как бы нам не хотелось, мы не можем делать выводы по сырым данным, потому что измерения по выборке не отражают вот прям ровно то, что есть в генеральной совокупности. Нам их надо определенным образом обсчитать, чтобы наши выводы были корректными. Этим и занимается статистика.\nВозможно, это звучит достаточно абстрактно, но я хочу, чтобы на данном моменте вы поймали некоторое интуитивное понимание того, зачем нужна статистика. Далее это обрастёт содержанием и уложится, я надеюсь, в достаточно стройную систему.\nИтог — статистика помогает нам делать выводы о нашей генеральной совокупности по выборке.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L5 // Введение в статистику. Случайный эксперимент и случайные величины</span>"
    ]
  },
  {
    "objectID": "l5.html#случайный-эксперимент",
    "href": "l5.html#случайный-эксперимент",
    "title": "5  L5 // Введение в статистику. Случайный эксперимент и случайные величины",
    "section": "5.2 Случайный эксперимент",
    "text": "5.2 Случайный эксперимент\nОтвлечемся немного на любимый объект статистиков — игральный кубик.\n\n\n\n\n\n\nБросание игрального кубика — это случайный эксперимент.\nВыпавшее число — это случайная величина.\n\nТеперь более строго.\nСлучайный эксперимент — это математическая модель некоторого реального эксперимента, результат которого невозможно точно предсказать.\nКлючевой момент в случайном эксперименте — это то, что его результат невозможно точно предсказать, то есть какой стороной упадёт кубик заранее неизвестно.\nЭто не соотносится с экспериментом как методом исследования. Эксперимент как метод исследования включает в себя сложную процедуту экспериментального воздействия, контроля систематических и несистематических смешений, манипуляции с независимыми переменными и фиксирование зависимых переменных и т.д. Случайный эксперимент же является частью эксперимента.\nПосмотрим на примеры из психологического поля:\n\nответ респондента на пункт (айтем) опросника — это случайный эксперимент\n\nнеизвестно, какой балл выберет респондент\n\nклик на стимул на экране в эксперименте на зрительный поиск — это случайный эксперимент\n\nзаранее неизвестно, когда точно испытуемый кликнет по стимулу\n\nзапись ЭЭГ-активности в конкретный момент времени — это случайный эксперимент\n\nнеизвестно, что мы зафиксируем в конкретный момент\n\nи т.д.\n\nОтсюда мы делаем важный вывод: любой акт измерения — это [с точки зрения статистики] случайный эксперимент.\n\n5.2.1 Модель случайного эксперимента\nНа модель случайного эксперимента накладывается ряд требований:\n\nадекватность описания реального эксперимента (в нашем случае, акта (момента) измерения)\nопределение совокупности наблюдаемых результатов случайного эксперимента (при фиксированных начальных данных)\nпринципиальная возможность осуществления эксперимента со случайным исходом сколько угодно большое количество раз (при фиксированных начальных данных)\nстохастическая устойчивость относительной частоты для любого наблюдаемого результата\n\nДля того, чтобы это всё понять, нужно ввести некоторые концепты.\n\n\n5.2.2 Элементарные исходы и события\nВ случайном эксперименте возможны различные исходы, называемые элементарными событиями (\\(\\omega_i\\)). Например, в случае упомянутого выше игрального кубика при его бросании возможны шесть элементарных событий (исходов):\n\n\\(\\omega_1\\) — выпала грань с одной точкой\n\\(\\omega_2\\) — выпала грань с двумя точками\n\\(\\omega_3\\) — выпала грань с тремя точками\n\\(\\omega_4\\) — выпала грань с четырьмя точками\n\\(\\omega_5\\) — выпала грань с пятью точками\n\\(\\omega_6\\) — выпала грань с шестью точками\n\nМножество всех элементарных событий называется пространством элементарных событий (\\(\\Omega\\)) случайного эксперимента.\nВ случае игрального кубика можно записать так:\n\\[\n\\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4, \\omega_5, \\omega_6\\}\n\\]\nВ общем случае, когда возможны \\(n\\) случайных исходов случайного эксперимента, пространство элементарных событий будет выглядеть так:\n\\[\n\\Omega = \\{\\omega_1, \\omega_2, \\dots, \\omega_{n-1}, \\omega_n\\}\n\\]\nАналогично, в случае, когда случайным экспериментом будет ответ респодента на пункт опросника по пятибалльной шкале Ликерта, пространство элементарных событий будет выглядеть так:\n\\[\n\\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4, \\omega_5\\},\n\\]\nгде\n\n\\(\\omega_1\\) — дан ответ «1» / «не согласен»\n\\(\\omega_2\\) — дан ответ «2» / «скорее, не согласен»\n\\(\\omega_3\\) — дан ответ «3» / «ни то, ни другое»\n\\(\\omega_4\\) — дан ответ «4» / «скорее, согласен»\n\\(\\omega_5\\) — дан ответ «5» / «согласен»\n\nВ пространстве элементарных событий определяются случайные события — любое подмножество множества элементарных событий. Например, для игрального кубика\n\nслучайное событие «выпало четное число очков» соответствует множеству \\(A_{\\text{even}} = \\{\\omega_2, \\omega_4, \\omega_6\\}\\)\nслучайное событие «выпало нечетное число очков» соответствует множеству \\(A_{\\text{odd}} = \\{\\omega_1, \\omega_3, \\omega_5\\}\\)\nслучайное событие «выпала грань с тремя точками» соответствует множеству \\(A_3 = \\{\\omega_3\\}\\)\nслучайное событие «не выпало ни одной грани» соответствует множеству \\(A_0 = \\varnothing\\) — такое событие называется невозможным\nслучайное событие «выпала любая грань» соответствует множеству \\(A_{\\text{any}} = \\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4, \\omega_5, \\omega_6\\}\\) — такое событие называется достоверным\n\nОбратите внимание, что \\(A_{\\text{even}} \\subset \\Omega\\), \\(A_{\\text{odd}} \\subset \\Omega\\), \\(A_3 \\subset \\Omega\\), \\(A_0 \\subset \\Omega\\) и \\(A_{\\text{any}} \\subset \\Omega\\), что как раз и утверждает определение случайного события.\nВсё множество случайных событий \\(A_i\\) обозначается2 \\(\\mathcal A\\).\n\nЕсли пространство элементарных событий конечно или счетно, то оно называется дискретным.\nЕсли пространство элементарных событий недискретно и элементарными исходами являются числа, то оно называется непрерывным.\n\n\n\n5.2.3 Вероятность\nОкей, мы ввели пространство элементарных событий \\(\\Omega\\) и множество случайных событий \\(\\mathcal A\\). Однако этого оказывается недостаточно, чтобы работать с результатами случайного эксперимента. Так как исход случайного эксперимента невозможно точно предсказать, необходимо ввести меру, которая будет описывать возможность наступления того или иного события. Такая мера называется вероятностью.\nВероятность (\\(\\mathbb{P}\\))— относительная мера возможности наступления некоторого события в результате случайного эксперимента.\n\n5.2.3.1 Классическая вероятность\nВернемся вновь к игральному кубику. Напомним себе, что мы определили пространство элементарных событий для случайного эксперимента «бросание игрального кубика» следующим образом:\n\\[\n\\Omega = \\{\\omega_1, \\omega_2, \\omega_3, \\omega_4, \\omega_5, \\omega_6\\}\n\\]\nЗададим следующие ограничения, чтобы было удобнее работать с чиселками:\n\nвероятность достоверного события должна равняться единице — \\(\\mathbb{P}(\\Omega) = 1\\)\nвероятность невозможного события должна равняться нулю — \\(\\mathbb{P}(A_0) = 0\\)\n\nПредполагая, что кубик честный, то есть выпадение каждой грани равновозможно, можно определить вероятность выпадения каждой грани как\n\\[\n\\mathbb{P}(\\omega_i) = \\frac{1}{n},\n\\]\nгде \\(n\\) — количество всех возможных элементарных исходов случайного эксперимента. Получается, вероятность выпадения каждой грани — \\(\\frac{1}{6}\\).\nАналогично можно определить вероятность любого случаного события \\(A_i\\):\n\\[\n\\mathbb{P}(A_i) = \\frac{n_i}{n},\n\\]\nгде \\(n_i\\) — количество элементарных исходов, составляюших событие \\(A_i\\), а \\(n\\) — количество всех возможных элементарных исходов случайного эксперимента. Получается, что вероятность выпадения четного числа очков \\(\\mathbb{P}(A_\\text{even}) = \\frac{3}{6} = \\frac{1}{2}\\), что достаточно логично.\nТакой подход к вероятности называется классической вероятностью. Возвращаясь к требованиям модели случайного эксперимента, можно заключить, что такая модель (1) адекватно описывает реальный эксперимент, так как предсказанные вероятности согласуются с наблюдениями, (2) определяет совокупность наблюдаемых результатов случайного эксперимента (\\(\\Omega\\) и \\(\\mathcal A\\)) и допускает принципиальную возможность осуществления случайного эксперимента сколь угодно большое количество раз.\nТройку \\((\\Omega, \\mathcal A, \\mathbb{P})\\) называются вероятностным пространством.\n\n\n5.2.3.2 Статистическая вероятность\nС игральным кубиком классическая вероятность прекрасно работает. Однако задумается о следующем случае: какова вероятность встретить динозавра на улице?\nВоспользуемся классическим подходом к вероятности. Зададим пространство элементарных событий, которое будет выглядеть так: \\(\\Omega = \\{\\omega_1, \\omega_2\\}\\), где \\(\\omega_1\\) — встреча с динозавром случилась, а \\(\\omega_2\\) — встреча с динозавром не случилась.\nПолучается, что вероятность встретить динозавра на улице равняется\n\\[\n\\mathbb{P}(\\omega_1) = \\frac{1}{n} = \\frac{1}{2}\n\\]\nТо есть, каждый второй день мы по пути на работу должны сталкиваться с каким-нибудь жителем триаса. Однако наши наблюдательные данные говорят, что этого не происходит. Получается, такая модель неадекватна реальности. В чем тут дело?\nКлассическая вероятность исходит из допущения равновозможности наступления любого элементарного исхода. Данное допущение в случае с динозавром нарушается. Чтобы корректно определить вероятность в данном случае, необходимо использовать статистическую вероятность.\nФормально статистическая вероятность определяется как предел частоты наблюдений некоторого события при стремлении количества наблюдений к бесконечности [при их независимости (наблюдения не влияют друг на друга) и однороности (условия наблюдений одинаковы)]:\n\\[\n\\mathbb{P}(A) = \\lim_{N \\rightarrow \\infty} \\frac{n}{N},\n\\]\nгде \\(N\\) — количество наблюдений, а \\(n\\) — количество наступлений события \\(A\\).\nТо есть, чтобы оценить вероятность некотрого события, надо многократно повторить случайный эксперимент (провести большое количество наблюдений), посчитать, в скольки наблюдениях наступило интересующее нас событие, и поделить одно на другое. И чем больше наблюдений мы проведем, тем более точну оценку вероятности мы получим.\nТакой подход дает адекватное описание происходящему в реальном мире — вероятность встретить динозавра оказывается равной (крайне близкой) к нулю.\nОтметим, что этот подход работает и с игральным кубиком: совершив много бросков кубика мы сможем выяснить, что вероятность выпадения каждой грани — \\(\\frac{1}{6}\\), при условии, что кубик честный (что является допущением классической вероятности).\nМы будем опираться на статистический подход, поскольку в реальной исследовательской работе мы обычно имеем дело именно с неравновозможными событиями.\n\n\n5.2.3.3 Геометрическая вероятность\nПосмотрим еще на один подход к определению вероятности. Поставим себе весьма абстрактную, но веселую задачу.\nВозьмем квадрат, в который вписан круг:\n\n\n\n\n\n\n\n\n\nБудем бросать в этот квадрат точки случайным образом:\n\n\n\n\n\n\n\n\n\nВопрос: какова вероятность, что случайно брошенная точка попадет в круг (событие \\(A\\))?\nМы можем воспользоваться статистическим подходом к вероятности, набросать побольше точек и посчитать, сколько из них попало в пределы круга:\n\n\n\n\n\n\n\n\n\nИз визуализации видно, что в конечном итоге при очень большом количестве бросаний точек они заполнят всю площадь квадрата, а значит, и всю площадь круга, поэтому вероятность попадания случайно прошенной точки в круг равняется отношению площади круга к площади квадрата, то есть:\n\\[\n\\mathbb{P}(A) = \\lim_{N \\rightarrow \\infty} \\frac{n}{N} = \\frac{S_\\text{circle}}{S_\\text{square}}\n\\]\nМожно расписать точнее, если вспомнить геометрические формулы:\n\\[\n\\mathbb{P}(A) = \\frac{S_\\text{circle}}{S_\\text{square}} = \\frac{\\pi r^2}{a^2} = \\frac{\\pi \\big(\\frac{1}{2}a\\big)^2}{a^2} = \\frac{1}{4}\\pi \\approx 0.785\n\\]\nТакое подход к определению вероятности называется геометрической вероятностью.\nОтметим, что плоский случай можно обобщить и до объемного. В случае куба и шара вероятность будет равна\n\\[\n\\mathbb{P}(A) = \\frac{V_\\text{sphere}}{V_\\text{cube}} = \\frac{\\frac{4}{3}\\pi r^3}{a^3} = \\frac{1}{6} \\pi \\approx 0.523\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L5 // Введение в статистику. Случайный эксперимент и случайные величины</span>"
    ]
  },
  {
    "objectID": "l5.html#случайные-величины",
    "href": "l5.html#случайные-величины",
    "title": "5  L5 // Введение в статистику. Случайный эксперимент и случайные величины",
    "section": "5.3 Случайные величины",
    "text": "5.3 Случайные величины\nКогда мы первый раз обратились к игральному кубику, мы сказали, что бросание игрального кубика — это случайный эксперимент, а выпавшее число — это случайная величина. Со случайным экспериментом разобрались, приступим к случайным величинам.\nСлучайная величина — это некоторая переменная, значения которой представляют собой численные исходы некоторого случайного эксперимента.\nВ частности, исход бросания кубика — выпавшее число, исход фиксации времени реакции в эксперименте — количество миллисекунд (число), исход измерения психометрического конструкта — суммарный балл по опроснику (число) и т.д.\nСами исходы случайного эксперимента числами быть в общем случае не обязаны, поэтому формально случайную величину \\(\\xi\\) определяют как функцию \\(y = \\xi(\\omega)\\), или \\(\\xi: \\Omega \\rightarrow \\mathbb{R}\\), на вероятностном пространстве \\((\\Omega, \\mathcal A, \\mathbb{P})\\), которая сопоставляет исходам случайного эксперимента некоторые числа.\nПоскольку случайная величина это численное выражения исходов случайного эксперимента, а сами исходы осуществляются с определенными вероятностями, чтобы мочь работать со случайной величиной, необходимо задать эту самую случайную величину, то есть описать её вероятностные свойства. Эти мы далее и займемся.\nВ зависимости от того, какое пространство элементарных событий было в случайном эксперименте — дискретное или непрерывное — случайные величины также могут быть дискретными или непрерывными.\n\n5.3.1 Дискретные случайные величины\nСлучайная величина является дискретной, если множество её значений конечно или счётно. Это позволяет задать случайную величину с помощью функции вероятности (probability mass function, PMF).\n\nМножеством определения (domain) этой функции будет множество значений случайной величины\nМножеством её значений (range) будет отрезок \\([0, 1]\\)\nСами значения будут определять вероятность, с которой происходит реализация определенного элементарного исхода соответствующего случайного эксперимента\n\nПосмотрим на примере игрального кубика. Множество определения функции вероятности известно — \\(\\{1, 2, 3, 4, 5, 6\\}\\). Каждое значение выпадает с вероятностью \\(\\frac{1}{2}\\), поэтому множество значений также известно. Можно построить график функции вероятности для этой случайной величины \\(X\\):\n\n\n\n\n\n\n\n\n\nПоскольку случайная величина дискретная, то и функция у нас получается дискретная, что выражается в том, что она отображена точками. Задав таким способом случайную величину мы полностью определяем её математическое поведение и можем с ней работать. Более того, мы можем записать её следующим образом:\n\\[\nf(x) = \\mathbb{P}(X = x),\n\\]\nгде \\(f(x)\\) — функция вероятности, \\(X\\) — случайная величина, \\(x\\) — конкретное значение случайной величины.\nИз того, как задана эта функция, вытекают её следующие свойства:\n\n\\(\\forall i \\in \\mathbb{N}\\, f(x_i) \\geq 0\\)\n\\(\\sum_{i=1}^\\infty f(x_i) = 1\\)\n\nДругим способом задания случайной величины является функция распределения (cumulative distribution function, CFD). Эта функция задается следующим образом:\n\\[\nF(x) = \\mathbb{P}(X &lt; x),\n\\] где \\(F(x)\\) — функция распределения, \\(X\\) — случайная величина, \\(x\\) — конкретное значение случайной величины. То есть значение функции распределения для данного значения случайной величины определяет вероятность того, что случайная величина примет такое или меньшее значение.\nФункция распределения может быть определена через функцию вероятности следующим образом:\n\\[\nF(x) = \\sum_{x' \\leq x} f(x')\n\\]\nОпираясь на это определение, можно изобразить график для случая игрального кубика:\n\n\n\n\n\n\n\n\n\nЭта функция также является дискретной.\n\n\n5.3.2 Непрерывные случайные величины\nСлучайная величина является непрерывной, если множество её значений обладает мощностью континуума.\nЕсли с дискретными случайными величинами все достаточно просто, то при работе с непрерывными мы сталкиваемся с определенной проблемой.\nЗададимся вновь абстрактной, но интересной задачей: будем набирать числа из отрезка от нуля до единицы \\([0, 1]\\). Возникает очень простой вопрос — с какой вероятностью мы сможем вытащить, скажем, число 0.5? или 0.341? или любое другое? Считаем, что все числа нам могут попасться равновероятно.\nНесмотря на допущение равновероятности, классическая вероятность нам здесь не поможет. При попытке определить вероятность как \\(\\frac{1}{n}\\), мы выясним, что \\(n\\) у нас бесконечно3. Получается, что\n\\[\n\\mathbb{P}(X = x) = \\frac{1}{n} = \\frac{1}{\\infty} = 0\n\\]\nИли если записать это более аккуратно:\n\\[\n\\mathbb{P}(X = x) = \\lim_{n \\rightarrow \\infty} \\frac{1}{n} = 0\n\\]\nСтатистическая вероятность нам тоже не поможет. Да, мы можем нагенерировать сколько угодно случайных чисел от 0 до 1, однако все они будут уникальными — то есть нам все равно придется делить на очень большое \\(n\\), которое будет стремиться к бесконечности при увеличении количества сгенерированных чисел, и мы получим тот же самый результат.\n— Получается, что вероятность вытащить конкретное значение равна нулю?  — Да.  — Значит ли это, что мы не сможем извлечь из отрезка от 0 до 1 ни одного числа?  — Нет.\nДействительно, для непрерывных случайных величин справедливо утверждение:\n\\[\n\\mathbb{P}(X = x) = 0,\n\\]\nгде \\(X\\) — непрерывная случайная величина, а \\(x\\) — её конкретное значение. То есть действительно,\n\nвероятность того, что непрерывная случайная величина примет своё конкретное значение, равна нулю.\n\nОднако это утверждение стоит понимать следующим образом: непрерывная случайная величина, конечно же, будет принимать какие-то конкретные значения в реализациях случайного эксперимента, однако если мы попытаемся орагнизовать случайный эксперимент так, чтобы она приняла значение \\(x\\), мы не сможем этого сделать — значение, которое мы с в итоге получим будет \\(x \\pm \\varepsilon, \\, \\varepsilon &gt; 0\\). Это значит что мы можем попадать рядом с требуемым значением, сколь угодно близко, но в него никогда не попадем.\nЧто же делать в этой ситуации? Ведь нам очень надо работать с непрерывнями случайными величинами — в частности потому, что значения статистических критериев являются непрерывными случайными величинами. Чтобы обойти это нулёво-вероятностную неприятность, вводится понятие плотности вероятности.\n\n5.3.2.1 Плотность вероятности\nПлотность вероятности — это весьма трудно перевариваемый математический концепт. Попробуем провести аналогию между дискретными и непрервными случайными величинами, чтобы попытаться облечь это в какое-то осязаемое содержание.\nМы видели функцию вероятности для игрального кубика — напомним себе, как она выглядит:\n\n\n\n\n\n\n\n\n\nЗначения на гранях кубика выпадают равновероятно, поэтому мы видим «прямую» из точек. Если мы допускаем, что точки из нашего отрезка извлекаются также равновероятно, то мы хотели бы увидеть что-то такое:\n\n\n\n\n\n\n\n\n\nПо оси \\(x\\) у нас идут значения случайной величины — от 0 до 1. За этими пределами чисел мы не набираем, поэтому линия туда не продолжается. Внутри этого отрезка у нас прямая линия, которая выражает идею равновероятного выбора любой точки из отрезка. Вопрос только один: что по оси \\(y\\)???\nЧтобы это понять, давайте сравним два графика ниже:\n\n\n\n\n\n\n\n\n\nОни похожи, только на верхнем отображена функция вероятности для дискретной величины. а на нижнем что-то аналогичное для непрерывной. Если на верхнем по оси \\(y\\) явно идет вероятность, то на нижнем какая-то другая величина. Однако и по верхнему, и по нижнему графику мы можем сказать, что значения около 15 встречаются чаще, чем значения около, например, 20. Значения же около 2–3 встречаются так же редко, как значения около 29–30.\nОкей, мы не можем работать в случае непрерывных случайных величин с конкретными значениями, но если мы выделим интервалы около интересующих нас значений, то вероятности попадания случайной величины в них мы уже можем посчитать. Возьмем значения 15 и 20 и немного отступим от них вправо — получатся интервалы \\([15, 15 + \\Delta x]\\) и \\([20, 20 + \\Delta x]\\):\n\n\n\n\n\n\n\n\n\nГрафик нам визуально подсказывает, что значения из интервала \\([15, 15 + \\Delta x]\\) встречаются чаще, чем значения из интервала \\([20, 20 + \\Delta x]\\) — это мы выяснили выше. Но можем ли мы на визуализации этих интервалов найти вероятность? Да, это будут закрашенные области под графиком. Идея здесь похожа на геометрическую вероятность, которую мы обсуждали ранее — чем больше площадь под графиком, тем больше вероятность попасть в заданный интервал.\nДействительно, такой способ отображения вероятности согласуется с тем, что вы получили аналитически — если мы сократим \\(\\Delta x\\) до нуля, то и площадь сократится до нуля, что будет означать нулевую вероятность принятия случайной величиной своего конкретного значения.\nХорошо, мы нашли вероятность. Но что же отображено на оси \\(y\\)? Это и есть та самая плотность вероятности.\nФормально плотность вероятности случайной величины \\(\\xi\\) — это числовая фукнция \\(f(x)\\), отношение \\(\\dfrac{f(x_1)}{f(x_2)}\\) значений которой в точках \\(x_1\\) и \\(x_2\\) задаёт отношение вероятностей попадания случайной величины \\(\\xi\\) в интервалы \\([x_1, x_1 + \\Delta x]\\) и \\([x_2, x_2 + \\Delta x]\\) при \\(\\Delta x \\rightarrow 0\\). Получается, что эта функция действительно отражает то, что определенные значения встречаются чаще, чем другие. График, который мы рассматривали выше, называется графиком функции плотности вероятности (probability density function, PDF).\nТаким образом, введение концепта плотности вероятности для непрерывных случайных величин позволяет визуально изучать их аналогично дискретным случайным величинам. Отличия проявляются в аналитической работе с вероятностью. Поскольку вероятность того, что непрерывная случайная величина примет своё конкретное значение, равняется нулю, мы не можем работать с вероятностями отдельных значений. Однако можем работать с вероятностями интервалов значений непрерывной случайной величины. То есть, вероятность попадания значения случайной величины в интервал \\([a, b]\\) определяется как интеграл функции плотности вероятности:\n\\[\n\\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f(x) dx\n\\]\nПоскольку площадь под графиком функции плотности теперь определяет вероятность, необходимо потребовать, чтобы вся площадь под графиком была равна единице — вероятность того, что случайная величина примет хотя бы какое-то из своих значений:\n\\[\n\\int_{-\\infty}^{+\\infty} f(x) dx = 1\n\\]\nТак и задается функция плотности вероятности.\nИз функции плотности вероятности можно построить функцию распределения непрерывной случайной величины (cumulative distribution function, CDF) — это будет первообразная от функции плотности:\n\\[\nF(x) = \\int_{-\\infty}^x f(t) dt\n\\]\n\n\n\n5.3.3 Нормальное распределение\nСамое популярное распределение из всех распределений случайных величин — это нормальное распределение. Во-первых, потому что оно в принципе часто встречается в природе, а во-вторых, потому что из него получаются другие распределения. Познакомимся с ним подробнее.\nЭто непрерывное распределение, которое задается двумя параметрами — математическим ожиданием и дисперсией. Если некоторая случайная величина починяется нормальному распределению, это записывают следующим образом:\n\\[\nX \\thicksim \\mathcal N (\\mu, \\sigma^2),\n\\]\nгде \\(X\\) — случайная величина, \\(\\mathcal N\\) — обозначение нормального распределения, \\(\\mu\\) — математическое ожидание, \\(\\sigma^2\\) — дисперсия.\nАналитически нормальное распределение задается следующей функцией плотности:\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\tfrac{(x - \\mu)^2}{2\\sigma^2}},\n\\]\n\\(x \\in \\mathbb{R}, \\, \\mu \\in \\mathbb{R}, \\, \\sigma \\in \\mathbb{R}_{&gt;0}\\).\nГрафически в зависимости от параметров \\(\\mu\\) и \\(\\sigma^2\\) может выглядеть по-разному, но это всегда хорошо знакомый всем «колокол»:\n\n\n\n\n\n\n\n\n\nМатематическое ожидание \\(\\mu\\) задает положение середины «колокола» на оси \\(x\\), а дисперсия \\(\\sigma^2\\) — ширину колокола.\nПомимо математического ожидания и дисперсии, нормальное распределение характеризуется также коэффициентом асимметрии и коэффициентом эксцесса. Первый показывает, насколько симметрично распределение относительно математического ожидания, а второй — насколько оно сжато по бокам или, наоборот, растянуто вдоль оси \\(x\\).\n\nПоскольку любое нормальное распределение симметрично относительно математического ожидания, то коэффициент асимметрии любого нормального распределения равен нулю.\nКоэффициент эксцесса зависит от дисперсии\n\nдля дисперсии, равной единице, коэффициент эксцесса равен нулю\nесли дисперсия меньше 1 — значит, пик распределения высокий — то коэффициент эксцесса положительный\nесли дисперсия больше 1 — значит, пик распределения гладкий — то коэффициент эксцесса отрицательный\n\n\nШирину «колокола» распределения можно описывать не только через дисперсию, но и через старндартное отклонение \\(\\sigma = \\sqrt{\\sigma^2}\\). Использование стандартного отклонения позволяет определить верояности попадания значений случайной величины в определенные диапазоны:\n\n\n\n\n\n\n\n\n\nКонкретно с этими вероятностями мы работаем реже — полезнее оказываются следующие:\n\n\\(\\mathbb{P}(X \\in (\\mu - \\sigma, \\mu + \\sigma)) = 0.682\\)\n\\(\\mathbb{P}(X \\in (\\mu - 2\\sigma, \\mu + 2\\sigma)) = 0.956\\)\n\\(\\mathbb{P}(X \\in (\\mu - 3\\sigma, \\mu + 3\\sigma)) = 0.998\\)\n\nТо есть\n\nв пределах одного стандартного отклонения от среднего значения лежит почти 70% значений — это очень частотные значения\nв пределах двух стандартных отклонений от среднего значения лежит 95% значений — бо́льшая часть выборки\nв пределах трех стандартных отклонений от среднего значения лежит практически 100% выборки — то есть вся выборка",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L5 // Введение в статистику. Случайный эксперимент и случайные величины</span>"
    ]
  },
  {
    "objectID": "l5.html#footnotes",
    "href": "l5.html#footnotes",
    "title": "5  L5 // Введение в статистику. Случайный эксперимент и случайные величины",
    "section": "",
    "text": "Возможно, не очень хорошо называть человека «объектом», однако так как мы сейчас в методологии количественных исследований, будем оперировать именно этим термином.↩︎\nСтрого говоря, этим символом обозначается другая структура, называемая алгеброй (или сигма-алгеброй) событий, но нам будет достаточно этого более поверхностного понимания.↩︎\nМы можем даже сказать более точно: \\(n = \\aleph_1\\).↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>L5 // Введение в статистику. Случайный эксперимент и случайные величины</span>"
    ]
  },
  {
    "objectID": "l6.html",
    "href": "l6.html",
    "title": "6  L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез",
    "section": "",
    "text": "6.1 Оценивание параметров\nНапомним себе весьма малоприятную ситуацию, в которой мы находимся, когда решаем провести некоторое исследование. Мы заинтересованы в изучении генеральной совокупности. Объекты интересующей нас генеральной совокупности обладают определенными признаками, которые мы, собственно, хотели бы изучать. Признаки количественно выражены в определенных показателях.\nПризнаки могут быть очень разными и измеряться могут с помощью разных показателей. Независимо от того, как измеряется признак, генеральная совокупность характеризуется параметром.\nПараметр (\\(\\theta\\)) — относительно постоянная [от одной совокупности к другой] величина, характеризующая генеральную совокупность по некоторому показателю.\nПроблема в том, что величина параметра, который мы изучаем, неизвестна. И никогда не будет известна. Потому что\nНам остаётся работать только с выборочной совокупность (выборкой) и опираться на статистические данные, которые мы собираем на ней. Измеряя что-либо на выборке, мы получаем выборочную характеристику, или оценку (\\(\\hat \\theta\\)) — эмпирический (измеримый) аналог параметра.\nВыборка извлекается из генеральной совокупности случайным образом, поэтому что там именно — с точки зрения данных — в нашей выборке будет нам также неизвестно. Отсюда происходят два ключевых свойства статистических данных — неопределённость и вариативность.\nНеопределённость нам говорит о том, что мы не знаем, что именно мы получим в результате наших измерений для конкретной выборки. В том числе потому, что мы работаем на просторах случайных величин.\nВариативность означает, что наши данные будут различаться ещё и от респондента к респонденту. И между выборками тоже. Здесь и ошибка измерения, и различные смешения и ещё куча всего.\nВ итоге что мы имеем: так как нам не доступны истинные значения параметров, придётся использовать оценки этих параметров. Возникает вопрос: как нам получить эти оценки? и какими свойствами они должны обладать, чтобы хорошо отражать параметры генеральной совокупности?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "l6.html#оценивание-параметров",
    "href": "l6.html#оценивание-параметров",
    "title": "6  L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез",
    "section": "",
    "text": "мы никогда не можем изучать всю генеральную совокупность, так как она содержит слишком много объектов\nнаши измерения всегда содержат ошибку, из-за чего мы ничего не можем измерить точно\n\n\n\n\n\n\n\n6.1.1 Точечные оценки\nПусть у нас есть некоторый параметр генеральной совокупности \\(\\theta\\). Его аналогом на выборочной совокупности является его точечная оценка \\(\\hat \\theta\\). Точечная она, потому что представляет собой некоторое одно число. Таким образом, это наиболее компактный способ составить представление о значении параметра. По своей сути она, на самом деле, является функцией — по факту, случайной величиной — от результатов наблюдений:\n\\[\n\\hat \\theta = \\hat \\theta (\\mathbf{x}), \\; \\mathbf{x} = \\pmatrix{ x_1 & x_2 & \\dots & x_n}\n\\]\nЭто всё замечательно, но что это значит для нас как для практиков? Значение оценки зависит от наблюдений, поэтому на разных выборках мы будем получать разные значения оценки. Возьмем для примера такой параметр как среднее значение. Пусть мы изучаем интеллект — это наш признак — который мы измеряем как коэффициент IQ — это наш показатель (\\(X\\)). Известно, что в генеральной совокупности этот признак распределен нормально с математическим ожиданием 100 и стандартным отклонением 15, то есть \\(X \\thicksim \\mathcal N(100, 225)\\):\n\n\n\n\n\n\n\n\n\nТут нам, конечно, повезло, потому что мы знаем, как устроена шкала IQ1, поэтому мы знаем значение нашего параметра — \\(\\mu = 100\\). В общем случае, конечно, значение параметра, как мы отмечали выше, неизвестно.\nТеперь попробуем наизвлекать выборок человек по 50 и посчитать оценки среднего (выборочные средние) \\(\\hat \\mu\\) на них:\n\n\n\n\n\n\n\n\n\nНаблюдаем, что иногда мы при подсчёте оценке параметра попадаем близко к истинному его значению, иногда промахиваемся. Собственно, как раз об этом неопределённость и вариативность.\n\n6.1.1.1 Метод моментов\nЧтобы получить точечные оценки параметров, используются разные методы. Метод зависит от того, какой параметр мы хотим оценить, а также с какой моделью мы сейчас работаем. Сейчас мы познакомимся с самым простым — методом моментов.\nСлово «момент» обычно вызывает странные ощущения — какой момент? момент чего? что в этот момент случается? Тут надо отпустить привычное понимание слова «момент» как некоторого момента времени и принять тот факт, что «момент случайной величины» — а именно о нём мы говорим — это просто характеристика распределения случайной величины. То есть математическое ожидание — это момент распределения случайной величины, дисперсия — это момент распределения случайной величины.\nВ методе моментов есть три этапа:\n\nустанавливается связь между оцениваемым параметром и моментом распределения случайной величины\n\n\\[\n\\quad \\theta = \\xi(\\mu_k),\n\\]\nгде \\(\\mu_k\\) — это момент случайной величины.\n\nнаходятся выборочные моменты\n\n\\[\n\\hat \\theta = \\xi(\\mu_k^*)\n\\]\n\nистинный момент заменяется на выборочный — получается оценка.\n\nВернемся к IQ. Эквивалентом среднего значения в случае генеральной совокупности является математическое ожидание, поэтому значение параметра \\(\\mu\\) определяется как\n\\[\n\\mu = \\mathbb{E}X\n\\]\nВыборочным аналогом математического ожидания является выборочное среднее:\n\\[\n\\hat \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i = \\bar x\n\\]\nИ это, собственно, всё. Если вы хотя бы раз анализировали данные, вы имплицитно пользовались этим знанием. Просто, скорее всего, не задумывались, что это так работает.\n\n\n\n6.1.2 Свойства точечных оценок\nТак как точечные оценки всё же оценки, мы можем и промахнуться мимо истинного среднего — это мы наблюдали на гистограмме. Поэтому нам надо предъявить определённые требования к точечным оценкам, которые будут отражать «хорошесть» точечной оценки. Таких требования три: несмещённость, состоятельность и эффективность.\n\n\n\n\n\n\nСвойства матемаческого ожидания\n\n\n\nПри обсуждении свойств точечных оценок нам потребуются два свойства математического ожидания:\n\nМатематическое ожидание суммы независимых случайных величины равно сумме их математических ожиданий\n\n\\[\n\\mathbb{E}(X_1 + X_2) = \\mathbb{E}X_1 + \\mathbb{E}X_2\n\\]\n\nКонстанту можно выносить за знак математического ожидания\n\n\\[\n\\mathbb{E}(cX) = c \\mathbb{E}X\n\\]\n\n\n\n6.1.2.1 Несмещенность\nНесмещённость выражает следующую идею: когда мы постоянно используем выборочную оценку нашего параметра на выборках некоторого объема, мы в среднем не ошибаемся в оценке параметра.\nТо есть, конечно, при каждой конкретной оценке нашего параметра на отдельной выборке мы будем совершать ошибку, однако в среднем при многократном повторении измерений и получения оценки мы будем попадать точно в цель — в параметр генеральной совокупности.\n\\[\n\\forall n \\; \\mathbb{E} \\hat \\theta = \\theta\n\\] где \\(n\\) — объём выборок.\nВыше мы рассматривали выборочное среднее как оценку математического ожидания генеральной совокупности. Давайте проверим, является ли такая оценка несмещнной. Для этого нам надо проверить, что выполняется следующее соотношение:\n\\[\n\\mathbb{E}(\\bar x) = \\mu\n\\]\nПусть у нас есть \\(n\\) выборок, на которых измерена переменная \\(X\\) — \\(X_1\\), \\(X_2\\), \\(\\dots\\), \\(X_n\\). Эти выборки пришли из одной генеральной совокупности, то есть \\(X_1, X_2, \\dots ,X_n \\overset{\\text{i.i.d}}{\\thicksim} (\\mu, \\sigma^2)\\). Запись \\(\\text{i.i.d.}\\) означает «независимые одинаково распределенные» (independent identically distributed).\nТогда получается следующее:\n\\[\n\\mathbb{E}(\\bar X) = \\mathbb{E}\\Big( \\frac{1}{n} (X_1 + X_2 + \\dots + X_n) \\Big) = \\frac{1}{n} \\Big( \\mathbb{E}(X_1) + \\mathbb{E}(X_2) + \\dots + \\mathbb{E}(X_n) \\Big)\n\\]\nПоскольку все \\(X_i\\) пришли из одного и того же распределения \\((\\mu, \\sigma^2)\\), то \\(\\forall i \\, \\mathbb{E}(x_i) = \\mu\\). Тогда\n\\[\n\\mathbb{E}(\\bar X) = \\frac{1}{n} \\cdot n \\cdot \\mu = \\mu\n\\]\nА это ровно то, что утверждается в качестве несмещённости. Таким образом, среднее является несмещенной оценкой математического ожидания.\nКроме среднего, у нас есть еще дисперсия, и мы её тоже всегда оцениваем в ходе анализа данных. На практике мы говорили, что дисперсия случайной величины определяется как \\(\\text{var}(X) = \\mathbb{E}(X^2) - \\big( \\mathbb{E}X\\big)^2\\). Эта формула удобна для расчетов, однако дисперсию можно определить и иначе:\n\\[\n\\text{var}(X) = \\mathbb{E}(X - \\mathbb{E}X)^2 = \\frac{\\sum_{i=1}^n(\\mu - x_i)^2}{n}\n\\]\nДве формулы, кстати, эквивалентны друг другу и одна выводится из другой:\n\\[\n\\begin{split}\n\\text{var}(x) &= \\mathbb{E}\\big( (X - \\mathbb{E}X )^2 \\big) = \\mathbb{E}\\big( X^2 - 2 X \\mathbb{E}X + (\\mathbb{E}X)^2 \\big) = \\\\\n& = \\mathbb{E}(X^2) - 2 \\mathbb{E}X \\mathbb{E}X + (\\mathbb{E}X)^2 = \\mathbb{E}(X^2) - 2 (\\mathbb{E}X^2) + (\\mathbb{E}X)^2 = \\\\\n& = \\mathbb{E}(X^2) - (\\mathbb{E}X^2)\n\\end{split}\n\\]\nИтак, в качестве оценки дисперсии, кажется, можно использоваться \\(\\text{var}(X) = \\frac{\\sum_{i=1}^n(\\mu - x_i)^2}{n}\\), однако из столкновения с реальностью мы знаем, что в знаменателе формулы, которую мы реально используем стоит \\(n-1\\). Почему?\nПроверим оценку дисперсии на несмещенность. Нам нужно показать, что \\(\\mathbb{E}(\\hat \\sigma^2) = \\sigma^2\\), тогда мы сможем сказать, что оценка является несмещенной. По формуле получается, что\n\\[\n\\begin{split}\n\\mathbb{E}(\\hat \\sigma^2) & = \\mathbb{E}\\Big( \\mathbb{E}(X^2) - (\\mathbb{E}X)^2 \\Big) = \\\\\n& = \\mathbb{E}\\Big( \\overline{X^2} - \\bar X^2\\Big) = \\mathbb{E}(\\overline{X^2}) - \\mathbb{E}(\\bar X^2)\n\\end{split}\n\\]\nРассмотрим сначала \\(\\mathbb{E}(\\overline{X^2})\\):\n\\[\n\\mathbb{E}(\\overline{X^2}) = \\mathbb{E}\\Big( \\frac{X_1^2 + X_2^2 + \\dots + X_n^2}{n} \\Big) = \\frac{1}{n} \\Big( \\mathbb{E}X_1^2 + \\mathbb{E}X_2^2 + \\dots + \\mathbb{E}X_n^2\\Big)\n\\]\nТак как все наблюдения приходят из одного и того же распределения, то все математические ожидания будут равны, поэтому:\n\\[\n\\mathbb{E}(\\overline{X^2}) = \\frac{1}{n} \\cdot n \\cdot \\mathbb{E}(X_i^2) = \\mathbb{E}(X_i^2)\n\\]\nТеперь \\(\\mathbb{E}(\\bar X^2)\\):\n\\[\n\\begin{split}\n\\mathbb{E}(\\bar X^2) &= \\mathbb{E}\\Big( \\frac{X_1 + X_2 + \\dots + X_n}{n} \\Big)^2 = \\\\\n& = \\frac{1}{n^2} \\mathbb{E}(X_1 + X_2 + \\dots + X_n)^2 = \\\\\n& = \\frac{1}{n^2} \\mathbb{E}(X_1^2 + X_2^2 + \\dots X_n^2 + 2X_1X_2 + \\dots + 2X_{n-1}X_n) = \\\\\n& = \\frac{1}{n^2} \\mathbb{E}\\Big( (X_1^2 + X_2^2 + \\dots X_n^2) + (2X_1X_2 + \\dots + 2X_{n-1}X_n) \\Big)\n\\end{split}\n\\]\nВнутри скобок получается два слагаемых: если с \\(X_i^2\\) все понятно — выше мы уже с ним сталкивались, то со вторым надо разбираться, а именно, подсчитать, сколько попарных произведений случайных величин у нас будет. Их будет \\(C_n^2 = \\frac{n(n-1)}{2}\\). Поэтому если мы будем раскрывать скобки, то получим следующее:\n\\[\n\\begin{split}\n\\mathbb{E}(\\bar X^2) & = \\frac{1}{n^2} \\cdot n \\cdot \\mathbb{E}(X_i^2) + \\frac{1}{n^2} \\cdot \\frac{n(n-1)}{2} \\cdot 2 \\mathbb{E}(X_iX_j) = \\\\\n& = \\frac{1}{n} \\mathbb{E}(X_i^2) + \\frac{n-1}{n} (\\mathbb{E}X_i)^2\n\\end{split}\n\\]\nТеперь соберем две части вместе:\n\\[\n\\begin{split}\n\\mathbb{E}(\\hat \\sigma^2) & = \\mathbb{E}(\\overline{X^2}) - \\mathbb{E}(\\bar X^2) = \\\\\n& = \\mathbb{E}(X_i^2) - \\frac{1}{n} \\mathbb{E}(X_i^2) - \\frac{n-1}{n} (\\mathbb{E}X_i)^2 = \\\\\n& = \\frac{n}{n} \\mathbb{E}(X_i^2) - \\frac{1}{n} \\mathbb{E}(X_i^2) - \\frac{n-1}{n} (\\mathbb{E}X_i)^2 = \\\\\n& = \\frac{n-1}{n} \\Big ( \\mathbb{E}(X_i^2) - (\\mathbb{E}X_i)^2 \\Big) = \\\\\n& = \\frac{n-1}{n} \\sigma^2\n\\end{split}\n\\]\nПолучается, что математическое ожидание нашей оценки оказывается равно не самому значению интересующего нас параметра, а значению параметра, умноженному на некоторое число \\(\\frac{n-1}{n}\\), то есть оценка является смещенной. Именно поэтому для расчета дисперсии на выборке используется выборочная, или исправленная, дисперсия.\nКак она исправляется? Если у нас оценка дисперсии отличается от значения параметра в \\(\\frac{n-1}{n}\\) раз, то надо домножить оценку на \\(\\frac{n}{n-1}\\):\n\\[\ns^2 = \\frac{n}{n-1} \\cdot \\hat \\sigma^2 = \\frac{n}{n-1} \\cdot \\frac{1}{n} \\sum (x_i - \\bar x)^2 = \\frac{1}{n-1} \\sum (x_i - \\bar x)^2\n\\]\nИ вот мы получили знакомую нам формулу для расчета выборочной дисперсии. Такая оценка является несмещенной.\nИногда несмещённость от оценки бывает потребовать сложно, тогда можно ограничиться ассимптотической несмещенностью:\n\\[\n(\\mathbb{E}\\hat \\theta - \\theta) \\underset{n \\rightarrow \\infty}{\\rightarrow} 0,\n\\]\nгде \\((\\mathbb{E}\\hat \\theta - \\theta)\\) — смещение. Ассимптотическая несмещенность требует, чтобы математическое ожидание нашей оценки приближалось к значению параметра с ростом объема выборки.\n\n\n6.1.2.2 Состоятельность\nМатематически состоятельность определяется следующим образом:\n\\[\n\\lim_{n \\rightarrow \\infty} \\mathrm{P}(|\\hat \\theta - \\theta| &lt; \\varepsilon) = 1, \\, \\varepsilon &gt; 0\n\\]\nСодержательно эта запись нам говорит следующее, что при неограниченном росте мощности выборки наша оценка стремится к истинному значению параметра. Или, проще, с ростом выборки значение нашей оценки все реже выпадает из некоторого достаточно узкого интервала \\((\\theta - \\varepsilon, \\theta + \\varepsilon)\\). Может быть, такая формулировка не совсем точна математически, но позволяет представить, что происходит.\nДавайте посмотрим на это на картинке. Нже изображено поведение состоятеной оценки с ростом выборки:\n\n\n\n\n\n\n\n\n\nА так ведет себя несостоятельная оценка:\n\n\n\n\n\n\n\n\n\n\n\n6.1.2.3 Эффективность\nЭффективность точечной оценки определяется достаточно просто. Так как оценка параметра — это случайная величина, но у неё есть дисперсия. Чтобы оценка была эффективна, её дисперсия должна быть минимальной:\n\\[\n\\sigma^2_{\\hat \\theta} = \\min\n\\]\nОпять же попробуем посмотреть на это на картинке:\n\n\n\n\n\n\n\n\n\n\n\n\n6.1.3 Интервальные оценки\nКроме самого значения оценки, необходимо определить качество этой оценки, иначе говоря — её точность. Для этого используется такая величина как надёжность:\n\\[\n\\gamma = \\mathbb{P}(\\theta_\\min &lt; \\theta &lt; \\theta_\\max)\n\\]\nТакая форма оценки называется интервальной оценкой параметра, так как мы указываем интервал, в котором находится истинное значение с определённой вероятностью.\nТакая форма оценки даёт исчерпывающую информацию о параметре: мы знаем (1) интервал, в котором находится значение параметра генеральной совокупности, а также (2) надёжность, с которой выбранный интервал накрывает это значение.\nЗначение надежности \\(\\gamma\\) может быть выбрано произвольно, но обычно оно близко к единице. Однако необходимо помнить, что чем выше надёжность, тем шире границы интервальной оценки.\n\n6.1.3.1 Стандартная ошибка\nДля того, что получить интервальную оценку нашего параметра, нам нужно изучить, как ведет себя наша выборочная оценка в случае, когда мы много раз извлекаем выборку из генеральной совокупности. Рассмотрим на примере среднего значения.\nПусть мы в том же примере с IQ, который обсуждали выше. Мы знаем, что распределение параметра в генеральной совокупности такое:\n\n\n\n\n\n\n\n\n\nВновь извлечем несколько выборок из нашей генеральной совокупности:\n\n\n\n\n\n\n\n\n\nМы уже отмечали, что на отдельной выборке мы будем получать оценку среднего, которая будет отличаться от значения параметра в генеральной совокупности. Однако если мы извлечем много выборок — скажем, 1000 выборок по 100 наблюдений — посчитаем на каждой среднее и построим распределение выборочных средних, то получим нечто такое:\n\n\n\n\n\n\n\n\n\nНаши средние будут как-то распределены, при это среднее средних будет оказываться очень близко с значению нашего параметра. При этом данное распределение крайне похоже на нормальное и может быть описано как \\(\\mathcal N(\\overline{\\bar x}, \\sigma_{\\bar x}^2)\\), то есть как и любое нормальное распределение получившееся распределение будет описываться некоторой дисперсией. Стандартное отклонение этого распределения называется стандартной ошибкой среднего (standard error of mean):\n\\[\n\\text{se}(\\bar x) = \\sqrt{\\sigma^2_{\\overline x}} = \\sigma_{\\overline x}\n\\]\nВ нашем случае оно будет равно 1.43.\nСтандартная ошибка среднего является одной из интервальных оценок среднего значения. Однако выше мы сказали, что для интервальной оценки нам надо указать надежность, то есть вероятность, с которой значение изучаемого параметра находится в интервале, задаваемом интервальной оценкой. Можем ли мы это сделать в случае стандартной ошибки? Да, поскольку мы знаем как устроено нормальное распределение. Так как стандартная ошибка является стандартным отклонением распределения выборочных средних, а в пределах одного стандартного отклонения от среднего лежит 68.2% значений нормально распределенной случайной величины, то мы можем записать:\n\\[\n\\begin{split}\n0.682 & = \\mathbb{P}(\\overline{\\bar x}-\\sigma^2_{\\overline x} &lt; \\mu &lt; \\overline{\\bar x}+\\sigma^2_{\\overline x}) \\\\\n& = \\mathbb{P}(98.57 &lt; \\mu &lt; 101.43)\n\\end{split}\n\\]\nТо, что мы проделали выше легитимизировано центральной предельной теоремой. Визуализацию можно найти здесь.\nЭто конечно хорошо, но мы же не можем каждый раз извлекать по 1000 выборок, чтобы рассчитать стандартную ошибку среднего. Нам необходим способ её расчета по одной выборке, чтобы мы могли производить интервальную оценку нашего параметра в отдельном исследовании. Такой способ подсчета есть, и он выглядит так:\n\\[\n\\text{se}_X = \\frac{\\text{sd}_X}{\\sqrt{n}} = \\frac{\\hat \\sigma^2_X}{\\sqrt{n}}\n\\]\nВообще-то такое равенство совершенно неочевидно, поэтому попробуем понять, почему оно справедливо.\n\n\n\n\n\n\nСвойства дисперсии\n\n\n\nДля того, чтобы вывести эту формулу нам потребуются два свойства дисперсии:\n\nДисперсия суммы двух независимых случайных величин равна сумме их дисперсий\n\n\\[\n\\text{var}(X + Y) = \\text{var}(X) + \\text{var}(Y)\n\\]\n\nКонстанта выносится из-под знака дисперсии с возведением в квадрат:\n\n\\[\n\\text{var}(aX) = a^2 \\text{var}(X)\n\\]\n\n\nТак как наблюдения извлекаются из независимых одинаково распределенных величин (independent identically distributed, iid), то они независимы. Получается что дисперсия распределения выборочных средних будет равна:\n\\[\n\\text{var}\\bar X_i = \\text{var}\\Big( \\frac{1}{n} \\sum X_i \\Big)\n\\]\nПо свойствам дисперсии:\n\\[\n\\begin{split}\n\\text{var}\\bar X_i & = \\text{var}\\Big( \\frac{1}{n} \\sum X_i \\Big) \\\\\n& = \\frac{1}{n^2} \\sum \\text{var}(X_i) = \\frac{1}{n^2} \\sum \\sigma^2 = \\frac{n}{n^2} \\sigma^2 = \\frac{\\sigma^2}{n}\n\\end{split}\n\\]\nТак как стандартная ошибка это старндартное отклонение распределения выборочных средних, то:\n\\[\n\\text{se}_X = \\sqrt{ \\text{var}\\Big( \\frac{1}{n} \\sum X_i \\Big)} = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\nВот и получается формула для расчета стандартной ошибки на одной выборке.\n\n\n6.1.3.2 Доверительный интервал\nДругим вариантом интервальной оценки является доверительный интервал (confidence interval). На практике он используется гораздо чаще стандартной ошибки, но оказывается тесно с ней связан. Итак, ещё раз:\n\\[\n\\mathrm{P}(\\theta_\\min &lt; \\theta &lt; \\theta_\\max) = \\gamma, \\; \\gamma \\rightarrow 1\n\\]\n\\(\\theta_\\min\\) и \\(\\theta_\\max\\) — границы доверительного интервала, \\(\\gamma\\) — доверительная вероятность. На практике её значение чаще всего принимается равным \\(0.95\\).\nЧто нам нужно, чтобы определить границы, в которых значение параметра лежит с вероятностью \\(0.95\\), если мы знаем, что наша оценка распределена нормально?\nМы знаем особое нормальное распределение \\(z \\thicksim \\mathcal N(0, 1)\\), которое называется стандартным нормальным распределением. Можно рассчитать границы следующего интервала:\n\n\n\n\n\n\n\n\n\nОказывается, что \\(z\\)-значение с вероятностью 0.95 попадается в интервал \\([z_{\\min}, z_{\\max}] = [-1.96, 1.96]\\). Зная эти точки и воспользовавшись принципом стандартизации, мы можем записать следующее:\n\\[\n\\mathbb{P}\\Big( \\bar x - z_\\min \\text{se}_X &lt; \\mu &lt;\n\\bar x + z_\\max \\text{se}_X \\Big) = \\gamma\n\\]\nИли конкретнее:\n\\[\n\\mathbb{P}\\Big( \\bar x - -1.96 \\text{se}_X &lt; \\mu &lt;\n\\bar x + 1.96 \\text{se}_X \\Big) = 0.95\n\\]\nТо есть, зная стандартную ошибку мы можем рассчитать доверительный интервал. В общем случае, любой — 95%, 90%, 99% и др. — подставляя разные \\(z_\\min\\) и \\(z_\\max\\).\n\n\n6.1.3.3 Интерпретация доверительного интервала\nС учетом расчета нам очень хочется сказать, что границы доверительного инетервала задают диапазон, в котором значение нашего параметра — математического ожидания в генеральной совокупности — лежит с вероятностью 0.95. Однако это не верно!\nЗдесь в игру вступает фреквентистский подход (см. далее), в котором мы с вами живем. Корректная интерпретация границ доверительного интервала звучит так:\n\nЕсли мы будет бесконечно извлекать новые выборки из генеральной совокупности, рассчитывать на них средние и 95% доверительные интервалы к ним, то генеральное среднее попадёт в границы 95% таких доверительных интервалов.\n\nТо есть, если мы извлечем 100 выборок, посчитаем на каждой из них среднее и построим 95% доверительный интервал к каждому из 100 средних, то 95 доверительных интервалов из 100 будут содержать генеральное среднее — а 5 интервалов содержать его не будут.\nВизуализацию этого можно наблюдать здесь.\nРеальная вероятность, что значение параметра генеральной совокупности попадет в пределы конкретного доверительного интервала, рассчитанного в данном исследовании, оказывается меньше — около 84.3%.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "l6.html#тестирование-гипотез",
    "href": "l6.html#тестирование-гипотез",
    "title": "6  L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез",
    "section": "6.2 Тестирование гипотез",
    "text": "6.2 Тестирование гипотез\nВ ходе статистического анализа мы, главным образом, заняты тем, что тестируем статистические гипотезы. Ведь на какого рода вопросы мы отвечаем с помощью анализа?\n\nРазличаются ли группы между собой?\nЗначимо ли влияние какого-либо фактора? → Различаются ли группы между собой?\nХороша ли та модель, которую мы построили? → Отличается ли она от нулевой модели?\n\nИ так далее. Так или иначе, всё сводится в тому, что мы ищем какие-то различия. Но в силу того, что у нас неопределённость и вариативность в данных, мы просто так «в лоб» сказать о различиях по оценкам параметров не можем. Приходится тестировать статистические гипотезы.\n\n6.2.1 Нулевая и альтернативная гипотезы\nЧто такое гипотеза и какие они вообще бывают?\n\nГипотеза (\\(H\\)) — это предположение, которое подлежит проверке на основе результатов наблюдений.\nГипотезы бывают:\n\nтеоретические — про конструкты\nэмпирические — про переменные\nстатистические — про параметры [генеральной совокупности] и данные\n\n\nСтатистические гипотезы бывают простыми и сложными:\n\nПростая гипотеза — это такое предположение, которое включает в себя какое-либо однозначно определеяемое утверждение. Например, истинная величина параметра соответствует некоторому строго заданному значению: \\(H : \\theta = \\theta_0\\). Другой вариант — две генеральные совокупности имеют одно и то же значение одной и той же характеристики: \\(H : \\theta_1 = \\theta_2\\).\nСложная гипотеза предполагает множественность вариантов для параметра, которые укладываются в рамки проверяемого предположения. Например, \\(H : \\theta &gt; \\theta_0\\) или \\(H : \\theta_1 \\neq \\theta_2\\).\n\nВ рамках самого хода тестирования гипотез существует проверяемая (нулевая) гипотеза (\\(H_0\\)). Её обычно стараются предельно упростить, поэтому она формулируется как простая гипотеза. В противовес ей выдвигается альтернативная гипотеза (\\(H_1\\)), которая будет иметь вид сложной гипотезы.\nДля проверки гипотезы необходимы две вещи:\n\nрезультаты наблюдений и\nкритерий.\n\nРезультаты наблюдений, полученные на выборке, сами по себе, как правило, не используются. Однако на их основе рассчитываются выборочные статистики (показатели), которые непосредственно участвуют в проверке гипотезы.\n\n\n6.2.2 Подходы к тестированию статистических гипотез\n\n6.2.2.1 Фреквентистский подход\nФреквентистский подход, широко распространенный в тестировании статистических гипотез, задается следующим вопросом:\n\nКакова вероятность получить такие данные, если допустить, что нулевая гипотеза верна?\n\nТо есть в этой логике мы собираем какие-то данные в ходе исследования, формулируем некоторую статистическую гипотезу \\(H_0\\) об отсутствии закономерности в генеральной совокупности, и далее определяем, насколько вероятно было бы получить вот эти данные, которые у нас сейчас есть, в случае, когда закономерности нет.\nДалее, если эта вероятность мала, то мы делаем вывод, что в генеральной совокупности закономерность всё-такие есть — поэтому мы получили данные, которые не характерны для случая, когда верна нулевая гипотеза. Если же мы получаем, что такая вероятность велика, то мы остаемся с нулевой гипотезой, которую не удалось отклонить.\nЭтот подход требует возможности много раз повторять наше исследование, чтобы проверять, действительно ли мы из раза в раз будем получать схожие результаты — то есть, фреквентистский подход хорошо работает в долгосрочной перспективе и именно когда у нас есть возможность повторять наше исследование, в которых мы тестируем одни и те же гипотезы, мы можем получать достаточно однозначные выводы об изучаемых закономерностях.\n\n\n6.2.2.2 Байесовский подход\nЕсли мы внимательно вдумаемся в тот вопрос, которым задается фреквентистская статистика, мы поймем, что он достаточно тупой: нас вообще-то не интересует вероятность получить вот эти данные, если нулевая гипотеза верна. Почему? Потому что мы эти данные уже получили. Гораздо более интересным является вопрос о том,\n\nнасколько вероятна справедливость нулевой или альтернативной гипотезы при условии, что мы получили такие данные.\n\nЭтим вопросом задается байесовская статистика, которая постепенно набирает популярность среди исследователей. Сложно сказать, какой из двух подходов лучше — у каждого есть свои плюсы, у каждого есть свои минусы, и каждый обладает своими особенностями. В целом, они хорошо дополняют друг друга, и одни и те же данные могут быть проанализированы как с помощью байесовского, так и с помощью фреквентистского подхода.\nОднако в нашем курсе мы будем идти классическим путем и рассматривать тестирование статистических гипотез во фреквентистском подходе.\n\n\n\n6.2.3 Возможные результаты проверки гипотез\nВ результате проверки статистических гипотез могут возникнуть четыре ситуации.\nМы изучаем в исследовании какую-либо закономерность, которая в реальном мире может существовать, а может и не существовать. В силу неопределённости и вариативности наших данных мы может либо обнаружить интересующую нас закономерность, либо не обнаружить.\nВ качестве нулевой гипотезы мы выдвигаем предположение о том, что закономерность отсутствует — так мы упрощаем нашу нулевую гипотезу. Пусть \\(H_0\\) обозначает, что предположение, которое мы проверяем справедливо, а \\(H_1\\) — не справедливо. На основании данных мы можем либо не отклонить наше предположение (\\(\\hat H_0\\)), либо отклонить (\\(\\hat H_1\\)).\nТогда имеем следующую ситуацию:\n\n\n\n\n\\(H_0\\)\n\\(H_1\\)\n\n\n\n\n\\(\\hat H_0\\)\n✓\nОшибка II рода\n\n\n\\(\\hat H_1\\)\nОшибка I рода\n✓\n\n\n\n\nОшибка I рода возникает, когда в генеральной совокупности искомой закономерности нет, но мы в силу случайных флуктуаций в данных её нашли.\nОшибка II рода возникает, когда в генеральной совокупности искомая закономерность есть, но мы в силу каких-либо причин её не нашли.\n\nОшибки — это нехорошо, они нас не устраивают. Надо каким-то образом их контролировать.\n\nОшибка I рода контролируется достаточно просто. Так как мы нашли закономерность, которую искали, мы можем посчитать вероятность, с которой потенциально ошиблись. А собственно контролировать ошибку мы будем с помощью уровня значимости \\(\\alpha\\), который выбирается до начала процедуры тестирования гипотезы. Он и задает вероятность, с который мы позволяем себе ошибиться — отклонить нулевую гипотезу, при условии, что она верна.\nОшибку II рода контролировать сложнее, так как мы не нашли закономерность, которую искали. Нам нужна какая-то метрика, которая позволит сказать, что мы сделали всё возможное для того, чтобы обнаружить искомую закономерность. Вероятность ошибки II рода обозначается \\(\\beta\\) — тогда вероятность того, что мы не совершили ошибку II рода будет \\(1 - \\beta\\). Эта величина называется статистической мощностью, и она связана с размером эффекта и объемом выборки. Статистическую мощность рассчитывают до проведения статистического анализа — она помогает определить требуемый объема выборки.\n\nСоберем все обозначения в единую табличку2:\n\n\n\n\n\n\n\n\n\n\\(H_0\\)\n\\(H_1\\)\n\n\n\n\n\\(\\hat H_0\\)\n\\(\\mathrm P (\\hat H_0 | H_0)\\)\n\\(\\mathrm P (\\hat H_0 | H_1) = \\beta\\)\n\n\n\\(\\hat H_1\\)\n\\(\\mathrm P (\\hat H_1 | H_0) = \\alpha\\)\n\\(\\mathrm P (\\hat H_1 | H_1) = 1 - \\beta\\)\n\n\n\nУровень значимости \\(\\alpha\\) выбирается близким к нулю — всем знакомо конвенциональное значение \\(0.05\\). Вообще \\(\\alpha\\) можно выбрать сколь угодно малым, однако при выборе уровня значимости руководствуются принципом разумной достаточности, так как если устремить \\(\\alpha\\) к нулю, то устремиться к нулю и вероятность отклонения нулевой гипотезы.\n\\[\n\\mathrm P (\\hat H_1) = \\mathrm P (\\hat H_1 | H_0) \\cdot \\mathrm P (H_0) = \\alpha \\cdot \\mathrm P(H_0)\n\\]\nДостаточной статистической мощностью считается \\(0.8\\). Аналогично, устремляя мощность к единице (\\((1 - \\beta) \\rightarrow 1 \\Rightarrow \\beta \\rightarrow 0\\)), мы устремляем вероятность не отклонения нулевой гипотезы к нулю:\n\\[\n\\mathrm P (\\hat H_0) = \\mathrm P (\\hat H_0 | H_1) \\cdot \\mathrm P (H_1) = \\beta \\cdot \\mathrm P (H_1)\n\\]\n\n6.2.3.1 Асимметрия статистического вывода\nВыше мы сказали, что для проверки гипотезы нужны две вещи:\n\nрезультаты наблюдений и\nкритерий.\n\nС результатами наблюдений более-менее очевидно.\nКритерий — это правило, согласно которому гипотезу либо принимают, либо отклоняют. Однако перед тем как проверять гипотезу, её так-то нужно сформулировать, и сделать это правильно, поскольку от формулировки гипотезы зависит интерпретация результатов проверки и дальнейшее использование полученной информации.\nИспользуемая статистика сама по себе является [непрерывной] случайной величиной, а значит может быть построено её распределение. Критерий будет разделять это распределение на непересекающиеся области. В результате чего возникает критическая область — область отклонения гипотезы. Дополнением к ней является область неотклонения гипотезы.\nКритическая область может быть односторонней (при \\(H_1:\\theta &gt; \\theta_0\\) или \\(H_1: \\theta &lt; \\theta_0\\)) и двусторонней (при \\(H_1:\\theta \\neq \\theta_0\\)). «Размер» критической области определяется уровнем значимости.\nСтатистический вывод — заключение о том, получили ли мы подтверждение альтернативной гипотезы — по структуре представляет собой импликацию и звучит так:\n\nЕсли значение статистики критерия попало в критическую область, то у нас есть основания отклонить нулевую гипотезу в пользу альтернативной\n\nК чему приводит факт, что это утверждение является имликацией:\n\nЕсли значение нашей статистики, которое мы рассчитали на выборке, попало в критическую область, то мы говорим о том, что нулевая гипотеза отклоняется.\nЕсли значение нашей статистики, которое мы рассчитали на выборке, не попало в критическую область, то мы не получаем оснований для того, чтобы отклонить нулевую гипотезу. Однако мы также не получаем оснований, чтобы её «принять». Мы остаёмся в некотором неведении: мы не нашли различий, а есть они там или нет — хто ж их знает… Итого, мы не можем сделать никакого вывода.\n\nВ этом и заключается асимметрия статистического вывода. Как раз для того, чтобы с ней как-то жить, мы работаем со статистической мощностью.\n\n\n6.2.3.2 Связь ошибки первого и второго рода\nНеобходимо также помнить, что ошибки первого и второго рода связаны между собой так, что\n\\[\n\\alpha \\rightarrow 0 \\Rightarrow \\beta \\rightarrow 1\n\\]\n\\[\n\\begin{split}\n\\beta \\cdot \\mathrm P (H_1) & = \\mathrm P (\\hat H_0) = \\mathrm P (\\hat H_0 | H_0) \\cdot \\mathrm P (H_0) \\Rightarrow \\\\\n\\beta & = \\frac{1}{\\mathrm P (H_1)} \\cdot \\mathrm P (H_0) \\cdot \\mathrm P(\\hat H_0 | H_0) \\\\\n\\beta & = \\frac{1}{\\mathrm P (H_1)} \\cdot \\big (1 - \\mathrm P (H_1 | H_0)\\big) = \\frac{1}{\\mathrm P (H_1)} \\cdot \\mathrm P (H_0) \\cdot (1 - \\alpha)\n\\end{split}\n\\]\n\n\n\n6.2.4 Алгоритм тестирования статистических гипотез\nДля тестирования гипотез есть два сценария: первый и тот, которым мы будем пользоваться. Первый вариант чуть более классический, второй — более гибкий.\nСценарий номер раз\n\nФормулировка гипотезы\nВыбор статистического критерия\nВыбор уровня значимости \\(\\alpha\\)\nПостроение закона распределения статистики критерия при условии, что нулевая гипотеза верна\nОпределение границ критической области\nРасчёт выборочной статистики\nОпределение, попадает ли наблюдаемое значение статистики в критическую область и вынесение решения\n\nСценарий номер два\n\nФормулировка гипотезы\nВыбор статистического критерия\nВыбор уровня значимости \\(\\alpha\\)\nПостроение закона распределения статистики критерия при условии, что нулевая гипотеза верна\nРасчёт выборочной статистики\nРасчёт достигнутого уровня значимости p-value\nСопоставление \\(\\alpha\\) и p-value и вынесение решения\n\nПочему второй вариант более гибкий? Представим, что мы захотели понизить уровень значимости с \\(0.05\\) до \\(0.01\\) — такие уровни значимости встречаются, например, в медицине. Если мы идем по первому сценарию, то нам надо заново пересчитать критические значения и вновь проанализировать, попадает ли наблюдаемое значение в критическую область. Если мы адепты второго сценария, то нам надо только выполнить одно новое сравнение нашего p-value с новым уровнем значимости.\n\n\n6.2.5 Размер эффекта и статистическая мощность\nОшибку второго рода контролировать сложнее, чем ошибку первого рода, так как мы не обнаруживаем закономерность. Собственно, ошибка второго рода соответствует ситуации, когда мы не обнаружили закономерность при условии, что закономерность в генеральной совокупности присутствует. На эту вероятность влияет «размер» закономерности, её «сила», в генеральной совокупности. Численным выражением силы взаимосвязи в генеральной совокупности является размер эффекта (effect size).\nИз-за того, что в случае ошибки второго рода мы не можем работать с её вероятностью \\(\\beta\\) — опять же, так как мы не обнаруживаем в этом случае закономерность — мы работает с вероятностью \\(1-\\beta = \\mathbb{P}(\\hat H_1|H_1)\\). Это вероятность, с которой мы обнаружим закономерность при условии, что в генеральной совокупности закономерность есть. Эта величина называется статистическая мощность (statistical power) исследования.\nСтатистическая мощность зависит от размера эффекта и объема выборки. Вопрос: как размер эффекта, статистическая мощность и объем выборки соотносятся между собой?\n\nЧем больше размер эффекта, тем меньшую по объему выборку нам необходимо набрать, чтобы достигнуть требуемой статистической мощности.\nЧем больше выборка, тем выше статистическая мощность исследования.\n\n\nПосмотреть, как все эти штуки друг с другом соотносятся можно тут.\n\n\n\n6.2.6 Ложноположительный вывод\n\n6.2.6.1 Проблема множественных сравнений\nИтак, мы сравниваем попарно все группы наблюдений между собой. В каждом сравнении мы фиксируем вероятность ошибки первого рода с помощью уровня значимости на уровне \\(0.05\\). А какова будет вероятность ошибки, если мы проводим несколько сравнений?\nСчитаем, что наши сравнения независимы, поэтому вероятности будут перемножаться. Если вероятность ошибиться в одном сравнении равна \\(\\alpha\\), то вероятность сделать правильный вывод — \\(1 - \\alpha\\). Тогда вероятность сделать правильный вывод в \\(m\\) сравнениях — \\((1 - \\alpha)^m\\). Отсюда мы можем вывести вероятность ошибиться хотя бы в одном сравнении:\n\\[\n\\mathbb{P}^′ = 1 - (1 - \\alpha)^m\n\\]\nПусть у нас есть три группы, которые нам надо сравнить друг с другом — получается необходимо провести три сравнения. Итого вероятность ошибиться получается:\n\\[\n\\mathbb{P}^′ = 1 - (1 - 0.05)^3 \\approx 0.143\n\\]\nЗначительно больше, чем \\(0.05\\), что нехорошо. И дальше только хуже. Поэтому нам надо либо корректировать уровень значимости, либо использовать мощные методы типа дисперсионного анализа.\n\n\n\n\n\n\n\n\n\n\n6.2.6.1.1 Корректировка уровня значимости\nКорректировать уровень значимости можно по-разному. Например, можно разделить \\(\\alpha\\) на количество попарных сравнений — такой способ называется поправкой Бонферрони (Bonferroni):\n\\[\n\\alpha’ = \\frac{\\alpha}{n},\n\\]\nгде \\(n\\) — число попарных сравнений.\nПоправка Бонферрони считается самой консервативной поправкой — она достаточно сильно уменьшает уровень значимости, и мы можем не поймать искомую закономерность, то есть совершить ошибку второго рода. Поэтому придумали более либеральные поправки, например, поправку Холма (Холма–Бонферрони, Holm) или поправку Тьюки (Tukey’s HSD test).\nНа практике в силу того, что в статистических пакетах мы работаем с p-value, корректируется именно его значение по достаточно незамысловатой логике. Здесь: вариант для поправки Бонферрони.\n\\[\np &lt; \\frac{\\alpha}{n} \\Rightarrow np &lt; \\alpha\n\\]\nТаким образом, мы просто сравниваем уже скорректированное p-value, которое нам считает программа, с тем же самым \\(\\alpha = 0.05\\). Жизнь становится значительно проще и приятнее.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "l6.html#footnotes",
    "href": "l6.html#footnotes",
    "title": "6  L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез",
    "section": "",
    "text": "Шкала IQ устроена так, что её среднее значение равно 100, а стандартное отклонение 15.↩︎\nЗдесь использовано обозначение условной вероятности \\(\\mathrm P(A|B)\\), то есть это вероятность того, что случилось событие \\(A\\) при условии, что случилось событие \\(B\\).↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "l7.html",
    "href": "l7.html",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "",
    "text": "7.1 Описательные статистики",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#andan-descriptives",
    "href": "l7.html#andan-descriptives",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "",
    "text": "7.1.1 Виды статистики\nСтатистика [как набор методов и инструментов] делится на два вида — описательная статистика и статистика вывода.\n\nОписательная статистика (descriptive statistics1) занимается обработкой статистических данных, их наглядным представлением, и собственно описанием через некоторые характеристики.\n\nЭти характеристики, количественно описывающие особенности имеющихся данных, называются описательными статистиками (descriptive statistics2).\nЗадача описательной статистики — ёмко описать имеющиеся данные и составить на основе этих описаний общее представление о них, а также обнаружить особенности, которые могут повлиять на дальнейший анализ.\n\nСтатистика вывода (inferential statistics) занимается поиском ответов на содержательные вопросы, которые мы задаем данным в ходе их анализа в рамках научных и практических исследований.\n\nСостоит из двух компонентов — тестирования статистических гипотез и статистических методов.\n\n\n\n\n\n\n\n\nЗамечание о машинном обучении\n\n\n\nВ названии курса упомянуто «машинное обучение». Иногда его причисляют к статистике, иногда рассматривают отдельно. На самом же деле, статистические методы лежат где-то между статистикой вывода и машинным обучением.\nПочему?\nДело в том, что на статистические методы можно смотреть по-разному.\n\nЕсли нашей задачей является поиск ответов на исследовательские вопросы о закономерностях, о связи каких-либо факторов или влиянии переменных друг на друга, то мы будем смотреть на статистические модели с точки зрения статистики вывода. Это позволит нам находить ответы на интересующие нас вопросы — причем не важно, говорим мы о научных исследованиях или об исследованиях в индустрии.\nЕсли перед нами стоит задача хорошо предсказывать одни переменные на основании значений других — например, выдавать рекомендации на Яндекс.Музыке или в Яндекс.Лавке — то мы будем смотреть на те же статистические модели с точки зрения машинного обучения.\n\nТо есть, модели в анализе данных и машинном обучении одни и те же, но то, какую модель мы назовем хорошей и как мы эту «хорошесть» определим, будет отличаться в зависимости от задачи — исследовательская или предиктивная — которая перед нами стоит.\n\n\n\n\n7.1.2 Меры центральной тенденции\nИтак, мы хотим описать наши данные. Точнее, распределения переменных, которые у нас в данных есть. Хотим сделать это просто и ёмко. Насколько просто и ёмко? Ну, допустим максимально — одним числом. Для этого неплохо подойдет значение переменной, которое лежит в центре распределения.\nКак мы будем искать, что там в центре распределения? Зависит от шкалы, в которой измерена конкретная переменная.\n\n\n\nШкала\nМера центральной тенденции\n\n\n\n\nНоминальная\nМода\n\n\nПорядковая\nМедиана\n\n\nИнтервальная\nСреднее арифметическое\n\n\nАбсолютная\nСреднее арифметическое, геометрическое и др.\n\n\n\nОднако есть некоторые нюансы.\n\n7.1.2.1 Мода\nСамый простой вариант найти центральную тенденцию — это определить наиболее часто встречающееся значение переменной. Это значение называется модой (mode).\n\nОпределение 7.1 Мода [дискретной переменной] — наиболее часто встречающееся значение данной переменной.\n\nНапример, у нас есть следующий ряд наблюдений по какой-то переменной:\n\\[\n\\begin{bmatrix}\n1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1\n\\end{bmatrix}\n\\]\nЕсли мы посчитаем, сколько раз встретилась каждое значение переменной и составим таблицу частот, то получим следующее:\n\\[\n\\begin{matrix}\n\\text{Значение} & 1 & 2 & 3 & 4 & 6 \\\\\n\\text{Частота}  & 2 & 2 & 4 & 2 & 1\n\\end{matrix}\n\\]\nОчевидно, что \\(3\\) встречается чаще других значений — это и есть мода.\nПонятно, что если на нашей шкале нет чисел, а есть текстовые лейблы, это ничего не меняет. Пусть у нас есть переменная с кодами аэропортов:\n\\[\n\\begin{bmatrix}\n\\text{DME} & \\text{LED} & \\text{IST} & \\text{AER} & \\text{IST} &\\text{SVO} & \\text{LED} & \\text{VKO} & \\text{LED} & \\text{IST} & \\text{IST} & \\text{VKO} & \\text{AER} & \\text{DME}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{matrix}\n\\text{Значение} & \\text{DME} & \\text{LED} & \\text{IST} & \\text{AER} & \\text{SVO} & \\text{VKO}\\\\\n\\text{Частота}  & 2 & 3 & 4 & 2 & 1 & 2\n\\end{matrix}\n\\]\nМода — \\(\\text{IST}\\) (Международный аэропорт Стамбула, İstanbul Havalimanı).\nТак мы действуем в случае с эмпирическим распределением. Если нам известна функция вероятности переменной (probability mass function, PMF), то мы можем определить моду, основываясь на ней:\n\nОпределение 7.2 Мода [дискретной переменной] — это значение переменной, при котором её функция вероятности принимает своё максимальное значение.\n\n\\[\n\\text{mode}(X) = \\arg \\max(\\text{PMF}(X)) = \\arg \\max_{x_i}(\\mathbb{P}(X = x_i)),\n\\tag{7.1}\\]\nгде \\(X\\) — дискретная случайная величина, \\(x_i\\) — значение этой случайной величины.\n\n\n\n\n\n\n\n\nРисунок 7.1: Определение моды с помощью функции вероятности\n\n\n\n\n\nОкей, мы видим, что мода отлично считается на дискретных переменных. А как же быть с непрерывными?\nНапомним себе, что вероятность того, что непрерывная случайная величина примет своё конкретное значение, равна нулю. Из этого следует, что все значения непрерывной случайное величины уникальны — каждое повторяется только один раз. Получается, что строить частотную таблицу бессмысленно…\nПо этой причине для непрерывных переменных моду не считают.\n\n7.1.2.1.1 Мода для непрерывной переменной\nДа, это так. Действительно, посчитать моду для непрерывной переменной способом, аналогичным тому, что мы увидели выше, не получится. Однако математиков это не остановило.\nЕсли мы посмотрим на график плотности вероятности (probability density function, PDF), который является аналогом PMF для дискретных переменных, мы увидим, что какие-то значения встречаются чаще, а какие-то реже. Что в общем-то логично. Напомним себе, как это выглядит, например, для любимого [стандартного] нормального распределения:\n\n\n\n\n\n\n\n\nРисунок 7.2: Частоты интервалов значений непрерывной случайной величины на функции плотности распределения\n\n\n\n\n\nТо есть, самые часто встречающиеся значения — это пик распределения. Там и должна быть мода. Визуально это выглядит достаточно справедливо.\nМатематики так и решили:\n\nОпределение 7.3 Мода [непрерывной переменной] — это значение переменной, при котором её функция плотности вероятности достигает локального3 максимума.\n\n\\[\n\\text{mode}(X) = \\arg \\max(\\text{PDF}(X)) = \\arg \\max_{x \\in S}f(x),\n\\tag{7.2}\\]\nгдe \\(X\\) — непрерывная случайная величина, \\(x\\) — значение этой случайной величины, \\(S\\) — имеющаяся выборка значений переменной.\n\n\n\n\n\n\n\n\nРисунок 7.3: Положение моды на функции плотности [стандартного] нормального распределения\n\n\n\n\n\nХотя моду для непрерывной переменной вычислить можно, обычно этого не делают, так как достаточно других мер центральной тенденции для описания распределения.\n\n\n\n7.1.2.2 Унимодальные и полимодальные распределения\nНормальное распределение, как и ряд других — биномиальное, отрицательное биномиальное, пуассоновское — относятся к унимодальным. Такие распределения имеют только одну моду (см. Рисунок 7.4, Рисунок 7.5, Рисунок 7.6).\n\n\n\n\n\n\n\n\nРисунок 7.4: Нормальное распределение (μ = 2$, σ = 0.5). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.5: Биномиальное распределение (n = 50, p = 0.3). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.6: Распределение Пуассона (λ = 5.5). Пунктирной линией обозначено положение моды.\n\n\n\n\n\nЭто теоретические распределения. С эмпирическими распределениями дело обстоит так же, хотя они обычно менее гладенькие и красивые (см. Рисунок 7.7 и Рисунок 7.8).\n\n\n\n\n\n\n\n\nРисунок 7.7: Эмпирическое распределение, сгенерированное из нормального распределения (μ = 8, σ = 4, n = 100). set.seed(314). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.8: Эмпирическое распределение, сгенерированное из логнормального распределения (μ = 1.1, σ = 1.39, n = 30). set.seed(314). Пунктирной линией обозначено положение моды.\n\n\n\n\n\nОднако на практике возможны и другие ситуации. Например, такие (Рисунок 7.9, Рисунок 7.10):\n\n\n\n\n\n\n\n\nРисунок 7.9: Бимодальное распределение. Сгенерировано из двух нормальных распределений (μ1 = 1.5, σ1 = 0.4, n1 = 80; μ2 = 4, σ2 = 0.5, n2 = 40). set.seed(65). Пунктирными линиями обозначены положения мод.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.10: Полимодальное распределение. Сгенерировано из двух нормальных распределений (μ1 = 1.5, σ1 = 0.3, n1 = 80; μ2 = 3.4, σ2 = 0.5, n2 = 40) и бета-распределения (α = 2, β = 4, n = 50). set.seed(65). Пунктирными линиями обозначены положения мод.\n\n\n\n\n\nВ первом случае (Рисунок 7.9) мы видим два локальных максимума функции плотности вероятности — такое распределение называется бимодальным. Во втором случае (Рисунок 7.10) функция плотности вероятности имеет три локальных максимума — такое распределение называется полимодальным. Бимональное распределение является частным случаем полимодального распределения.\nВ прицнипе, пиков может быть и больше, однако при работе с реальными данными чаще всего мы сталкиваемся с бимодальными распределениями.\nЧто это значит и что с этим делать?\nБимодальное распределение сигнализирует нам о гетерогенности выборки. Если мы видим два выделяющихся пика, стоит подумать о том, что наша выборка неоднородна и в ней выделяются две подвыборки. Посмотрим на структуру выборки из примера выше (Рисунок 7.11):\n\n\n\n\n\n\n\n\nРисунок 7.11: Структура бимодального распределения из Рисунок 7.9. Для удобства сопоставления графиков плотностей вероятности по оси ординат отложены частоты.\n\n\n\n\n\nДействительно, наше распределение состоит из двух других распределений, у каждого из которого есть своя мода — поэтому итоговое распределение получается бимодальным. Конечно, сейчас нам это очень удобно показать, потому что мы знаем, как это распределение генерировалось. Когда же у нас есть реальные данные и мы там наблюдаем такого «верблюда», бывает достаточно сложно сказать, что «пошло не так».\nСамо по себе распределение не даст нам ответ на вопрос, почему оно бимодальное — чтобы выяснить причины такого поведения переменной нам потребуются другие данные. Обычно у вас в данных есть «соцдем» — пол, возраст, сфера и место работы, уровень обрвазования и др. Попробуйте построить распределение с разбиением исследуемой бимодальной переменной по переменным «соцдема». Это, к сожалению, не является рецептом успеха, поскольку причина гетерогенности выборки может и не содержаться в ваших данных, но такое изучение данных станет хорошим показателем того, что вы не просто «забили» на странное распределение своей переменной, а поисследователи возможные его причины.\nЕсли вам удалось найти причины гетерогенности выборки — допустим, у вас выделяются подвыборки «бакалавры» и «магистры» — стоит подумать о том, как обойтись с этой переменной в планируемом анализе, так как игнорировать её, по-видимому, нельзя, поскольку она влияет на вариатиность данных.\n\n\n\n\n\n\nСоцдем лишним не бывает\n\n\n\nНа этапе планирования исследования подумайте о том, чем могут отличаться ваши респонденты или испытуемые между собой, помимо индивидуальных различий.\n\nЕсли в эксперименте используете задачу мысленного вращения (mental rotation), вполне возможно, испытуемые, работающие в сфере 3D-моделирования или дизайна интерьеров, могут сформировать подвыборку.\nВ случае HR-исследования, где фиксируется доход респондента, необходимо записать город, в котором он проживает и/или работает.\nПри изучении удовлетворенности городским пространством важными пунктами станут беременность, наличие/отсутствие детей, наличие/отсутствие автомобиля и др.\n\nИ так далее. Примеров для каждого случая можно подобрать много.\nСтоит ли, скажем, в первом случае сразу исключить из выборки 3D-моделлеров? Зависит. От количества времени и денег на проведение исследования. Однако как минимум эту информацию надо зафиксировать в данных. А решить, исключать ли этих респондентов из выборки или нет, можно и позже. Главно об этом написать в отчете/статье, когда будете описывать предобработку данных.\n\n\n\n\n7.1.2.3 Медиана\nДля номинальной шкалы мода — это единственно возможная мера центральной тенденции, потому что на этой шкале отсутствует порядок элементов. На других шкалах наблюдения уже можно сортировать по возрастнию или убыванию, поскольку начиная с ранговой (порядковой) шкалы на всех них определена операция сравнения на «больше-меньше».\nВозьмем тот же ряд наблюдений, что и в предыдущем разделе:\n\\[\n\\begin{bmatrix}\n1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1\n\\end{bmatrix}\n\\]\nОтсортируем наблюдения по возрастанию:\n\\[\n\\begin{bmatrix}\n1  & 1 & 2 & 2 & 3 & 3 & 3 & 3 & 4 & 4 & 6\n\\end{bmatrix}\n\\]\nНаша задача — определить центральную тенденцию. Давайте посмотрим, что оказалось в середине отсортированного ряда:\n\\[\n\\begin{bmatrix}\n1 & 1 & 2 & 2 & 3 & \\mathbf{3} & 3 & 3 & 4 & 4 & 6\n\\end{bmatrix}\n\\]\nЭто медиана. В данном случае она равна \\(3\\).\n\nОпределение 7.4 Медиана (median) — это значение, которое располагается на середине отсортированного ряда значений переменной.\n\nМедиана делит все наблюдения переменной ровно пополам и половина наблюдений оказывается по одну сторону от медианы, а половина — по другую.\nЕсли число наблюдений нечётное, то всё ясно — в середине отсортированного ряда будет какое-то значение. А если число наблюдений чётное? Тогда мы попадаем между значениями.\nВозьмем для примера такой вектор наблюдений:\n\\[\n\\begin{bmatrix}\n14 & 10 & 9 & 16 & 30 & 3 & 25 & 8 & 18 & 7\n\\end{bmatrix}\n\\]\nОтсортируем:\n\\[\n\\begin{bmatrix}\n3 & 7 & 8 & 9 & 10 & 14 & 16 & 18 & 25 & 30\n\\end{bmatrix}\n\\]\nНайдем середину:\n\\[\n\\begin{bmatrix}\n3 & 7 & 8 & 9 & 10 & | & 14 & 16 & 18 & 25 & 30\n\\end{bmatrix}\n\\]\nВ таком случае в качестве медианы берется среднее между двумя срединными значениями:\n\\[\n\\text{median} = \\frac{10 + 14}{2} = 12\n\\]\nИтого, формализовать вычисление медианы можно следующим образом:\n\\[\n\\text{median}(X) = X(a) =\n\\cases{\nX\\left(\\frac{n+1}{2}\\right), & if  2 | n \\\\\n\\dfrac{X(\\frac{n}{2}) + X(\\frac{n}{2} + 1)}{2}, & otherwise\n}\n\\tag{7.3}\\]\nгде \\(X\\) — ряд наблюдений случайной величины, \\(n\\) — число наблюдений, \\(X(a)\\) — наблюдение с индексом \\(a\\) в отсортированном векторе \\(X\\).\nЕсли мы будем смотреть на медиану с позиции описания распределения, то она будет той самой линией, которая разделит площадь под графиком функции плотности вероятности пополам:\n\n\n\n\n\n\n\n\nРисунок 7.12: Медиана нормального распределения\n\n\n\n\n\nПри этом форма распределения не имеет значения — площадь под графиком всегда будет делиться пополам:\n\n\n\n\n\n\n\n\nРисунок 7.13: Медиана распределения с отрицательной асимметрией.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.14: Медиана распределения с положительной асимметрией.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 7.15: Медиана бимодального распределения.\n\n\n\n\n\n\n\n\n7.1.2.4 Среднее\nЕсли наша переменная измерена в самых мощных шкалах — интервальной или абсолютной — то нам доступна ещё одна мера центральной тенденции.\n\n7.1.2.4.1 Арифметическое среднее\nС этим существом все знакомы еще со школы. Арифметическое среднее (arithmetic mean, mean, average) считается так:\n\\[\n\\mathbb{M}_X = \\bar X = \\dfrac{\\sum_{i=1}^{n}x_i}{n},\n\\]\nгде \\(\\bar X\\) — среднее арифметическое, \\(x_i\\) — наблюдение в векторе \\(X\\), \\(n\\) — количество наблюдений.\nНу, то есть всё сложить и поделить на количество того, чего сложили. Изи.\n\n7.1.2.4.1.1 Свойства среднего арифметического\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то среднее увеличится на это же константу.\n\n\\[\n\\mathbb{M}_{X+c} = \\mathbb{M}_X + c\n\\]\nВот почему:\n\\[\n\\mathbb{M}_{X+c} = \\frac{\\sum_{i=1}^n (x_i + c)}{n} = \\frac{\\sum_{i=1}^n x_i + nc}{n} = \\frac{\\sum_{i=1}^n x_i}{n} + c = \\mathbb{M}_X + c\n\\]\nИначе говоря, распределение просто сдвинется. Например, если к каждому значению синего распределения прибавить \\(2\\), получится красное:\n\n\n\n\n\n\n\n\n\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то среднее увеличится во столько же раз.\n\n\\[\n\\mathbb{M}_{X \\times c} = \\mathbb{M}_X \\times c\n\\]\nВот почему:\n\\[\n\\mathbb{M}_{X \\times c} = \\frac{\\sum_{i=1}^n (x_i \\times c)}{n} = \\frac{c \\times \\sum_{i=1}^n x_i}{n} = \\frac{\\sum_{i=1}^n x_i}{n} \\times c = \\mathbb{M}_X \\times c\n\\]\nНапример, здесь каждое значение синего распределения умножили на \\(3\\) и получили красное:\n\n\n\n\n\n\n\n\n\nТут, правда, явно что-то ещё произошло, но мы пока этого не знаем. Однако, отметит этот факт.\n\nСумма отклонений от среднего значения равна нулю.\n\n\\[\n\\sum_{i=1}^n(x_i - \\bar X) = 0\n\\]\nЭлегантное доказательство:\n\\[\n\\begin{split}\n\\sum_{i=1}^n(x_i - \\bar X) & = \\sum_{i=1}^n x_i - \\sum_{i=1}^n \\bar X = \\sum_{i=1}^n x_i - n \\bar X = \\\\\n& = \\sum_{i=1}^n x_i - n \\times \\frac{1}{n} \\sum_{i=1}^n x_i = \\sum_{i=1}^n x_i - \\sum_{i=1}^n x_i = 0\n\\end{split}\n\\]\nНо можно это осмыслить и более просто графически.\nОтклонение — это разность между средним и конкретным значением переменной. И, действительно, так как среднее находится в центре распределения, то часть значений лежит справа, а часть слева. Значит, будут как положительные, так и отрицательные отклонения — и их сумма в итоге будет равна нулю.\n\n\n\n\n\n\n\n\n\nСреднее арифметическое не одиноко — есть и другие. Встретяться они вам примерно нигде — то есть о-о-о-очень редко и, скорее всего, в каком-то изощрённом виде. Но упомянуть их, пожалуй, стоит.\n\n\n\n7.1.2.4.2 Геометрическое среднее\nРедко встречается в научных работах, но заради общего представления пусть будет. Поскольку оно считается через умножение, то может быть рассчитано только на абсолютной шкале.\n\\[\nG_{X} = \\sqrt[n]{\\prod_{i=1}^n x_i} = \\Big(\\prod_{i=1}^n x_i\\Big)^{\\tfrac{1}{n}}\n\\]\n\n\n7.1.2.4.3 Квадратичное среднее\n\nА вот это уже более полезная история. Мы с ним столкнёмся далее, правда под разными масками.\n\nКвадратичное среднее (quadratic mean, root mean square, RMS) — это квадратный корень из среднего квадрата наблюдений. Ничего не понятно, поэтому по порядку.\n\nесть наблюдение \\(x_i\\)\nзначит есть и его квадрат \\(x_i^2\\)\nмы умеем считать обычно среднее арифметическое, но ведь \\(x_i^2\\) — это тоже наблюдение, просто в квадрате, так?\nзначит можем посчитать среднее арифметическое квадратов наблюдений — средний квадрат\n\n\\[\n\\frac{\\sum_{i=1}^n x_i^2}{n}\n\\]\n\nнорм, а теперь извлечём из этого дела корень — получим то, что там надо\n\n\\[\nS_X = \\sqrt{\\frac{\\sum_{i=1}^n x_i^2}{n}}\n\\]\nPer se4 мы его вряд ли ещё когда-то увидим, но пару раз оно внезапно всплывет.\n\n\n7.1.2.4.4 Гармоническое среднее\n\nСуперэкзотичный покемон.\n\n\\[\nH_X = \\frac{n \\prod_{i=1}^n x_i}{\\sum_{i=1}^n (\\tfrac{1}{x} \\prod_{j=1}^n x_j)} = \\frac{n}{\\sum_{i=1}^n \\tfrac{1}{x_i}}\n\\]\n\n\n7.1.2.4.5 Взвешенное среднее\nЧасто бывает такая ситуация, что нас нужно посчитать среднее по каким-либо имеющимся параметрам, но одни параметры для нас важнее, чем другие. Например, мы хотим вычислить суммарный балл обучающегося за курс на основе ряда работ, выполненных в течение курса, однако мы понимаем, что тест из десяти вопросов с множественном выбором явно менее показателен, чем, например, аналитическое эссе или экзаменационная оценка. Что делать? Взвесить параметры!\nЧто значит взвесить? Умножить на некоторое число. На самом деле, любое. Пусть мы посчитали, что написать эссе в три абстрактных раза тяжелее, чем написать тест, а сдать экзамен в два раза тяжелее, чем написать эссе. Тогда мы можем присвоить баллу за тест вес \\(1\\), баллу за аналитическое эссе вес \\(3\\), а экзамену — вес \\(6\\). Тогда итоговая оценка за курс будет рассчитываться следующим образом:\n\\[\n\\text{final score } = 1 \\cdot \\text{test} + 3 \\cdot \\text{essay} + 6 \\cdot \\text{exam}\n\\]\nСуперкласс. Однако! Весьма вероятно, что в учебном заведении принята единая система оценки для всех видов работ (ну, скажем, некая абстрактная десятибалльная система в сферическом вакууме). Получается, если и за тест, и за эссе, и за экзамен у студента по 10 баллов, то суммарный балл 100, что, кажется, больше, чем 10. Чтобы вернуться к изначальным границам баллов, нужно моделить суммарный балл на сумму весов параметров:\n\\[\n\\text{final score } = \\frac{1 \\cdot \\text{test} + 3 \\cdot \\text{essay} + 6 \\cdot \\text{exam}}{1 + 3 + 6}\n\\]\nКайф! Собственно, это и есть взвешенное среднее. Коэффициенты, на которые мы умножаем значение парамернов, называются весами параметров. И в общем виде формула принимает следующий вид.\n\\[\n\\bar X = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i} = \\sum_{i=1}^n w_i' x_i,\n\\]\nгде \\(x_i\\) — значения конкретных параметров, \\(w_i\\) — веса конкретных параметров, \\(w_i'\\) — нормированные веса параметров.\nВторая часть формулы показывается нам, что можно облегчить себе вычислительную жизнь, если заранее нормировать веса, то есть разделить каждый коэффициент на сумму коэффициентов:\n\\[\nw_i' = \\frac{w_i}{\\sum_{i=1}^n w_i}\n\\]\nТогда сумма коэффициентов будет равна единице. Так чаще всего и поступают, так как тогда коэффициент будет представлять долю, которую весит данный параметр в суммарной оценке. Удобно, практично, красиво.\nВзвещенное среднее часто применяется именно во всякого рода ассессментах, и не только образовательных. Например, вы HR-аналитик и оцениваете персонал. Вы аналитически вычисляете веса коэффициентов (допустим, с помощью линейной регрессии), а далее на их основе высчитаете интегральный балл, по которому будете оценивать сотрудников. Это как один из индустриальных примеров.\nТакже оно применяется, когда в наших данных есть какая-то группировка (например, когорты), при этом группы неравномерны.\n\n\n\n7.1.2.5 Среднее vs медиана\nПомимо того, что среднее и медиана информативны сами по себе, полезно смотреть на их взаимное расположение.\n\nСравнивать будем моду, медиану и среднее [арифметическое].\n\nИтак, все три статистики — мода, медиана и среднее — описывают центральную тенденцию — некоторое значение изучаемой нами переменной, вокруг которого собираются другие значения. Но если их три и все они используются, значит между ними должны быть какие-то различия. Посмотрим, какие.\nВо-первых, моду невозможно посчитать для непрерывной переменной.\nТак как вероятность того, что непрерывная случайная величина принимает своё конкретное значение, равна нулю, каждое наблюдение в нашей выборке будет уникально — встретится ровно один раз. Вспомните [посмотрите] пример из предыдущей главы, где мы набирали числа из отрезка. Получается, что мода теряет свой смысл.\nВо-вторых, медиану нельзя посчитать на номинальной шкале.\nНа номинальной шкале нет отношения порядка между элементами. Помните, на ней нельзя сравнивать на больше-меньше. Поэтому невозможно отсортировать наблюдения, а значит, и найти медиану.\nВ-третьих, среднее тоже нельзя посчитать на номинальной шкале.\nВообще, конечно, да — нельзя, потому что на номинальной шкале не определена операция сложения, входящая в вычисление среднего. Однако если на номинальной шкале есть только две категории, которые закодированы 0 и 1, то посчитать среднее можно. Но что оно будет значить?\nИсходный математический смысл среднего явно утерян. Посмотрим на это по-другому: посчитать сумму единиц это всё равно, что посчитать количество единиц. То есть, если мы сложим все нули и единицы, то получим количество единиц среди всех наших наблюдений. А разделив количество единиц на количество наблюдений, мы получим долю единиц — то есть долю наблюдений с лейблом 1.\nВ-четвертых, для дискретной переменной значение среднего арифметического будет не особо осмысленно. Ну, скажем, странно сказать, что в аудитории в среднем стоят 15.86 столов или в российских семьях в среднем 1.5 ребенка. Конечно, в ряде случаев можно это как-то более-менее содержательно интерпретировать, но это требует усилий, а мы ленивые, поэтому лучше использовать медиану.\nИтого, делаем следующие выводы:\n\nдля номинальной шкалы пригодна только мода\nдля дискретных переменных подходят мода и медиана\n\nмода иногда лучше, так как точно всегда будет целым числом\n\nдля непрерывных переменных подходят медиана и среднее\n\nТеперь нам надо разобраться, как будут себя вести меры центральной тенденции в зависимости от формы распределения.\nНа симметричном распределении мода, медиана и среднее совпадают [или, по крайней мере, находятся очень близко друг к другу]. Здесь и далее: красная линия — среднее, синяя — медиана, зелёная — мода.\n\n\n\n\n\n\n\n\n\nНа асимметричном распределении мода [практически] в пике. Практически, потому что функция плотности вероятности [черная линия на графике] на всегда точно аппроксимирует (в данном случае то же, что и сглаживает) эмпирическое распределение. На картинке ниже мы видим, что на гистограмме мода — самый высокий столбик, что и показывает нам зелёная линия, которой обозначена мода. Однако при сглаживании гистограммы пик немного съехал, и мода оказалась не совсем в вершине графика функции плотности вероятности.\nВообще-то это нормально, потому что мода для непрерывной величины, которую мы и визуализируем с помощью графика плотности, либо не может быть посчитана вовсе, либо — если так получилось, и у нас все же есть повторяющиеся значения — не слишком хорошая мера центральной тенденции. В целом, и на симметричном распределении мода тоже может находиться немного в стороне от пика.\nНа асимметричном распределении медиана и среднее смещены в сторону хвоста. Среднее смещено сильнее медианы. Это связано с тем, что медиана зависит только от количества наблюдений, а среднее ещё и от самих значений. На картинке ниже пример для распределения с правосторонней асимметрии (потому что хвост справа) — среднее (красная линия) правее медианы (синяя линия).\n\n\n\n\n\n\n\n\n\nА это пример для распределения с левосторонней асимметрией (так как хвост слева) — среднее (красная линия) левее медианы (синяя линия).\n\n\n\n\n\n\n\n\n\nДля того, чтобы лучше разобраться с тем, как большие и малые значения влияют на моду и медиану посмотрим такой пример. Пусть у нас есть оценки за выпускную квалификационную работу. Например, такие:\n\n\n[1] 6 7 7 8 8\n\n\nПосчитаем медиану и среднее:\n\n\n[1] 7\n\n\n[1] 7.2\n\n\nСреднее \\(7.2\\) округлиться до \\(7\\), то есть можно считать, что среднее и медиана совпали. Ну, ок.\nНо в комиссии сидят два требовательных доктора наук, которые поставили оценки, сильно отличающиеся от остальных:\n\n\n[1] 6 7 7 8 8 3 4\n\n\nПосчитаем медиану и среднее теперь:\n\n\n[1] 7\n\n\n[1] 6.142857\n\n\nМедиана осталась на месте — всё ещё \\(7\\). А вот среднее \\(6.1\\) округлится до \\(6\\). Казалось бы, это немного, но в смысле оценок — это прилично, и может сильно повлиять на GPA.\nИтого, среднее более чувствительно к нетипичным значениям (очень большим или очень малым).\nЕсть ещё один интересный вариант распределений — бимодальные. Значит ли, что у этого распределения две моды? Не всегда. Посмотрим пример ниже:\n\n\n\n\n\n\n\n\n\nМы видим, что на графике есть два пика, однако строго математически мода одна (зеленая линия) — и она в более высоком пике. Это логично, ибо там самые часто встречающиеся значения.\nСо средним и медианой происходит примерно то же, что и в случае асимметричного распределения. Второй пик смещает к себе обе меры центральной тенденции, причем среднее вновь сильнее, чем медиану.\n\n\n\n7.1.3 Меры разброса\nИтак, мы разобрались с мерами центральной тенденции. Однако для описания распределения их оказвается недостаточно. Почему?\n\n7.1.3.1 Зачем нужны меры разброса\nПосмотрим на несколько распределений:\n\n\n\n\n\n\n\n\n\nМетодом пристального взгляда можно установить, что у всех распределений одинаковые средние:\n\n\n\n\n\n\n\n\n\nОднако мы видим, что значения по-разному группируются вокруг среднего. Как они группируются — плотно, как на третьем рисунке, или не особо, как на втором — можно описать с помощью мер разброса, или мер вариативности.\n\n\n7.1.3.2 Минимум, максимум, размах\nНачнем с самого простого. Как наиболее просто описать вариативность? Мы работаем с выборкой, а в выборке, как известно, ограниченное число наблюдений. А если оно ограниченое, значит среди них точно есть наибольшее — максимальное — и наименьшее — минимальное.\nДопустим, мы открыли ведомость по «Анатомии и физиологии ЦНС» некоторой академической группы и пронаблюли следующее:\n\n\n [1]  7  4  6  9 10  5  6  9  6  6  3  6  8  8  5 10  7  5  7  3  9  4  8  3  8\n[26]  4  6  8  7  5\n\n\nМы можем посчитать минимальное и максимальне значение по этому ряду наблюдений:\n\n\n[1] 3\n\n\n[1] 10\n\n\nПолучается, что оценки варьируются от \\(3\\) до \\(10\\). Ну, приемлемо. Разница между максимальным и минимальным значением называется размах (range):\n\\[\n\\mathrm{range}(X) = \\max(X) - \\min(X)\n\\]\nИ вот мы преисполнившиеся идёт описывать вариативность переменной с помощью размаха, но обнаруживаем в другой ведомости этой же группы (по «Введению в психологию») вот что:\n\n\n [1]  6  8  4  6  7  5  7 10  4  6  7  8  7  6  8 10  8  7  7  6  8  7  6  8  6\n[26]  3  8  6  6  4\n\n\nРазмах вроде как такой же:\n\n\n[1]  3 10\n\n\nЗначит ли это, что вариативность одинаковая?\nНарисуем.\n\n\n\n\n\n\n\n\n\nКажется, что вариативность различна. Распределение оценок по «Анатомии и физиологии ЦНС» более равномерное, в то время как оценки по «Введению в психологию» активнее группируются где-то в середине.\nШтош, размах хоть и дает нам некоторую информацию о вариативности, нам этого маловато. Будем искать другие меры разброса.\n\n\n7.1.3.3 Дисперсия\nХотя описание разброса переменных с помощью квантилей (в частности, квартилей) может дать нам много полезной информации, все же у них есть существенный недостаток: они никак не взаимодействуют с самими значениями нашей переменной.\nДействительно, мы делим нашу сортированную выборку на равные части, и смотрим, что в эти части попало. Но хотелось бы как-то учесть ещё и сами значения переменной в некотрой числовой мере разброса.\nНу, хорошо. Поступим следующим образом. Мы все ещё хотим узнать, как наши значения группируются вокруг среднего. В предыдущей главе мы уже видели, что наши наблюдения отклоняются от среднего значения — значит мы можем посчитать отклонение для каждого наблюдения:\n\\[\nd_i = \\bar X - x_i\n\\]\nОкей. Если мы сложим все отклоненияи и поделим на их количество (которое равно количеству наблюдений), то мы получим среднее отклонение, да?\n\\[\n\\bar d = \\frac{1}{n} \\sum_{i=1}^n \\bar X - x_i\n\\]\nДа. Однако есть одна проблема. Ранее мы выяснили, что сумма отклонений от среднего равна нулю, а значит и среднее отклонение также будет равно нулю.\nХорошо. Но отрицательные значения ведь можно победить! Есть два пути:\n\nМодуль. Преимущество первого в том, что размерность величины разброса остается той же, что и у измеряемой переменной.\nКвадрат. Преимущество второго в том, что сильные отклонения будут оказывать более сильное влияние на окончательное значение статистики, в то время как для первого малые и большие отклонения равноценны.\n\nВторой путь на практике оказывается полезнее, так как мы хотим, чтобы сильно отличающиеся наблюдения вносили вклад в меру разброса.\nВозведя отклонения в квадрат, получим формулу дисперсии (вариации, variation):\n\\[\n\\mathbb{D}_X = \\text{var}(X) = \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (\\bar X - x_i)^2\n\\]\nГениально.\nНе совсем. Формула, которую мы получили, пригодна для расчета дисперсии генеральной совокупности — на выборке же она будет давать смещенную оценку. Это мы выводили математически и проверяли на симуляциях в предыдущем блоке.\nЧтобы получить точную (несмещенную) оценку дисперсии по выборке, нам нужно исправить знаменатель дроби — вместо \\(n\\) использовать \\(n-1\\):\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2\n\\]\nНо почему?\n\n7.1.3.3.1 Степени свободы\nВо всём виновата выборка.\nВзглянем на формулу дисперсии: в неё входит среднее арифметическое. То есть для того, чтобы рассчитать дисперсию на выборке, сначала нам необходимо на этой же выборке рассчитать среднее. Тем самым, мы как бы «фиксируем» нашу выборку этим средним значением — у значений нашего распределения становится меньше свободы для варьирования. Теперь свободно варьироваться могут \\(n-1\\) наблюдение, так как последнее всегда будет возможно высчитать, исходя из среднего значения. По этой причине нам необходимо корректировать исходную формулу расчета дисперсии.\n\n\n\n7.1.3.4 Стандартное отклонение\nИ вот мы получили невероятное! У нас есть формула расчета меры разброса, которая позволяет учесть сами значения переменной! Ну не чудо ли!\nЧудо, конечно, однако есть некоторая проблема. Мы возводили отклонения в квадрат. Представим, что мы хотим посчитатить дисперсию роста студентов психфака. Пусть мы измеряли рост в метрах. Отклонения тоже будут в метрах (потому что среднее — это тоже метры, а если из метров вычитать метры, то мы получим метры). А при возведении метров в квадрат получаются метры в квадрате. Очевидно, что если мы модели квадратные метры на некоторое число (\\(n\\)), они все еще останутся метрами в квадрате.\nО, нет! А счастье было так близко, так возможно! Получается, мы не можем интерпретировать эту меру разброса? Не сможем даже нарисовать?\nДа, но это не очень большая беда. Для того, чтобы вернуться обратно к единицам измерения нашей переменной, нам всего лишь нужно извлечь корень из дисперсии:\n\\[\n\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\bar X - x_i)^2}\n\\]\nМы получили величину, называемую стандартным отклонением (standard deviation). Чем она хороша? Тем, что её размерность совпадает с размерностью нашей переменной. Стандартное отклонение уже может быть достаточно интерпретабельно и хорошо визуализируемо.\nКстати, формула выше, которая что-то очень напоминает, — это стандартное отклонение генеральной совокупности, потому что под корнем стоит дисперсия генеральной совокупности.\nЧтобы посчитать стандартное отклонение по выборке, нам надо извлечь корень из выборочной дисперсии:\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2}\n\\]\n\n\n7.1.3.5 Свойства дисперсии и стандартного отклонения\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то дисперсия не изменится.\n\n\\[\n\\mathbb{D}_{X+c} = \\mathbb{D}_{X}\n\\]\nВот почему:\n\\[\n\\begin{split}\n\\mathbb{D}_{X+c} &= \\frac{\\sum_{i=1}^n \\big((\\bar X + c) - (x_i + c)\\big)^2}{n-1} = \\\\\n&= \\frac{\\sum_{i=1}^n \\big(\\bar X + c - x_i - c\\big)^2}{n-1} \\\\\n& = \\frac{\\sum_{i=1}^n \\big(\\bar X - x_i\\big)^2}{n-1} = \\mathbb{D}_X\n\\end{split}\n\\]\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то дисперсия увеличится в \\(c^2\\) раз.\n\n\\[\n\\mathbb{D}_{X \\cdot c} = c^2\\mathbb{D}_{X}\n\\]\nВот почему:\n\\[\n\\mathbb{D}_{X \\cdot c} = \\frac{\\sum_{i=1}^n (c\\bar X - cx_i)^2}{n-1} = \\frac{\\sum_{i=1}^n c^2(\\bar X - x_i)^2}{n-1} = \\frac{c^2 \\sum_{i=1}^n (\\bar X - x_i)^2}{n-1} = c^2\\mathbb{D}_X\n\\]\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то стандартное отклонение не изменится.\n\n\\[\ns_{X+c} = s_X\n\\]\nЭто следует из свойства дисперсии:\n\\[\ns_{X+c} = \\sqrt{s^2_{x+c}} = \\sqrt{s^2_x} = s_x\n\\]\nКак мы уже видели, распределение просто сдвигается на константу. Например, если к каждому значению синего распределения прибавить \\(2\\), получится красное — разброс у обоих распределений одинаковый:\n\n\n\n\n\n\n\n\n\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то стандартное отклонение увеличится во столько же раз.\n\n\\[\ns_{X \\cdot c} = c\\cdot s_X\n\\]\nЭто также следует из свойства дисперсии:\n\\[\ns_{X \\cdot c} = \\sqrt{s^2_{X \\cdot c}} = \\sqrt{s_X \\cdot c^2} = c \\cdot s_x\n\\]\nНапример, здесь каждое значение синего распределения умножили на \\(3\\) и получили красное — разброс также увеличился в три раза, поэтому распределение более плоское:\n\n\n\n\n\n\n\n\n\n\n\n\n7.1.4 Квантили\nВозьмем распределение суммарного балла по шкале «Доверие к техническим интеллектуальным системам». Выглядит оно как-то так:\n\n\n\n\n\n\n\n\n\nТеперь нам понадобится определение квантиля распределения.\nКвантиль — это значение переменной, которое не превышается с определенной вероятностью (обозначим её \\(p\\)). Иначе говоря, слева от значения квантиля лежит \\(p\\%\\) наблюдений.\nПосмотрим на картинки.\nСлева относительно квантиля-0.05 (\\(x_{0.05}\\)) лежит 5% наблюдений:\n\n\n\n\n\n\n\n\n\nСлева относительно квантиля-0.68 (\\(x_{0.68}\\)) лежит 68% наблюдений:\n\n\n\n\n\n\n\n\n\nСлева относительно квантиля-0.99 (\\(x_{0.99}\\)) лежит 99% наблюдений:\n\n\n\n\n\n\n\n\n\nИтак, мы поняли, а также приняли и осознали, что такое квантиль. Неясно только, как он нам поможет описать вариативность данных.\n\n7.1.4.1 Квартили\nДля этого нам пригодятся специально обученные квантили. Оказалось достаточно удобно поделить все наблюдение на четыре равные части — вот так:\n\n\n\n\n\n\n\n\n\nЗначения переменной, которые делят выборку на четыре равные части называются квартили. Получается, что\n\nслева от первого (нижнего) квартиля (\\(Q_1\\), \\(x_{0.25}\\)) лежит 25% наблюдений\nслева от второго (среднего) квартиля (\\(Q_2\\), \\(x_{0.50}\\)) лежит 50% наблюдений\n\nа значит и справа 50% — получается второй квартиль делит выборку пополам — это медиана\n\nслева от третьего (верхнего) квартиля (\\(Q_3\\), \\(x_{0.75}\\)) лежит 75% наблюдений\n\nЧетвертый квартиль не используется, потому что это максимальное значение — слева от него лежит 100% наблюдений.\nКстати, можно также отметить, что первый квартиль — это медиана нижней (меньшей) половины наблюдений, а третий — медиана верней (большей) половины наблюдений.\nВот такая вот прикольная история.\n\n\n7.1.4.2 Децили\nК слову, делить выборку можно не только на четверти — можно поделить, скажем, на 10 частей и получить децили. Так, слева от первого дециля (\\(x_{0.10}\\)) лежит 10% наблюдений, а слева от третьего (\\(x_{0.30}\\)) — 30%.\nДецили встречаются редко (в основном в психометрике), но знать о них полезно.\n\n\n7.1.4.3 Перцентили\nГораздо чаще встречаются перцентили — значения переменной, которые делят выборку на 100 равных частей. Например, так устроен ваш рейтинг. Только стоит помнить, что в рейтинге отсчет ведется от максимального среднего балла, поэтому если у вас нулевой перцентиль (\\(x_{0.00}\\)) по программе, значит выше вас в рейтинге никого нет. А если ваш перцентиль, скажем, 36-ой (\\(x_{0.36}\\)), то выше вас в рейтинге 36% ваших однокурсников, то есть вы все ещё в первой половине рейтинга, что очень неплохо!\n\n\n7.1.4.4 Интерквартильный размах\nИ — о, ура! — мы наконец-то добрались до того, ради чего тут собрались! Зная первый и третий квартили распределения, можно рассчитать интерквартильный (межквартильный) размах (interquartile range, IQR).\n\\[\n\\mathrm{IQR}(X) = Q_3(X) - Q_1(X)\n\\]\nИнтерквартильный размах — это разница между третьим и первым квартилем распределения. Эта величина описывает интервал значений признака, в котором лежит 50% наблюдений.\n\n\n\n\n\n\n\n\n\nВ данном случае он равен 40 — то есть 50% наблюдений лежит в пределах 40 единиц шкалы.\n\n\n7.1.4.5 Визуализация квартилей. Боксплот\nОтображать квартили на гистограмме, во-первых, совершенно неудобно, а во-вторых, не то чтобы график получается информативный. Для визуализации квартилей придумали специальный тип графика — ящик с усами, или боксплот (boxplot).\n\n\n\n\n\n\n\n\n\nПрикольная ерунда. Научимся его читать.\nЗначения переменной идут по вертикальной оси (оси ординат). По горизонтальной оси (оси абсцисс) здесь ничего не идет5. Жирная линия по середине ящика — медиана (второй квартиль). Нижняя граница ящика — первый квартиль, верхняя — третий. Получается, что границы ящика показывают нам значения, в пределах которых лежит половина наблюдений.\nНижний ус — первый квартиль минус полтора межквартильных размаха. Верхний ус — третий квартиль плюс полтора мехквартильных размаха.\n\n\n\n\n\n\n\n\n\n\n\n\nЗамечание\n\nЯщик может быть асимметричным — то есть верхняя его часть (расстояние между медианой и третьим квартилем) и нижняя его часть (расстояние между медианой и первым квартилем) могут быть разными. Это нам говорит об асимметричности распределения. Усы также могут быть неравными, если один из них упирается в максимум / минимум — тоже по причине асимметричности распределения.\n\n\nНу, допустим. А что тогда точки?\n\n7.1.4.5.1 Выбросы\nВообще справедливо было бы задаться вопросом, а зачем нам вообще усы на этом графике? И почему мы прибавляем полтора межквартильных размаха?\nЭто один из подходов к определению нехарактерных значений — выбросов. При исследовании данных мы часто задаемся вопросом, если ли в наших данных такие значения, которые сильно отличаются от распределения той или иной переменной. Но как определить это самое «сильно»?\nВот один из подходов. Будем считать, что значения, которые укладываются в интервал \\((Q_1 - 1.5 \\times \\mathrm{IQR}, \\, Q_3 + 1.5 \\times \\mathrm{IQR})\\), нас устраивают. Все что попадает в этот интервал — это «нормальные», типичные значения нашей переменной. Те же, которые будут находиться за пределами этого интервала, мы назовем нетипичными, аномальными значениями, или выбросами. Эти значения и будут отмечены точками на графике boxplot.\nЧто с ними делать? Во-первых, содержательной анализировать. Выбросы могут возникнуть по разным причинам. Может быть испытуемый отвлекся на прилетевшего в окно голубя, и у нас в данных появилось время реакции 200 секунд. Такие выбросы мы можем исключить из данных. А возможно в нашу выборку попали какие-то люди, которые, скажем, очень сильно или очень слабо доверяют искусственному интеллекту (как в примере на рисунке). Эти наблюдения необходимо дополнительно проанализировать — возможно, это представители специфических групп нашей генеральной совокупности (например, программисты-разработчики или люди пенсионного возраста). Анализ принесет нам дополнительную информацию, которую мы могли не учесть при планировании исследования. Крч, думать надо. И собирать побольше данных, чтобы можно было найти содержательную интерпретацию происходящему.\n\n\n\n\n7.1.5 Сравнение мер разброса\nКак и разные меры центральной тенденции, разные меры разброса по-своему хороши. Более того, они дружат с мерами центральной тенденции. Так, с медианой используется мехквартильных размах, а со средним арифметическим — стандартное отклонение.\nРазмах подходит для всего сразу. Его стоит рассчитать, чтобы составить самое первое представление в разбросе, о границах измерения изучаемого признака [на нашей выборке].\nСтоит также отметить, что все, что мы тут обсуждали, совершенно не годиться для номинативных переменных. Однако у них тоже есть вариативность. Согласитель, что выборка из Питера, Москвы, и Казани более вариативна, чем выборка из Москвы. Аналогом меры разброса для номинальной переменной можно назвать количество уникальных значений этой переменной.\n\n\n7.1.6 Асимметрия\nВыше мы видели, что распределения бывают асимметричными, и нам бы хотелось каким-то образом — желательно, числовым — эту асимметричность описывать. Для этого среди описательнрых статистик существует коэффициент асимметрии. Приведем его формулу, но запоминать не будем, потому что она в целом не особо нужна — всё-таки мы доверяем R считать всякие вычисления.\n\\[\n\\mathrm{skew}(X) = \\frac{\\frac{1}{n}\\sum_{i=1}^n (\\bar X - x_i)^3}{\\left(\\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2 \\right)^{3/2}}\n\\]\nОзнакомимся прежде всего в интерпретацией значений коэффицинета асимметрии:\n\nположительный коэффициент асимметрии (positive skew) указывает на наличие длинного правого хвоста распределения, соответственно всё распределение будет скошено влево (то есть преобладают низкие значения)\nотрицательный коэфффициент асимметрии (negative skew) указывает на наличие длинного левого хвоста распределения, соответственно всё распределения будет скошено вправо (то есть преобладают высокие значения)\nзначения коэффициента асимметрии, близкие к нулю, говорят о симметричности распределения\n\nПосмотрим на картинках:\n\nсимметричное распределение, коэффициент асимметрии равен нулю:\n\n\n\n\n\n\n\n\n\n\n\nлевосторонняя асимметрия, коэффициент асимметрии отрицательный:\n\n\n\n\n\n\n\n\n\n\n\nправосторонняя асимметрия, коэффициент асимметрии положительный:\n\n\n\n\n\n\n\n\n\n\nСодержательная интерпретация асимметрии очень сильно зависит от исследовательской области — когда-то мы вполне ожидаем асимметричность (например, для времени реакции), а когда-то это может свидетельствовать о проблемах с выборкой или, в случае психометрики, о проблемах формулировок вопросов.\n\n\n7.1.7 Эксцесс\nПомимо симметричности эмпирического распределения нас часто интересует, насколько наше распределение растянулось по горизонтальной оси. Это определяется коэффициентом эксцесса. Вновь приведем формулу, но не будем на ней останавливаться.\n\\[\n\\mathrm{kurt}(X) = \\frac{\\frac{1}{n}\\sum_{i=1}^n (\\bar X - x_i)^4}{\\left(\\frac{1}{n-1} \\sum_{i=1}^n (\\bar X - x_i)^2 \\right)^{2}} - 3\n\\]\nОпять же остановимся, прежде всего, на интерпретации значений коэффициента эксцесса:\n\nнулевой коэффициент эксцесса обозначает такой же эксцесс, как у стандартного нормального распределения (то есть, «нормальный»)\nположительный коэффициент эксцесса обозначает, что распределение имеет более острую вершину (то есть у нас очень много средних значений, но тонкие «хвосты» — мало низких и высоких значений)\nотрицательный коэффициент эксцесса обозначает, что распределение имеет более пологую вершину (то есть у нас меньше средних значений и толстые «хвосты» — много низких и высоких значений)\n\nИ также посмотрим на картинки:\n\n[стандартное] нормальное распределение, коэффициент эксцесса равен нулю\n\n\n\n\n\n\n\n\n\n\n\nвысокий пик распределения, коэффициент эксцесса положительный\n\n\n\n\n\n\n\n\n\n\n\nнизкий пик распределения, коэффициент эксцесса отрицательный",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#корреляционный-анализ",
    "href": "l7.html#корреляционный-анализ",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.2 Корреляционный анализ",
    "text": "7.2 Корреляционный анализ\nДо этого момента мы рассматривали только отдельные переменные и их характерики, однако в практике мы редко работаем только с одной переменной. Как правило, у нас есть многомерное пространство признаков, и нас интересуют взаимосвязи между ними.\n\n7.2.1 Ковариация\nМы хотим описать имеющиеся взаимосвязи как можно проще и опираясь на то, что у нас уже есть. Мы говорили, что дисперсия, или вариация, заключает в себе информацию об изменчивости признака. Если мы хотим исследовать взаимосвязь между признаками, то логично будет посмотреть, как изменяется один из признаков при изменении другого — иначе говоря, рассчитать совместную изменчивость признаков, или ко-вариацию (covariance).\nКак мы её будем считать? Подумаем графически. Расположим две переменные на осях и сопоставим каждому имеющемуся наблюдению точку на плоскости.\n\n\n\n\n\n\n\n\n\nОтметим средние значения по обеим переменным.\n\n\n\n\n\n\n\n\n\nЗаметим, что если наши наблюдения по переменной \\(X_1\\) отклоняются в большую сторону от среднего, то они отклоняются в большую сторону от среднего и по переменной \\(X_2\\). Аналогично, если они будут отклоняться от среднего в меньшую сторону по \\(X_1\\), то в меньшую же сторону от среднего они будут отклоняться и по \\(X_2\\). Обозначим сонаправленные отклонения синим, а разнонаправленные — красным.\n\n\n\n\n\n\n\n\n\nЕсли же у нас обратная ситуация — наблюдения по переменной \\(X_1\\) отклоняются в большую сторону от среднего и вместе с этим они отклоняются в меньшую сторону от среднего и по переменной \\(X_2\\), и наоборот — то мы получим следующую картину:\n\n\n\n\n\n\n\n\n\nЕсли же отклонения от среднего никак не связаны у двух переменны, то это будет выглядеть так:\n\n\n\n\n\n\n\n\n\nПолучается, мы можем на основании согласованности отклонений уже заключить о направлении связи. Произведение отклонений по обеим величинам будет положительно, если отклонения сонаправленны. Запишем это математически.\n\\[\n(\\bar x_1 - x_{i1}) (\\bar x_2 - x_{i2}) &gt; 0 \\Leftarrow \\big( (\\bar x_1 - x_{i1}) &gt; 0 \\wedge (\\bar x_2 - x_{i2}) &gt; 0 \\big) \\vee \\big( (\\bar x_1 - x_{i1}) &lt; 0 \\wedge (\\bar x_2 - x_{i2}) &lt; 0 \\big)\n\\]\nСоответственно, если отклонения будут направлены в разные стороны, из произведение будет отрицательным. Ну, осталось только понять, как совместные отклонения организованы в среднем — это и будет ковариацией двух величин:\n\\[\n\\mathrm{cov}(X_1, X_2) = \\frac{1}{n} \\sum_{i=1}^n (\\bar X_1 - x_{i1}) (\\bar X_2 - x_{i2})\n\\]\nВажно отметить, что ковариация улавливает только линейную составляющую взаимосвязи между признаками, поэтому если \\(\\mathrm{cov}(X_1,X_2) = 0\\), то мы можем сказать, что между переменными нет линейной взаимосвязи, однако это не значит, что между этими переменными нет никакой другой зависимости.\n\n\n\n\nНа рисунке приведены стандартизированные значения ковариации.\n\n\n\nУ ковариации есть два важных недостатка:\n\nэто размерная величина, поэтому её значение зависит от единиц измерения признаков,\nона зависит от дисперсий признаков, поэтому по её значению можно определить только направление связи (прямая или обратная), однако ничего нельзая сказать о силе связи.\n\nПоэтому нам нужно как-то модицифировать эту статистику, чтобы мы могли больше вытащить из её значения.\n\n\n7.2.2 Корреляция\nРаз ковариация зависит от дисперсии, то можно сделать некоторые математические преобразования, чтобы привести эмпирические распределения к какому-то одному виду — сделать так, чтобы они имели одинакое математическое ожидание (среднее) и одинаковую дисперсию. С этой задачей прекрасно справляется стандартизация. Напоминаю формулу:\n\\[\nx_i^* = \\frac{x_i - \\bar x}{s}\n\\]\nПосле такого преобразования математическое ожидание нашего распределения будет равно нуля, а стандартное отклонение — единице. Это избавит нас от влияния дисперсии на значение ковариации. Ковариация двух стандартно нормально распределенных величин называется корреляцией (correlation).\n\\[\n\\mathrm{cov}(X_1^*, X_2^*) = \\frac{1}{n-1} \\sum_{i=1}^n x_{i1}^* x_{i2}^* = \\mathrm{corr}(X_1, X_2),\n\\] где \\(X_1^*\\) и \\(X_2^*\\) — стандартизированные величины \\(X_1\\) и \\(X_2\\) соответственно.\nКорреляцию можно выразить через ковариацию:\n\\[\n\\mathrm{corr}(X_1, X_2) = \\frac{1}{n-1} \\sum_{i=1}^n \\Big( \\frac{\\bar x_1 - x_{i1}}{s_1} \\Big) \\Big( \\frac{\\bar x_2 - x_{i2}}{s_2} \\Big) =\n\\frac{1}{s_1 s_2} \\Big( \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x_1 - x_{i1})(\\bar x_2 - x_{i2}) \\Big) = \\frac{\\mathrm{cov}(X_1, X_2)}{s_1 s_2}\n\\]\nЕсли внимательно всмотреться в формуле, то можно обнаружить, что корреляция это не что иное, как стандартизированное значение ковариации.\nКоэффициент корреляции имеет четкие пределы изменения: \\([-1; \\,1]\\). Крайнее левое значение говорит о том, что присутствует полная обратная линейная взаимосвязь, крайнее правое — что присутствует полная прямая линейная взаимосвязь. Как и ковариация, корреляция ловит только линейную составляющую связи, поэтому нулевое значение корреляци показывает, что между переменными отсутствует линейная взаимосвязь. Это всё еще не значит, что связи нет вовсе.\n\n7.2.2.1 Интерпретация коэффициента корреляции\nПреимущество корреляции над ковариацией в том, что она отражает не только направление, но и силу связи:\n\n\n\nЗначение коэффициента\nИнтерпретация\n\n\n\n\n\\(-1.0\\) – \\(-0.9\\)\nочень сильная обратная связь\n\n\n\\(-0.9\\) – \\(-0.7\\)\nсильная обратная связь\n\n\n\\(-0.7\\) – \\(-0.5\\)\nсредняя обратная связь\n\n\n\\(-0.5\\) – \\(-0.3\\)\nслабая обратная связь\n\n\n\\(-0.3\\) – \\(0.0\\)\nочень слабая обратная связь\n\n\n\\(0.0\\) – \\(0.3\\)\nочень слабая прямая связь\n\n\n\\(0.3\\) – \\(0.5\\)\nслабая прямая связь\n\n\n\\(0.5\\) – \\(0.7\\)\nсредняя прямая связь\n\n\n\\(0.7\\) – \\(0.9\\)\nсильная прямая связь\n\n\n\\(0.9\\) – \\(1.0\\)\nочень сильная прямая связь\n\n\n\n\n\n7.2.2.2 Тестирование статистической значимости коэффициента корреляции\nОценку коэффициента корреляции мы получаем методом моментов, заменяя истинный момент \\(\\rho_{ij}\\) выборочным \\(r_{ij}\\):\n\\[\n\\hat \\rho_{ij} = \\overline{\\big( (X_{ki} - \\bar X_i) (X_{kj} - \\bar X_j) \\big)} = r_{ij}\n\\]\nЕсли в генеральной совокупности связь между признаками отсутствует, то есть \\(\\rho_{ij} = 0\\), будет ли равен нулю \\(r_{ij}\\)? Можно с уверенностью сказать, что не будет, так как выборочный коэффициент корреляции — случайная величина. А мы помним, что вероятность принятия случайной величиной своего конкретного значения равна нулю.\nТогда необходимо протестировать статистическую гипотезу:\n\\[\n\\begin{split}\nH_0&: \\rho_{ij} = 0 \\; \\text{(линейной связи нет)} \\\\\nH_1&: \\rho_{ij} \\neq 0 \\; \\text{(наиболее частый вариант альтернативы)}\n\\end{split}\n\\]\nДля проверки нулевой гипотезы используется следующая статистика:\n\\[\nt = \\frac{r_{ij}}{\\sqrt{\\frac{1 - r^2_{ij}}{n-2}}} \\overset{H_0}{\\thicksim} t(\\nu = n-2)\n\\]\nВывод о статистической значимости коэффициента корреляции делается согласно алгоритму тестировния статистических гипотез.\n\n\n7.2.2.3 Размер эффекта в корреляционном анализе\nЕще одна статистика, которая нам необходима — это размер эффекта. Напомним себе, что размер эффекта — это численное выражение силы взаимосвязи между переменными в генеральной совокупности. Здесь нам необходимо призадуматься, и осознать, что вообще-то корреляция сама по себе выражает силу взаимосвязи между переменными. И, да, это правда — размером эффекта для коэффициента корреляции является сам коэффициент корреляции. Удобненько.\nТак, к сожалению, будет не всегда, но вот с корреляцией это так. Ну, и хорошо.\nРекомендации по интерпретации [абсолютного значения] коэффициента корреляции с точки зрения размера эффекта для социальных наук такие:\n\n\n\nЗначение коэффициента\nРазмер эффекта\n\n\n\n\n\\(0.1\\)\nМалый (small)\n\n\n\\(0.3\\)\nСредний (medium)\n\n\n\\(0.7\\)\nБольшой (large)\n\n\n\n\n\n7.2.2.4 Доверительный интервал для коэффициента корреляции\nС построением интервальной оценки коэффциента корреляции возникают некоторые сложности. Наша задача состоит в том, чтобы определить в каких границах будет лежать значение истинного коэффициента корреляции с заданной вероятностью:\n\\[\n\\mathbb{P}(\\rho_{ij,\\min} &lt; \\rho_{ij} &lt; \\rho_{ij,\\max}) = \\gamma\n\\]\nНам необходимо найти статистику, закон распределения корой известен, однако ранее упомянутся статистика не подходит, так как она имеет распределение Стьюдента, когда верна нулевая гипотеза об отсутствии связи. Если же мы строим интервальную оценку, нас интересует случай наличия связи.\nТакую статистику искали долго, и её удалось найти, когда ввели определённое преобразование выборочного критерия корреляции — z-преобразования Фишера:\n\\[\nz(r_{ij}) = \\frac{1}{2} \\ln \\frac{1 + r_{ij}}{1 - r_{ij}} \\thicksim \\mathcal{N}(\\bar z_{ij}, \\tfrac{1}{n-3}),\n\\] где \\(n\\) — объём выборки, а \\(\\bar z_{ij}\\) получается расчётом по указанной формуле после подставления точечной оценки коэффициента корреляции.\nТогда интервальная оценка для величины \\(z_{ij, \\mathrm{true}}\\) приобретает такой вид:\n\\[\n\\mathbb{P}\\Big( \\bar z_{ij} - t_\\gamma \\sqrt{\\tfrac{1}{n-3}} &lt; z_{ij, \\mathrm{true}} &lt; \\bar z_{ij} + t_\\gamma \\sqrt{\\tfrac{1}{n-3}}  \\Big) = \\gamma\n\\]\nДалее путём обратного преобразования получаются значения границ интервала \\((\\rho_{ij,\\min}, \\; \\rho_{ij,\\max})\\).\n\n\n\n7.2.3 Коэффициенты корреляции для разных шкал\nДла разных шкал разработаны разные коэффициенты корреляции. Оценки коэффициентов будут рассчитываться по-разному, но логика тестирования статистических гипотез остаётся одинаковой.\n\n\n\n\n\n\n\n\nПеременная \\(X\\)\nПеременная \\(Y\\)\nМера связи\n\n\n\n\nИнтервальная или отношений\nИнтервальная или отношений\nКоэффициент Пирсона\n\n\nРанговая, интервальная или отношений\nРанговая, интервальная или отношений\nКоэффициент Спирмена\n\n\nРанговая\nРанговая\nКоэффициент Кенделла",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#частный-и-множественный-коэффициент-корреляции",
    "href": "l7.html#частный-и-множественный-коэффициент-корреляции",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.3 Частный и множественный коэффициент корреляции",
    "text": "7.3 Частный и множественный коэффициент корреляции\nЕсли у нас два признака, то с ними всё достаточно понятно. А если признаком много? Тогда у нас могут быть сложные взаимосвязи, и возможен такой случай, что некоторый признак оказывает связан как с одним, так и с другим из интересующих нас. Таким образом, мы можем наблюдать ложную корреляцию. Чтобы избавиться от влияния сторонних признаков, используюся частные коэффициенты корреляции.\nВ случае нескольких переменных удобно представить результаты вычисления коэффициентов корреляции в виде корреляционной матрицы, отображающей связи всех признаков со всеми:\n\\[\nR =\n\\begin{pmatrix}\n1 & r_{12} & \\dots & r_{1p} \\\\\nr_{12} & 1 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & 1\n\\end{pmatrix}\n\\]\nВ корреляционной матрице на главной диагонали стоят единицы, отражающай связь переменной в самой собой — разумеется, она будет абсолютно линейная.\nМатрица, как можно заметить, симметрична относительно главной диагонали, так как \\(r_{ij} = r_{ji}\\).\nИтак, возвращается к частному коэффициенту корреляции. Посмотрим пример для случая трех переменных:\n\\[\nR =\n\\begin{pmatrix}\n1 & r_{12} & r_{13} \\\\\nr_{21} & 1 & r_{23} \\\\\nr_{31} & r_{32} & 1\n\\end{pmatrix}\n\\]\n\\[\nr_{12,3} = \\frac{r_{12} - r_{13} \\cdot r_{23}}{\\sqrt{(1 - r^2_{23})(1-r^2_{13})}}\n\\]\n\\[\n\\begin{split}\nH_0&: \\rho_{12,3} = 0 \\\\\nH_1&: \\rho_{12,3} \\neq 0 \\\\\nt &= \\frac{r_{12,3} \\sqrt{n-3}}{\\sqrt{1 - r^2_{12,3}}} \\overset{H_0}{\\thicksim} t(\\nu = n-3)\n\\end{split}\n\\]\nХорошо, а если нас интересует связь одного признака с несколькими сразу? Тогда нам нужен множественный коэффициент корреляции. Он также вычисляется на основе корреляционной матрицы и определяется следующим образом. Пусть нас интересует связь первого признака со всеми остальными:\n\\[\nR_1 = \\sqrt{1 - \\frac{\\det R}{A_{11}}}\n\\]\nКвадрат множественного коэффициента корреляции называется коэффициентом детерминации6. Он показывает, во-первых, степень тесноты связи данного признака со всеми остальными, но, кроме того, ещё и долю дисперсии данного признака, определяемую вариацией все остальных признаков, включенных в данную корреляционную модель.\nМы подробнее его изучим в следуюшей теме, а также увидим, где нам его найти, чтобы не считать руками.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#другие-корреляции",
    "href": "l7.html#другие-корреляции",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.4 Другие корреляции",
    "text": "7.4 Другие корреляции\nМожно коррелировать не только количественные и ранговые шкалы между собой, но и качественные тоже:\n\n\n\nПеременная \\(X\\)\nПеременная \\(Y\\)\nМера связи\n\n\n\n\nДихотомическая\nДихотомическая\n\\(\\phi\\)-коэффициент\n\n\nДихотомическая\nРанговая\nРангово-бисериальный коэффициент\n\n\nДихотомическая\nИнтервальная или отношений\nБисериальный коэффициент\n\n\n\n\n7.4.1 \\(\\phi\\)-коэффициент\nЭтот коэффициент позволяет рассчитать корреляцию между двумы дихотомическими шкалами. Он основан на расчёте статистики \\(\\chi^2\\).\n\n7.4.1.1 Критерий независимости Пирсона\nПо двум дихотомическим переменным можно построить таблицы сопряженности. Сам хи-квадрат тестирует гипотезу о том, что между двумя категориальными переменными нет связи. Он это делает путём сравнения теоретической и эмпирической таблицы частот.\nЭмпирическую таблицу [частот] мы получаем по результатам наблюдений:\n\n\n\n\n\\(X_1\\)\n\\(X_2\\)\n\n\n\n\n\\(Y_1\\)\n\\(p_{X_1,Y_1} = a\\)\n\\(p_{X_2,Y_1} = b\\)\n\n\n\\(Y_2\\)\n\\(p_{X_1,Y_2} = c\\)\n\\(p_{X_2,Y_2} = d\\)\n\n\n\nДалее вычисляются теоретические частоты:\n\n\n\n\n\n\n\n\n\n\\(X_1^*\\)\n\\(X_2^*\\)\n\n\n\n\n\\(Y_1^*\\)\n\\(\\frac{(a+b) \\times (a+c)}{N}\\)\n\\(\\frac{(b+a) \\times (b+d)}{N}\\)\n\n\n\\(Y_2^*\\)\n\\(\\frac{(c+d) \\times (a+c)}{N}\\)\n\\(\\frac{(d+c) \\times (b + d)}{N}\\)\n\n\n\nгде \\(N = a + b + c + d\\).\nЗатем считаются расхождения частот, которые суммируются и получается статистика \\(\\chi^2\\):\n\\[\n\\chi^2 = \\sum_{i,j} \\frac{p_{X_i,Y_j} - p_{X_i^*,Y_j^*}}{p_{X_i^*,Y_j^*}}\n\\]\nСтатистика подчиняется распределению \\(\\chi^2\\), и чем больше значение этой статистики, тем сильнее связаны признаки. Статистические гипотезы для критерии независимости Пирсона формулируются так:\n\\[\n\\begin{split}\nH_0 &: p_{X_{i_1}, Y_{j_1}} = p_{X_{i_2}, Y_{j_2}}, \\, i_1 \\neq i_2, \\, j_1 \\neq j_2 \\\\\nH_1 &: \\exists i_1, i_2, j_1, j_2: p_{X_{i_1}, Y_{j_1}} \\neq p_{X_{i_2}, Y_{j_2}}\n\\end{split}\n\\]\n\nПо значению \\(\\chi^2\\) сложно что-то сказать о силе связи, поэтому его нормируют следующим образом, чтобы получить значения от 0 до 1, которые можно интерпретироват аналогично коэффициенту корреляции:\n\\[\n\\phi = \\sqrt{\\frac{\\chi^2}{N}}\n\\]\n\n\n\n7.4.2 Бисериальный коэффициент корреляции\nЭтот коэффициент используется для вычисления корреляции между количественной (\\(y\\)) и категориальной (\\(x\\)) шкалой и рассчитывается следующим образом:\n\\[\nr = \\frac{\\bar x_1 - \\bar x_2}{s_Y} \\sqrt{\\frac{n_1 n_2}{N(N-1)}},\n\\]\nгде \\(\\bar x_1\\) — среднее по элементам переменной \\(y\\) из группы \\(x_1\\), \\(\\bar x_2\\) — среднее по элементам \\(y\\) из группы \\(x_2\\), \\(s_y\\) — стандартное отклонение по переменной \\(y\\), \\(n_1\\) — число элементов в группе \\(x_1\\), \\(n_2\\) — число элементов в группе \\(x_2\\), \\(N\\) — общее число элементов.\nВажно отметить, что несмотря на то, что значение коэффициента может быть как положительным, так и отрицательным, это не влияет на интерпретацию. Это одно из исключений из общего правила.\n\n\n7.4.3 Рангово-бисериальный коэффициент корреляции\nЕсли у нас не количественная, а ранговая шкала, то применяется рангово-бисериальный коэффициент:\n\\[\nr = \\frac{2(\\bar x_1 - \\bar x_2)}{N},\n\\]\nгде \\(\\bar x_1\\) — средний ранг в группе \\(x_1\\), \\(\\bar x_2\\) — средний ранг в группе \\(x_2\\), \\(N\\) — общее количество наблюдений.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#andan-cor-fisher-transform",
    "href": "l7.html#andan-cor-fisher-transform",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "7.5 Преобразование Фишера",
    "text": "7.5 Преобразование Фишера\nУпомянутое ранее преобразование Фишера используется не только для вычисления доверительного интервала для коэффициента корреляции. Его применяют для нахождения так называемой pooled correlation — усредненной корреляции по нескольки отдельным коэффициентам. Для этого первоначально приводят коэффициенты корреляции к \\(z\\)-значениям по формуле:\n\\[\nz_i = \\frac{1}{2} \\ln \\frac{1 + r_i}{1 - r_i} = \\mathop{\\mathrm{artanh}}(r_i)\n\\]\nДалее усредняют каким-либо из способов — зависит от переменных, характеристик выборки и решаемой задачи — а затем возвращаются к исходной «размерности» корреляции через обратное преобразование:\n\\[\nr_P = \\dfrac{e^{2z_P} - 1}{e^{2z_P} + 1} = \\tanh(z_P)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l7.html#footnotes",
    "href": "l7.html#footnotes",
    "title": "7  L7 // Описательные статистики. Корреляционный анализ",
    "section": "",
    "text": "Mass (uncountable) noun↩︎\nCountable noun, plural in this case↩︎\nЗдесь в примере локальный максимум функции плотности вероятности на интервале \\((-4, \\, 4)\\) совпадает с глобальным максимумом — мы об этом знаем, потому что форма распределения нам известна. В случае эмпрического распределения корректнее говорить именно о локальном максимуме, так как глобальный максимум нам не доступен ввиду того, что мы работаем с выборкой.↩︎\nPer se (лат.) — «само по себе», «как таковое», «в чистом виде».↩︎\nНо если мы рисуем несколько боксплотов рядом, то на оси x будет категориальная переменная.↩︎\nВы точно видели это словосочетание, когда сталкивались с линейной регресией.↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>L7 // Описательные статистики. Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "l8.html",
    "href": "l8.html",
    "title": "8  L8 // Общие линейные модели. Простая и множественная линейная регрессия",
    "section": "",
    "text": "8.1 Простая линейная регрессия",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L8 // Общие линейные модели. Простая и множественная линейная регрессия</span>"
    ]
  },
  {
    "objectID": "l8.html#простая-линейная-регрессия",
    "href": "l8.html#простая-линейная-регрессия",
    "title": "8  L8 // Общие линейные модели. Простая и множественная линейная регрессия",
    "section": "",
    "text": "8.1.1 Ограничения корреляционного анализа\n\nКорреляционный анализ позволяет изучить линейную взаимосвязь между переменными, оценить её силу и направление, протестировать гипотезу о статистической значимости взаимосвязи.\nКорреляционный анализ не позволяет предсказывать значения одной переменной на основе значений другой. Изучения связей между несколькими переменными в корреляционном анализе также не очень удобно.\n\nНеобходимо построение некоторой модели.\n\n\n8.1.2 Идея регрессионной модели\nКогда мы строили диаграммы рассеяния, мы добавляли на них линию тренда, которая отражала линейную составляющую связи между визуализируемыми переменными.\n\n\n\n\n\n\n\n\n\nВизуально мы такую прямую проведём очень легко, а вот как мы нам получить её математическое выражение?\nПервое, что нам нужно вспомнить — это общее уравнение прямой. Оно выглядит так:\n\\[\ny = kx + b,\n\\]\nгде \\(k\\) — угловой коэффициент (slope), задающий угол наклона прямой к оси \\(x\\), а \\(b\\) — свободный член (intercept), который обозначает ординату точки пересечения прямой с осью \\(y\\).\n\n\n\n\n\nТаким образом, чтобы получить уравнение прямой, нам надо знать два этих числа.\n\n\n8.1.3 Формализация модели\nМы привыкли к тому, что неизвестными являются \\(x\\) и \\(y\\), но теперь, когда мы ищем уравнение прямой на основе имеющихся наблюдений, ситуация изменяется. Запишем уравнение прямой, используя общепринятые обозначения:\n\\[\ny = b_0 + b_1 x\n\\]\nУравнение отражает зависимость между переменными \\(x\\) и \\(y\\), значения которых нам известны, так как у нас есть результаты измерений, а вот неизвестными теперь являются \\(b_0\\) и \\(b_1\\).\nВ терминах статистической модели:\n\nпеременная \\(y\\) называется зависимая, предсказываемая, целевая переменная или регрессант\nпеременная \\(x\\) носит названия независимая переменная, предиктор или регрессор\nчисла \\(b_0\\) и \\(b_1\\) называются коэффициентами или параметрами модели\n\n\n\n\n\n\n\nЗависимые и независимые переменные\n\n\n\nНесмотря на использование терминов зависимая и независимая переменные, необходимо чётко понимать, что сам регрессионный анализ, как и корреляционный, ничего нам не говорит о причинности. Мы выражаем \\(y\\) через \\(x\\), но точно так же можем выразить и \\(x\\) через \\(y\\) — и модель будет подобрана, так как нет никаких математических ограничений. Поэтому если мы хотим сделать по результатам регрессионного анализа вывод о причинно-следственной связи между явлениями, нам необходимо либо серьёзное теоретическое обоснование нашего вывода — почему мы выбрали в качестве зависимой и независимой переменных именно эти? — либо использование экспериментельного дизайна исследования, где мы обосновываем причинно-следственный характер связи именно через дизайн эксперимента.\n\n\nОднако здесь необходимо еще несколько уточнений. Закономерность, которую мы будем моделировать, корректнее записать в следующем виде:\n\\[\ny = \\beta_0 + \\beta_1x,\n\\]\nгде \\(\\beta_0\\) и \\(\\beta_1\\) — параметры генеральной совокупности.\nКроме того, для каждого отдельного объекта генеральной совокупности значение целевой переменной \\(y_i\\) будет также зависеть и он случайных факторов, которые не учитываются параметрами \\(\\beta_0\\) и \\(\\beta_1\\), то есть для конкретного объекта генеральной совокупности модель примет вид:\n\\[\ny_i = \\beta_0 + \\beta_1 x + \\varepsilon_i,\n\\]\nгде \\(\\varepsilon_i\\) — случайная изменчивость целевой переменной.\nНа эту модель мы и будем опираться при оценке параметров \\(\\beta_0\\) и \\(beta_1\\).\n\n\n8.1.4 Идентификация модели\nМодель \\(y_i = \\beta_0 + \\beta_1 x + \\varepsilon_i\\) имеет место в генеральной совокупности, однако, как мы обсуждали много раз ранее, мы всегда работаем с выборкой, поэтому для выборки мы запишем модель в следующем виде:\n\\[\ny_i = b_0 + b_1 x_i + e_i,\n\\]\nгде \\(b_0 = \\hat \\beta_0\\) и \\(b_1 = \\hat \\beta_1\\) — оценка параметров генеральной совокупности, \\(e_i\\) — ошибки (или остатки, residuals) модели.\nИдентификация регрессионной модели сводится к нахождению коэффициентов \\(b_0\\) и \\(b_1\\). Мы хотим провести такую прямую, которая наилучшим образом будет описывать имеющуюся в данных закономерность, поэтому необходимо найти метрику, по которому мы будем определять «хорошесть» нашей прямой.\nГрафически мы делаем вот что: проводим прямую через облако точек. Очевидно, что красная прямая описывает закономерность совсем плохо, зелёная — чуть получше, а синяя — то, что нам нужно.\n\n\n\n\n\n\n\n\n\nИз картинки также видно, что даже синяя прамая не описывает наши данные максимально точно — не все точки попали на прямую. Ясно, что идеальную прямую мы провести и не сможем — точек же целое облако. Поэтому любая построенная нами модель будет содержать ошибку — те самые \\(e_i\\) — вновь по причине вариативности и неопределенности данных.\nУравнение подбираемой нами прямой — синяя на рисунке выше — запишем в следующем виде:\n\\[\n\\hat y_i = b_0 + b_1 x_i,\n\\]\nгде \\(\\hat y_i\\) — модельное значение целевой переменной, то есть то, что лежит на построенной нами прямой для конкретного значения регрессора \\(x_i\\).\nТогда мы сможем отобразить ошибки модели на графике:\n\n\n\n\n\n\n\n\n\nМы заинтересованы в том, чтобы наша модель ошибалась как можно меньше, то есть сумма ошибок \\(e_i\\) была минимальна. Получается, нам надо подобрать такие параметры \\(b_0\\) и \\(b_1\\), при которых сумма ошибок модели будет наименьшей. Математически это можно записать так:\n\\[\nQ_{\\text{res}} = \\sum_{i=1}^n e_i^2 \\to \\min_{b_0, b_1}\n\\]\nОбратите внимание, что минимизируется сумма квадратов ошибок, так как отдельные ошибки могут быть как положительными, так и отрицательными — что и отображено на графике выше —- в силу чего сумма ошибок будет равна нулю. Ситуация аналогичная расчету дисперсии, где мы возводили отклонения в квадрат.\nЕсли мы распишем, как определяется ошибка модели, то получится следующее:\n\\[\nQ_{\\text{res}} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat y_i)^2 = \\sum_{i=1}^n \\big( y_i - (b_0 + b_1 x_i) \\big)^2\n\\]\nВыходит, что \\(Q_{\\text{res}}\\) является функцией, зависящей от \\(b_0\\) и \\(b_1\\), что можно обозначить как \\(f(b_0, b_1)\\):\n\\[\nQ_{\\text{res}} = f(b_0, b_1) = \\sum (y_i - \\hat y_i)^2 = \\sum (y_i - b_0 - b_1x_i)^2 \\to \\min_{b_0, b_1}\n\\]\nВ итоге задача идентификации модели линейной регрессии сводится к нахождению минимума функции \\(Q_{\\text{res}} = f(b_0, b_1)\\). Этим занимается метод наименьшиъ квадратов.\n\n8.1.4.1 Метод наименьших квадратов\nМетод наименьших квадратов работает следующим образом. Как уже отмечено выше, условие минимизации ошибки модели представляет собой функцию двух аргументов:\n\\[\nf(b_0, b_1) = \\sum (y_i - b_0 - b_1x_i)^2\n\\]\nЭто квадратичная функция, и чтобы нам дальше удобнее было с ней работать, раскроем скобки:\n\\[\nf(b_0, b_1) = \\sum (y_i - b_0 - b_1x_i) (y_i - b_0 - b_1x_i)\n\\]\n\\[\nf(b_0, b_1) =\n\\sum (y_i^2 - b_0 y_i - b_1 x_i y_i - b_0 y_i - b_1 x_i y_i + b_0 b_1 x_i + b_1^2 x_i^2 + b_0^2 + b_0 b_1 x_i)\n\\]\n\\[\nf(b_0, b_1) =\n\\sum(y_i^2 - 2 b_1 x_i y_i - 2 y_i b_0 + x_i^2 b_1^2 + b_0^2 + 2 x_i b_1 b_0)\n\\]\nЧтобы определить, при каких значения \\(b_0\\) и \\(b_1\\) функция будет принимать минимальное значение, нужно взять две частные производные этой функции по \\(b_0\\) и \\(b_1\\) и приравнять их к нулю.\nБерём частные производные:\n\\[\n\\frac{f(b_0, b_1)}{\\partial b_0} = \\sum (-2y_i + 2b_0 + 2x_ib_1) =\n-2 \\sum \\big( y_i - (b_0 + b_1 x_i) \\big)\n\\]\n\\[\n\\frac{f(b_0, b_1)}{\\partial b_1} = \\sum (-2 x_i y_i + 2 x_i^2 b_1 + 2 x_i b_0) = -2 \\sum \\big( y_i - (b_0 + b_1 x_i) \\big) x_i\n\\]\nПриравниваем производные к нулю и решаем систему уравнений:\n\\[\n\\cases {\n-2 \\sum \\big( y_i - (b_0 + b_1 x_i) \\big) = 0 \\\\\n-2 \\sum \\big( y_i - (b_0 + b_1 x_i) \\big) x_i = 0\n}\n\\]\n\\[\n\\cases{\n\\sum \\big( y_i - (b_0 + b_1 x_i) \\big) = 0 \\\\\n\\sum \\big( y_i - (b_0 + b_1 x_i) \\big) x_i = 0\n}\n\\]\n\\[\n\\cases{\n\\sum y_i - \\sum b_0 + \\sum b_1 x_i = 0 \\\\\n\\sum y_i x_i - \\sum b_0 x_i + \\sum b_1 x^2_i = 0\n}\n\\]\n\\[\n\\cases{\n\\sum b_0 + \\sum b_1 x_i = \\sum y_i \\\\\n\\sum b_0 x_i + \\sum b_1 x_i^2 = \\sum y_i x_i\n}\n\\]\n\\[\n\\cases{\nb1 \\sum x_i + n b_0 = \\sum y_i \\\\\nb1 \\sum x^2_i + b_0 \\sum x_i = \\sum y_i x_i\n}\n\\]\n\\[\nb_0 = \\frac{\\sum y_i}{n} - b_1 \\frac{\\sum x_i}{n} = \\bar y - b_1 \\bar x\n\\]\n\\[\nb1 \\sum x_i^2 + (\\bar y - b_1 \\bar x) \\sum x_i = \\sum x_i y_i\n\\]\n\\[\n\\underline{b_1 \\sum x_i^2} + \\bar y \\sum x_i - \\underline{b_1 \\bar x \\sum x_i} = \\sum x_i y_i\n\\]\n\\[\nb_1 \\Big( \\sum x_i^2 - \\bar x \\sum x_i \\Big) =\n\\sum x_i y_i - \\bar y \\sum x_i\n\\]\n\\[\nb_1 = \\frac{\\sum x_i y_i - \\bar y \\sum x_i}{\\sum x_i^2 - \\bar x \\sum x_i} =\n\\frac{(\\sum x_i y_i - \\bar y \\sum x_i) \\times n}{(\\sum x_i^2 - \\bar x \\sum x_i) \\times n}\n\\]\n\\[\nb_1 = \\frac{\\overline{xy} - \\bar x \\cdot \\bar y}{\\overline{x^2} - \\bar x^2} =\n\\frac{\\overline{xy} - \\bar x \\bar y}{s_X^2}\n\\]\nВ сухом остатке из метода наименьших квадратов нам надо вынести две идеи:\n\nзадача идентификации модели линейной регрессии имеет аналитическое решение — то есть мы можем подобрать коэффициенты модели, опираясь только на имеющиеся данные\nэто аналитическое решение имеет следующий вид:\n\n\\[\n\\cases{\nb_0 = \\bar y - b_1 \\bar x \\\\\nb_1 = \\frac{\\overline{xy} - \\bar x \\cdot \\bar y}{\\overline{x^2} - \\bar x^2} =\n\\frac{\\overline{xy} - \\bar x \\bar y}{s_X^2}\n}\n\\]\n\n\n8.1.4.2 Матричное вычисление коэффициентов\nЧастные производные это, конечно, хорошо, однако можно вычислить коэффициенты и проще через матрицы. Имеющуюся у нас модель мы модем записать следующим образом: пусть у нас есть \\(n\\) наблюдений, каждое из которых описывается моделью \\(y_i = b_0 + b_1 x_i + e_i\\). Тогда мы можем записать следующую систему:\n\\[\n\\cases{\nb_0 + b_1 x_1 + e_1 = y_1 \\\\\nb_0 + b_1 x_2 + e_2 = y_2 \\\\\n\\dots \\\\\nb_0 + b_1 x_n + e_n = y_n \\\\\n}\n\\]\nЭту систему мы можем переписать в матричном виде:\n\\[\n\\mathbf{X} \\mathbf{b} + \\mathbf{e} = \\mathbf{y},\n\\]\nгде \\(\\mathbf{y}\\) — вектор нашей целевой переменной, \\(\\mathbf{X}\\) — матрица предикторов, \\(\\mathbf{b}\\) — вектор коэффициентов модели, \\(\\mathbf{e}\\) — вектор ошибок (остатков) модели.\nМожет возникнуть резонный вопрос: «почему \\(\\mathbf{X}\\) матрица, ведь у нас только одна независимая переменная?». Так как вектор коэффициентов модели \\(\\mathbf{b}\\) содержит два элемента \\(b_0\\) и \\(b_1\\), то для удобства вычислений к вектору значений предиктора добавляют вектор, состоящий из единиц, который будет отвечать за интерсепт нашей модели — в результате получается матрица \\(\\mathbf{X}\\), которая имеет следующий вид:\n\\[\n\\mathbf{X} = \\pmatrix{1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n}\n\\]\nОпуская детали, сразу укажем матричное решение для коэффициентов модели:\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y}\n\\]\nОтметим важную деталь из полученного решения: в ходе вычисления коэффициентов мы берём обратную матрицу от матрицы \\(\\mathbf{X}^\\top\\mathbf{X}\\). Этот факт нам пригодится в следующем разделе.\n\n\n\n8.1.5 Тестирование качества модели\nОкей, коэффициенты модели мы посчитали, а значит и модель теперь подобрана. Хотелось бы понять, насколько она получилась хорошей с точки зрения описания закономерностей данных.\n\n8.1.5.1 Коэффициент детерминации\nПервое, что хочется понять — насколько наша модель информативна. Поскольку у нас есть вариативность как одна из основных характеристик данных, подойдем к вопросу через неё. Иначе говоря, нам интересно, сколько дисперсии наших данных модель смогла объяснить. На практике работают не с дисперсией, а с суммой квадратов, что почти то же самое.\nВся изменчивость наших данных называется общая сумма квадратов (total sum of squares, TSS) и определяется так:\n\\[\n\\text{TSS} = \\sum_{i=1}^n (\\bar y - y_i)^2\n\\]\nЕсли мы попробуем изобразить это графически, то получим вот что:\n\n\n\n\n\n\n\n\n\nОдну часть этой изменчивости объясняет модель — это объясненная сумма квадратов (explained sum of squares, ESS):\n\\[\n\\text{ESS} = \\sum_{i=1}^n (\\bar y - \\hat y)^2\n\\]\n\n\n\n\n\n\n\n\n\nДругую часть этой изменчивости модель не улавливает, и она остаётся необъяснённой (остаточной) (residual sum of squares, RSS):\n\\[\n\\text{RSS} = \\sum_{i=1}^n (y_i - \\hat y_i)^2\n\\]\n\n\n\n\n\n\n\n\n\nЕсли объединить все три извенчивости на одной картинке, то получится следующее:\n\n\n\n\n\n\n\n\n\nЯвно видно, что\n\\[\n\\text{TSS} = \\text{ESS} + \\text{RSS}\n\\]\n\n\nВообще это бы хорошо доказать\n\nТак как в расчете сумм квадратов у нас используются, собственно, квадраты, напрямую из визуализации это равенство не следует, однако тем не менее, выполняется.\n\\[\n\\begin{split}\n\\text{TSS} & = \\sum (y_i - \\bar y)^2 = \\\\\n& =\\sum (y_i - \\hat y + \\hat y - \\bar y)^2 = \\\\\n& =\\sum \\big( (y_i - \\hat y_i) + (\\hat y_i - \\bar y) \\big)^2 = \\\\\n& = \\sum (y_i - \\hat y_i) = \\sum (\\hat y_i - \\bar y) + 2 \\sum (y_i - \\hat y_i)(\\hat y_i - \\bar y) = \\\\\n& = \\text{RSS} + \\text{ESS} + 2 \\sum (y_i - \\hat y_i)(\\hat y_i - \\bar y)\n\\end{split}\n\\]\nОкей, осталось доказать, что \\(2 \\sum (y_i - \\hat y_i)(\\hat y_i - \\bar y) = 0\\), и все будет найс.\nТак как \\(b_0 = \\bar y - b_1 x\\),\n\\[\n\\begin{split}\n\\sum (y_i - \\hat y_i)(\\hat y_i - \\bar y) & = \\sum (y_i - b_0 - b_1 x_i) (b_0 + b_1 x_i - \\bar y) = \\\\\n& = \\sum (y_i - \\bar y + b_1 \\bar x - b_1x_i) (\\bar y - b_1 \\bar x + b_1 x_i - \\bar y) = \\\\\n& = \\sum \\big( (y_i - \\bar y) - b_1(x_i - \\bar x) \\big) \\times b_1 (x_i - \\bar x) = \\\\\n& = \\sum \\big( b_1 (x_i - \\bar x) (y_i - \\bar y) - b_1^2 (x_i - \\bar x)^2 \\big) = \\\\\n& = b_1 \\sum (x_i - \\bar x) (y_i - \\bar y) - b_1^2 \\sum (x_i - \\bar x)\n\\end{split}\n\\]\nТак как \\(b_1 = \\frac{\\sum(x_i - \\bar x)(y_i - \\bar y)}{\\sum (x_i - \\bar x)^2}\\), получается, что:\n\\[\n\\begin{split}\n\\frac{\\Big( \\sum (x_i - \\bar x) (y_i - \\bar y) \\Big)^2}{\\sum (x_i - \\bar x)^2} - \\frac{\\Big( \\sum (x_i - \\bar x) (y_i - \\bar y) \\Big)^2 \\times \\sum (x_i - \\bar x)^2}{\\Big( \\sum (x_i - \\bar x)^2\\Big)^2} = \\\\\n= \\frac{\\Big( \\sum (x_i - \\bar x) (y_i - \\bar y) \\Big)^2}{\\sum (x_i - \\bar x)^2} - \\frac{\\Big( \\sum (x_i - \\bar x) (y_i - \\bar y) \\Big)^2}{\\sum (x_i - \\bar x)^2} = 0\n\\end{split}\n\\]\n\nВ качестве метрики информативности модели используется доля объясненной дисперсии — эта метрика называется коэффициент детерминации \\(R^2\\) и вычисляется по формуле:\n\\[\nR^2 = \\frac{\\text{ESS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}\n\\]\nИз формулы следует, что \\(0 \\leq R^2 \\leq 1\\). Считается, что если модель объясняется 0.8 и более дисперсии данных, то она хороша, хотя этот порог очень сильно зависит от конкретной задачи и исследовательской области.\nКроме того, отметим, что коэффициент детерминации равен квадрату коэффициента корреляции между целевой переменной и предиктором:\n\\[\nR^2 = r^2\n\\]\n\n\n8.1.5.2 F-статистика\nНа основе всё тех же сумм квадратов мы можем сделать вывод о том, насколько наша модель статистически значима. Здесь мы говорим о значимости модели в целом — не о значимости отдельных предикторов, это будет позже. Для этого нам надо заняться тестированием статистической гипотезы. Она формулируется так:\n\\[\n\\begin{split}\nH_0&: \\beta_0 = \\beta_1 = 0 \\\\\nH_1&: \\beta_0 \\neq 0 \\vee \\beta_1 \\neq 0\n\\end{split}\n\\]\nДля тестирования данной гипотезы используется следующая статистика:\n\\[\nF_{\\text{df}_e, \\text{df}_r} = \\frac{\\text{MS}_e}{\\text{MS}_r} = \\frac{\\text{ESS}/\\text{df}_e}{\\text{RSS}/\\text{df}_r} \\overset{H_0}{\\thicksim} F(\\text{df}_e, \\text{df}_r)\n\\]\nЗдесь \\(\\text{MS}_e\\) — это «средний объясненный квадрат», \\(\\text{MS}_r\\) — «средний остаточный квадрат», а \\(\\text{df}_e = p - 1\\) и \\(\\text{df}_r = n - p - 1\\) — степени свободы для объясненной и остаточной изменчивости, \\(p\\) — количество предикторов в модели, \\(n\\) — число наблюдений. По сути, эта статистика показывает, во сколько раз объясненная дисперсия больше остаточной.\nЭта статистика подчиняется F-распределению (распределению Фишера), которое выглядит так (изображен случай \\(\\text{df}_e = 3\\), \\(\\text{df}_e = 50\\)):\n\n\n\n\n\n\n\n\n\nКак и всегда, для наблюдаемой \\(F\\)-статистики рассчитывается p-value, на основе значения которого мы делаем вывод о статистической значимости модели в целом.\n\n\n8.1.5.3 Метрики качества модели\nКроме статистического критерия, мы можем использовать и другие показатели качества модели, называемыми метриками качества. В принципе, и коэффициент детерминации является метрикой качества, однако нам часто интересно, насколько ошибается наша модель в своих предсказаниях, поэтому появляются разные способы рассчитать эту ошибку.\n\n8.1.5.3.1 MSE\nПервый вариант — вычислить средний квадрат ошибки модели (mean squared error):\n\\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\Big ( y_i - \\hat y_i \\Big)^2\n\\]\n\n\n8.1.5.3.2 RMSE\nДля сравнения двух моделей это полезная метрика, однако для интерпретации она не очень удобна, так как выражена в квадрате единиц целевой переменной. Однако если извлечь квадратный корень из неё, то она превращается в среднеквадратичную ошибку (root mean squared error) и становится гораздо более интерпретабельной:\n\\[\n\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\Big ( y_i - \\hat y_i \\Big)^2}\n\\]\n\n\n8.1.5.3.3 MAE\nСуществует и другой способ измерения ошибки модели — если заменить квадрат на модуль, то получим среднюю абсолютную ошибку (mean absolute error):\n\\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat y_i|\n\\]\n\n\n8.1.5.3.4 MAPE\nБолее того, мы можем оценить точность предсказаний и в относительных значениях — перед суммированием разделим абсолютные отклонения на модуль значения целевой переменной:\n\\[\n\\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^n \\Bigg |\\frac{y_i - \\hat y_i}{y_i} \\Bigg|\n\\]\n\n\n\n\n8.1.6 Тестирование значимости предикторов\nЕсли модель значима в целом, значит среди её коэффициентов есть те, которые статистически отличны от нуля. Иначе говоря, есть такие предикторы, которые значимо влияют на нашу целевую переменную. В случае простой линейной регрессии предиктора всего два — intercept и slope. Intercept не всегда интерпретабелен, поэтому основное внимание уделают угловому коэффициенту. Формулы для интерсепта будут аналогичны.\nСтатистические гипотезы для тестирования значимости углового коэффициента будут стандартны:\n\\[\n\\begin{split}\nH_0&: \\beta_1 = 0 \\\\\nH_1&: \\beta_1 \\neq 0\n\\end{split}\n\\]\nТестирование гипотез осуществляется с помощью одновыборочного t-теста:\n\\[\nt = \\frac{b_1 - \\beta_1}{\\text{se}_{b_1}} = \\frac{b_1}{\\text{se}_{b_1}} \\overset{H_0}{\\thicksim} t(\\text{df}_t),\n\\]\nгде \\(\\text{df}_t = n-p-1\\), \\(\\text{se}_{b_1} = \\frac{s_r}{\\sum_{i=1}^n (x_i - \\bar x)^2}\\), \\(s_r = \\sqrt{\\frac{\\sum_{i=1}^n (y_i - \\hat y_i)^2}{n-2}}\\).\nСтатистический вывод осуществляется по стандартному алгоритму.\n\n\n8.1.7 Диагностика модели\nМодель линейной регрессии обладает рядом допущений, которые необходимо проверить. При построении модели мы, на самом деле, исходили из нескольких предположений:\n\nВо-первых, мы считали, что связь между предикторами и зависимой переменной линейная.\nВо-вторых, мы предположили, что наша модель полностью улавливает тренд закономерности, а значит остатки (ошибки) модели случайны.\n\nих среднее при любых значениях предиктора равно нулю: \\(\\bar \\varepsilon_i = 0\\),\nи они не зависят друг от друга: \\(\\text{cor}\\underset{i \\neq j}{(\\varepsilon_i, \\varepsilon_j)} = 0\\)\n\nКроме того, раз остатки заключают в себе случайный компонент модели, то они должны быть распределены нормально: \\(\\varepsilon \\thicksim \\mathcal N(0, \\sigma^2_\\varepsilon)\\)\n\nпричём их дисперсия должна быть одинакова при любых значениях предиктора: \\(\\sigma^2_{\\varepsilon_i} = \\sigma^2_\\varepsilon = \\text{const}\\)\n\n\n\n8.1.7.1 Линейность связи\nК сожалению, специального способа проверить это допущение не существует. В случае простой линейной регрессии нас спасает то, что у нас только один предиктор, а значит мы можем изобразить диаграмму рассеяния и визуально проверить, выполняется ли допущение линейности связи.\n\n\n8.1.7.2 Независимость остатков друг от друга\nИ вновь мы в ситуации, когда у нас нет возможности с помощью статистики проверить данное допущение. Независимость остатков следует из независимости наблюдений, что можно проконтролировать только с помощью дизайна исследования, в том числе дизайна сэмплинга (сбора) выборки.\n\n\n8.1.7.3 Нормальное распределение остатков\nА вот это уже проверяемое эмпирически предположение. Здесь существует несколько подходов: можно воспользоваться статистическим тестом (например, тестом Шапиро-Уилка), однако можно обойтись и графической диагностикой. Во-первых, можно построить гистограмму, а во-вторых, есть особы график QQ-plot, который также позволяет проверить допущение о нормальном распределении.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.1.7.4 Независимость остатков от предсказанных значений\nВ случае, если наша модель хорошо схватывает закономерность, представленную в данных, остатки не должны зависеть от предсказанных значений. Это значит два свойства:\n\nих среднее при любых значениях предиктора равно нулю: \\(\\bar \\varepsilon_i = 0\\)\nих дисперсия должна быть одинакова при любых значениях предиктора: \\(\\sigma^2_{\\varepsilon_i} = \\sigma^2_\\varepsilon = \\text{const}\\)\n\nДопущение о равенстве дисперсии остатков при любых значениях предиктора называется по-умному гомоскедастичностью. Её противоположность — это гетероскедастичность. Для проверки этого допущения также существуют статистические тесты, однако оба допущения можно проверить графически на одном и том же графике. Для этого нам необходимо построить диаграмму рассеяния, где по оси x будут идти предсказанные моделью значения, и по оси y остатки модели.\nЕсли мы наблюдаем такую картину, то мы можем заключить, что между остатками модели и предсказанными значениями связи нет:\n\n\n\n\n\n\n\n\n\nНа этом графике мы видим, что связь есть — среднее остатков зависит от предсказанного значения. Однако дисперсия остатков при всех значениях целевой переменной, а значит и при всех значениях предиктора, одинакова — гетероскедастичность отсутствует.\n\n\n\n\n\n\n\n\n\nНа этом графике мы видим, во-первых, что среднее распределения остатков зависит от предсказанных значений, а во-вторых, распределение остатков гетероскедастично — дисперсия увеличивается с ростом значений целевой переменной.\n\n\n\n\n\n\n\n\n\n\n8.1.7.4.1 Влиятельные наблюдения\nПоследним аспектом диагностики модели является поиск влиятельных наблюдений (influential points). Это такие точки, которые существенно влияют на положении регрессионной прямой — она сильно меняет свой наклон, если их исключить из выборки. Посмотрим на рисунок:\n\n\n\n\n\n\n\n\n\nЗдесь красная точка является влиятельным наблюдением, так как сильно меняет кгол наклона регрессионной прямой: черная линия построена только по черным точкам, красная — по всем, включая красную.\nВлиятельные наблюдения могут быть статистическими выбросами, но не обязательно. Это могут быть наблюдения, не подчиняющиеся общей закономерности между переменными, то есть наблюдения какой-либо особенной группы, в которых связь между изучаемыми переменными отличается от связи во всей остальной выборке.\nДля оценки влиятельных наблюдений вводятся метрики Leverage и Cook’s Distance (расстояние Кука). Мы не будем погружаться в то, как они вычисляются, а лишь скажем, что оценить влиятельность наблюдений можно с помощью графика Residual vs Leverage. Если расстояние Кука для данного наблюдения больше 0.5 (другой порог — больше 1), то это кандидат во влиятельные наблюдения. Необходимо изучить его и выяснить, чем оно может отличаться ото всех других.\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.1.8 Предсказания на основе модели\nПоскольку у нас теперь есть математическая модель, мы можем на её основе предсказывать значения целевой переменной для новых значений предиктора. Однако здесь стоит оговорить следующую деталь. Задача предсказания делится на два вида: интерполяция и экстраполяция.\n\nИнтерполяция — предсказания значений целевой переменной внутри заданного диапазона значений предиктора.\nЭкстраполяция — предсказания значений целевой переменной вне заданного диапазона значений предиктора.\n\nС задачей интерполяции линейные модели справляются хорошо. С задачей экстраполяции — хуже, потому что неизвестно, как себя поведет целевая переменная при тех значениях предиктора, которые нам не попадались. На картинке представлен пример этой мысли. Черные точки — имеющиеся наблюдения. Серые — те, которые мы пытаемся предсказать.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L8 // Общие линейные модели. Простая и множественная линейная регрессия</span>"
    ]
  },
  {
    "objectID": "l8.html#множественная-линейная-регрессия",
    "href": "l8.html#множественная-линейная-регрессия",
    "title": "8  L8 // Общие линейные модели. Простая и множественная линейная регрессия",
    "section": "8.2 Множественная линейная регрессия",
    "text": "8.2 Множественная линейная регрессия\nПростая линейная регрессия хороша, однако позволяет изучать взаимосвязи только между двумя переменными. Собственно, это позволяла делать и корреляция, разве что предсказывать мы не могли. Но пока что профита с введения новой модели немного. Нам же интересно изучать более сложные связи между несколькими переменными — и в этом нам поможет множественная линейная регрессия.\nНадо сказать, что вся логика работы с простой линейной регрессией сохраняется и для множественной линейной регрессии — мы так же будем идентифицировать модель методом наименьших квадратов, мы так же будем рассчитывать коэффициент детерминации, тестирования статистическую значимость модели в целом и значимость отдельных предикторов, сравнивать модели друг с другом на основе метрик качества и проводить диагностику модели. Всё — практически — остается таким же. Поэтому далее мы сосредоточимся, прежде всего, на разнообразии моделей множественной линейной регрессии, потому что возможно там много чего.\n\n8.2.1 Множественная линейная регрессия с количественными предикторами без взаимодействия\nИтак, мы хотим изучить связь нескольких предикторов с целевой переменной. Модель в общем-то меняется не сильно — просто добавляется ещё несколько слагаемых:\n\\[\ny_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + \\ldots + b_p x_{ip} + e_i\n\\]\nТеперь нам необходимо подобрать не два коэффициента, а \\(p+1\\) (\\(p\\) — количество предикторов), но, на самом деле, это ничего не меняет.\nКонечно, если мы будем пытаться решить задачу аналитически, то там будут определённые изменения. Однако мы познакомились с матричными вычислениями и можем обратиться к ним. В матричном виде модель будет записываться следующим образом:\n\\[\n\\mathbf{X} \\mathbf{b} + \\mathbf{e} = \\mathbf{y}\n\\]\nМожно пронаблюдать, что матричная запись идентична случаю простой линейной регрессии. Разница будет в организации матрицы \\(\\mathbf{X}\\) и вектора \\(\\mathbf{b}\\):\n\\[\n\\mathbf{X} = \\pmatrix{\n1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\dots & x_{np}}\n\\]\n\\[\n\\mathbf{b} = \\pmatrix{ b_0 & b_1 & b_2 & \\dots & b_p}\n\\]\nВычисление же коэффициентов будет осуществляться абсолютно аналогично простой линейной регрессии:\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y}\n\\]\nЯсно, что не важно, сколько предикторов будет содержать модель — вычисление коэффициентов будет работать одинаково. Однако по сравнению с простой линейной моделью здесь есть одна важная особеность.\n\n\n8.2.2 Проблема мультиколлинеарности\nПредикторы могут быть связаны не только с целевой перемненой, но и друг с другом, что обуславливает проблему мультиколлинеарности.\nВ чем она заключается? Посмотрим ещё раз на формулу, которая нам позволяет вычислить параметры модели:\n\\[\n\\mathbf{b} = (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y}\n\\]\nВ ходе вычислений мы берем обратную матрицу. Если наши предикторы сильно коррелируют друг с другом (≥ 0.8), то в нашей матрице возникают линейно зависимые столбцы, а значит обратная матрица не будет существовать.\nЧем это чревато? В случае абсолютно линейной связи коэффициент модели просто не вычислится — в аутпуте будет NA. Как правило, это намёк на то, чтобы проверить данные — возможно, у вас есть одна и та же переменная, записанная по-разному (например, рост в метрах и сантиметрах). В случае высоких (но меньших единицы) корреляций проблема подбора коэффициентов решается с помощью методов численной оптимизации, однако это может приводить к смещённым оценкам коэффициентов модели, а также большим ошибкам в оценках коэффициентов.\nПоэтому с мультиколлинеарностью надо бороться. Вариантов существует много. Наиболее часто используемые: исключение коллинеарных переменных из модели, метод служебной регресии, методы уменьшения размерности (кластерный анализ, PCA и др.).\n\n8.2.2.1 Коэффициент вздутия дисперсии\nДля исследования мультиколлинеарности есть два способа. Прежде всего, еще на этапе разведочного анализа, мы можем посмотреть на корреляционную матрицу предикторов — если там есть высокие корреляции, то вполне разумно ожидать проблему мультиколлинеарности. Когда модель уже построена, проверкой на мультиколлинеарность будет расчет коэффициента вздутия дисперсии (variance inflation factor, VIF).\nЧтобы его вычислить, проводятся следующие операции:\n\nпусть у нас есть модель\n\n\\[\ny = b_0 + b_1 x_1 + b_2 x_2 + \\ldots + b_p x_p + e\n\\]\n\nпостроим линейную регрессию, в которой один из предикторов будет регрессироваться по всем другим. Например, для первого предиктора:\n\n\\[\nx_1 = \\alpha_0 + \\alpha_2 x_2 + \\ldots + \\alpha_m x_m + e\n\\]\n\nвычислим коэффициент детерминации данной модели \\(R^2_j\\)\nдля коэффициента \\(b_j\\) VIF будет определяться так:\n\n\\[\n\\text{VIF}_j = \\frac{1}{1 - R^2_j}\n\\]\nПороговым значением для вынесение вердикта о наличии мультиколлинеарности считается 3 (иногда 2). Мы этот вердикт всё же вынесем, и будем с мультиколлинеарностью бороться.\n\n\n\n8.2.3 Множественная линейная регрессия с количественными и категориальными предикторами без взаимодействия\nКогда мы имеем дело с количественными предикторами, то всё более-менее ясно — есть зависимость между двумя количественными переменными, описываемая прямой. А что делать, если среди предикторов появляются категориальные переменные?\nРассмотрим картинку:\n\n\n\n\n\n\n\n\n\nУ нас есть целевая переменная y и количественный предиктор x, как и раньше. Однако мы видим, что теперь есть и категориальная переменная Group, которая разбивает наши наблюдения на две группы. Кроме того, чисто визуально мы наблюдаем, что обе группы подчиняются одной и той же закономерности, только зеленые точки лежат несколько выше красных. С точки зрения модели мы получаем ситуацию, когда в группах один и тот же наклон регрессионной прямой, но разные интерсепты. В модели это будет фиксироваться так:\n\\[\ny_i = b_0 + b_1 I_{i1} + b_2 x_{i2} + e_i\n\\]\nВидим, что в модели появляется некий предиктор \\(I\\). В принципе, вместо \\(I_{i1}\\) можно написать и \\(x_{i1}\\), однако мы выделим этот предиктор, так как он категориальный с помощью такой записи.\nПеременная \\(I\\) — это индикаторная переменная, которая обладает следующим свойством:\n\n\\(I_{i1} = 0\\), если наблюдение под номером \\(i\\) принадлежит к группе Group 1,\n\\(I_{i1} = 1\\), если наблюдение под номером \\(i\\) принадлежит к группе Group 2.\n\nТаким образом, у нас получается как бы две модели в одной:\n\nдля наблюдений из Group 1 (\\(I_{i1} = 0\\)) модель принимает вид: \\(\\hat y_i = b_0 + b_2 x_{i2}\\)\nдля наблюдений из Group 2 (\\(I_{i1} = 1\\)) — вид: \\(\\hat y_i = (b_0 + b_1) + b_2 x_{i2}\\)\n\nКоэффициент \\(b_0\\) называется базовым и показывает интесепт регрессионной прямой для одной из групп наблюдений. Коэффициент \\(b_1\\) называется поправочным и показывает разницу в интерсетах регрессионных прямых для двух групп наблюдений.\nВизуально представить эту ситуацию можно так:\n\n\n\n\n\n\n\n\n\nОценка и диагностика модели производится аналогично тому, как мы это делали на предыдущих моделях.\n\n\n8.2.4 Множественная линейная регрессия с количественными и категориальными предикторами со взаимодействием\nНо ведь могут быть различия не только в интерспетах, но и в угловых коэффициентах регрессионных прямых в разных группах наблюдений. Например, такая ситуация:\n\n\n\n\n\n\n\n\n\nДля того, чтобы учесть разную степень их связи предиктора с целевой переменной в разных группах наблюдений, нужно ввести взаимодействие предикторов в модель.\nВ данном случае математическая модель будет выглядеть так:\n\\[\ny_i = b_0 + b_1 I_{i1} + b_2 x_{i2} + b_3 I_{i1} x_2\n\\]\nПеременная \\(I\\) вновь выступает в качестве индикатора:\n\n\\(I_{i1} = 0\\), если наблюдение под номером \\(i\\) принадлежит к группе Group 1,\n\\(I_{i1} = 1\\), если наблюдение под номером \\(i\\) принадлежит к группе Group 2.\n\nТаким образом, у нас опять получается как бы две модели в одной, однако вторая модель теперь выглядит несколько иначе:\n\nдля наблюдений из Group 1 (\\(I_{i1} = 0\\)) модель принимает вид: \\(\\hat y_i = b_0 + b_2 x_{i2}\\)\nдля наблюдений из Group 2 (\\(I_{i1} = 1\\)) — вид: \\(\\hat y_i = (b_0 + b_1) + (b_2 + b_3) x_{i2}\\)\n\nКоэффициенты \\(b_0\\) и \\(b_1\\) называются базовыми и показывают интесепт и угловой коэффициент регрессионной прямой для одной из групп наблюдений. Коэффициенты \\(b_2\\) и \\(b_3\\) называются поправочными и показывают разницу в интерсетах и угловых коэффициентах регрессионных прямых для двух групп наблюдений.\nВизуально представить эту ситуацию можно так (\\(\\alpha_{\\text{Gr}_1}\\) и \\(\\alpha_{\\text{Gr}_2}\\) — углы наклона регрессионных прямых для Group 1 и Group 2 соответственно):\n\n\n\n\n\n\n\n\n\n\n\n8.2.5 Множественная линейная регрессия со взаимодействием количественных предикторов\nОкей, со взаимодействием категориальных и количественных предикторов все более-менее понятно. По крайней мере, это легко представимо графически. Возникает вопрос: возможно ли учесть в модели взаимодействие двух количественных предикторов? Да, это возможно. Модель от этого даже не слишком усложнится. Возьмем простейший случай, когда у нас два количественных предиктора. Тогда модель, включающая их взаимодействие, будет выглядеть так:\n\\[\ny_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + b_3 x_{i1}x_{i2} + e_i\n\\]\nКак можно наблюдать, взаимодействие — это, по сути, ещё один «предиктор», получающийся из перемножения двух исходных. Это достотачно сложно изобразить графически, и часто еще сложнее интерпретировать. Чтобы попытаться приблизиться к интерпретации, попробуем занулить сначала один предиктор, а затем второй, и посмотрим, что получается:\n\nесли \\(x_1 = 0\\), то модель приобретает следующий вид: \\(\\hat y_i = b_0 + b_2 x_{i2}\\),\nесли \\(x_2 = 0\\), то такой вид: \\(\\hat y_i = b_0 + b_1 x_{i1}\\).\n\nТо есть \\(b_1\\) и \\(b_2\\) показывают, соответственно, связи целевой переменной с предиктором, когда другой равен нулю — так называемые «условные» связи. Сам же коэффициент \\(b_3\\) при взаимодействии будет показывать сонаправленность связи предикторов с целевой переменной: если связи сонаправленны, то коэффициент положительный, если разнонаправленны — отрицательный. Его статистическая значимость будет говорить о том, «зависит» ли «влияние» одного предиктора от значений другого.\n\n\n8.2.6 Сравнение моделей\nКогда у нас есть несколько моделей с разным количеством предикторов, у нас возникает задача сравнить эти модели, чтобы выбрать наиболее простую и интерпретируемую модель, с которой нам дальше будет легче работать. Ведь вполне может случится такое, что мы добавили новый предиктор или несколько, а модель стала не сильно лучше. Здесь нам помогут два показателя.\n\n8.2.6.1 Скорректированный коэффициент детерминации\nМы уже говорили о коэффициенте детерминации, и, конечно же для множественной линейной регрессии он тоже рассчитывается. Однако есть хитрость: если несколько предикторов хотя бы как-то связаны с целевой переменной, при их включении в модель коэффициент детерминации неизбежно будет расти. По этой причине используется скорректированный коэффициент детерминации (adjusted R-squared). Коррестируется он на количество предикторов следующим образом:\n\\[\nR^2_{\\text{adj}} = 1 - (1 - R^2) \\frac{n-1}{n-p}\n\\]\nСкорректированный коэффициент детерминации позволяет сравнивать модели с разным количеством предикторов друг с другом.\n\n\n8.2.6.2 Частный F-критерий\nДля сравнения двух моделей существует и статистический тест, называемый частным F-критерием. Пусть у нас есть две модели — (1) полная (full), в которую включены несколько предикторов, и (2) сокращенная (reduced), из которой по сравнению с полной исключены некоторые предикторы:\n\\[\n\\begin{split}\n(1) &: y_i = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 + b_4 x_4 + e \\\\\n(2) &: y_i = b_0 + b_1 x_1 + b_2 x_2 + e\n\\end{split}\n\\]\nЧтобы выяснить, различаются ли статистически данные модели, испольуется следующая статистика:\n\\[\n\\begin{split}\nH_0 &: \\beta_3 = \\beta_4 = 0 \\\\\nH_1 &: \\beta_3 \\neq 0 \\vee \\beta_4 \\neq 0\n\\end{split}\n\\]\n\\[\nF = \\frac{(\\text{RSS}_\\text{reduced} - \\text{RSS}_\\text{full})/k}{\\text{RSS}_\\text{full} / (n-p)},\n\\]\n\\(\\text{RSS}_\\text{reduced}\\) — остаточная сумма квадратов сокращенной модели, \\(\\text{RSS}_\\text{full}\\) — остаточная сумма квадратов полной модели, \\(n\\) — количество наблюдений, \\(p\\) — количество предикторов полной модели, \\(k\\) — количество предикторов, исключенных из полной модели.\nСтатистический вывод осуществляется по стандартному алгоритму.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>L8 // Общие линейные модели. Простая и множественная линейная регрессия</span>"
    ]
  },
  {
    "objectID": "l9.html",
    "href": "l9.html",
    "title": "9  L9 // Дисперсионный анализ. Ковариационный анализ",
    "section": "",
    "text": "9.1 Регрессия только с категориальными предикторами\nПусть у нас есть один категориальный предиктор с двумя уровнями, то есть у нас есть две группы наблюдений. Если из моделей, рассмотренных ранее, мы исключим количественные предикторы, то получим модель такого вида:\n\\[\n\\hat y_i = b_0 + b_1 I,\n\\]\nгде \\(I\\) — переменная-индикатор, обозначающая, к какой группе принадлежит наблюдение (\\(I = 0\\), если наблюдение относится к первой группе 1, и \\(I = 1\\), если наблюдение относится ко второй группе). Итого, получается, что в одной модели заключены как бы две сразу:\n\\[\n\\begin{cases}\nI = 0 &: \\hat y_i = b_0 \\\\\nI = 1 &: \\hat y_i = b_0 + b_1\n\\end{cases}\n\\]\nЧто же будет, если у нас возникнет не две группы по какой-то категориальной переменной, а три? Тогда одной переменной-индикатором мы уже не обойдемся — нам понадобится две индикаторные переменные. Получится следующая модель:\n\\[\n\\hat y_i = b_0 + b_1 I_{\\text{Gr}_2} + b_2 I_{\\text{Gr}_3},\n\\]\nгде\n\\[\n\\begin{cases}\nI_{\\text{Gr}_2} = 0 \\wedge I_{\\text{Gr}_3} = 0 &: \\hat y_i = b_0 & (\\text{Group 1})\\\\\nI_{\\text{Gr}_2} = 1 \\wedge I_{\\text{Gr}_3} = 0 &: \\hat y_i = b_0 + b_1 & (\\text{Group 2}) \\\\\nI_{\\text{Gr}_2} = 0 \\wedge I_{\\text{Gr}_3} = 1 &: \\hat y_i = b_0 + b_2 & (\\text{Group 3})\n\\end{cases}\n\\]\nМы вводили в модели переменные-индикаторы для того, чтобы обозначит категориальные переменные и понять, как они меняют модель. Теперь же, когда мы работаем только с категориальными предикторами, мы можем обозначить перемеренные-индикаторы как \\(x_j\\), чтобы еще более наглядно увидеть, что модель с категориальными предикторами полностью совпадает с моделью обычной линейной регрессии. Значениями, которые принимают переменные-индикаторы, кодируются группы наблюдений. Мы можем составить следующую таблицу кодировки:\nТаким образом, мы можем записать модель следующим образом:\n\\[\n\\hat y_i = b_0 + b_1 x_1 + b_2 x_2\n\\]\nОбратим внимание, что сейчас мы рассматривает три группы и у нас две индикаторные переменные. В случае, если у нас \\(k\\) групп, то индикаторных переменных будет \\(k-1\\):\n\\[\n\\hat y_i = b_0 + b_1 x_1 + b_2 x_2 + \\dots + b_{k-1}x_{k-1}\n\\]\nОднако вернемся к случаю трех групп, так как этот случай достаточно прост и удобен для рассмотрения имеющейся ситуации. Попробуем визуализировать связь между некоторой целевой каоличественной переменной и категориальным предиктором. Получится следующая картинка:\nМы видим, что у нас есть категориальный предиктор, которые разбивает наши наблюдения на три группы — Gr₁, Gr₂ и Gr₃ — в каждой из которых есть определенный разборс значений. Само же положение это группы на вертикальной оси определяется средним целевой переменной в данной группе. Таким образом, мы можем дополнить визуализацию этими средними значениями (цвет добавлен для лучшего визуального разделения групп):\nМы видим, что с точки зрения линейной модели у нас есть три интерсепта, а с точки зрения данных эти три интерсепта являются среднии групп наблюдений. То есть,\n\\[\n\\cases{\n\\bar y_{\\text{Gr}_1} = b_0 \\\\\n\\bar y_{\\text{Gr}_2} = b_0 + b_1 \\\\\n\\bar y_{\\text{Gr}_3} = b_0 + b_2 \\\\\n}\n\\] или на картинке:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L9 // Дисперсионный анализ. Ковариационный анализ</span>"
    ]
  },
  {
    "objectID": "l9.html#регрессия-только-с-категориальными-предикторами",
    "href": "l9.html#регрессия-только-с-категориальными-предикторами",
    "title": "9  L9 // Дисперсионный анализ. Ковариационный анализ",
    "section": "",
    "text": "\\(I_{\\text{Gr}_2}\\) — переменная-индикатор, обозначающая принадлежность наблюдения ко второй группе (\\(I_{\\text{Gr}_2} = 0\\), если наблюдение не относится ко второй группе, и \\(I_{\\text{Gr}_2} = 1\\), если наблюдение относится ко второй группе)\n\\(I_{\\text{Gr}_3}\\) — переменная-индикатор, обозначающая принадлежность наблюдения к третьей группе (\\(I_{\\text{Gr}_3} = 0\\), если наблюдение не относится к третьей группе, и \\(I_{\\text{Gr}_3} = 1\\), если наблюдение относится к третьей группе). Итого, получается, что в одной модели заключены целых три сразу:\n\n\n\n\n\n\nГруппа\n\\(I_1 = x_1\\)\n\\(I_2 = x_2\\)\n\n\n\n\n\\(\\text{Gr}_1\\)\n0\n0\n\n\n\\(\\text{Gr}_2\\)\n1\n0\n\n\n\\(\\text{Gr}_3\\)\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nГруппа\n\\(x_1\\)\n\\(x_2\\)\n\\(\\dots\\)\n\\(x_{k-2}\\)\n\\(x_{k-1}\\)\n\n\n\n\n\\(\\text{Gr}_1\\)\n0\n0\n\\(\\dots\\)\n0\n0\n\n\n\\(\\text{Gr}_2\\)\n1\n0\n\\(\\dots\\)\n0\n0\n\n\n\\(\\text{Gr}_3\\)\n0\n1\n\\(\\dots\\)\n0\n0\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\ddots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(\\text{Gr}_{k-1}\\)\n0\n0\n\\(\\dots\\)\n1\n0\n\n\n\\(\\text{Gr}_k\\)\n0\n0\n\\(\\dots\\)\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n9.1.1 Параметризация индикаторов\nСпособ подбора коэффициентов модели, который мы только что рассмотрели, называется параметризацией индикаторов (dummy coding, treatment parametrization, reference cell model). В это способе получается следующее:\n\nодна из групп по категориальной переменной берется в качестве базовой — её интерсепт будет обозначен как \\(b_0\\)\nдля остальных групп подбираются поправочные коэффициенты (\\(b_1\\), \\(b_2\\), \\(...\\), \\(b_{k-1}\\)), которые определяют различия в интерсептах между этими группами и базовым уровнем\n\nСобственно, это ровно то, что и было у нас на предыдущих лекциях.\n\n\n9.1.2 Параметризация эффектов\nОднако на те же данные можно посмотреть и иным способом. Не всегда логично брать какую-то группу наблюдений в качестве базового уровня, к тому же в зависимости от того, какую из групп мы рассматриваем как базовую, меняются значения коэффициентов модели. Есть ли какой-то способ записать более «обобщённую» модель?\nДа, он есть. Давайте рассматривать в качестве базового уровня среднее по всем наблюдениям. Тогда коэффициентами при переменных в модели будут отклонения групповых средних от общего среднего. Вот картинка:\n\n\n\n\n\n\n\n\n\nСама математическая запись модели не изменится:\n\\[\n\\hat y_i = b_0 + b_1 x_1 + b_2 x_2\n\\]\nоднако интерпретация коэффициент здесь будет иная: теперь коэффициенты показывают отклонения [средних] групп от общего среднего, то есть коэффициенты показывают эффект предиктора для конкретной группы. Данный способ подбора называется параметризацией эффектов (effects coding, sum-to-zero parameterization). Отдельная интересность здесь в том, как считается интерсепт для третьей группы Gr₃ — он оказывает равен \\(b_0 - b_1 - b_2\\). Это связано с таблицей кодировки, использующейся в данном способе подбора коэффициентов:\n\n\n\nГруппа\n\\(x_1\\)\n\\(x_2\\)\n\n\n\n\n\\(\\text{Gr}_1\\)\n1\n0\n\n\n\\(\\text{Gr}_2\\)\n0\n1\n\n\n\\(\\text{Gr}_3\\)\n−1\n−1\n\n\n\nЧтобы модель работала, сумма по колонкам в таблице кодировки должна быть равна нулю, поэтому возникают −1.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L9 // Дисперсионный анализ. Ковариационный анализ</span>"
    ]
  },
  {
    "objectID": "l9.html#однофакторный-дисперсионный-анализ",
    "href": "l9.html#однофакторный-дисперсионный-анализ",
    "title": "9  L9 // Дисперсионный анализ. Ковариационный анализ",
    "section": "9.2 Однофакторный дисперсионный анализ",
    "text": "9.2 Однофакторный дисперсионный анализ\nЗадумаемся:\n\nу нас есть некоторая странная, но занимательная модель, получившаяся с использованием параметризации эффектов\nу нас есть данные, которые неопределенны и вариативны\nданные мы собирали для того, чтобы изучить связь между целевой переменной и предиктором\n\nРазумно предполагать, что если связь между целевой переменной и предиктором есть, то мы будем наблюдать какую-то такую картинку:\n\n\n\n\n\n\n\n\n\nЕсли же связи нет, то будет наблюдатся что-то такое:\n\n\n\n\n\n\n\n\n\nОписать эти ситуации можно, изучив структуру изменчивости данных.\n\n9.2.1 Структура изменчивости данных\nПри изучении линейной регрессии мы знакомились с такой метрикой изменчивости, как сумма квадратов — используем её и здесь. В данных есть общая изменчивость, или общая сумма квадратов (total sum of squares, \\(\\text{SS}_t\\)) — отклонения наблюдений от общего среднего значения:\n\n\n\n\n\n\n\n\n\nЕсть в данных факторная изменчивость, или объясненная сумма квадратов (explained sum of squares, \\(\\text{SS}_X\\)) — отклонения групповых средних от общего среднего:\n\n\n\n\n\n\n\n\n\nЭта изменчивость складывается из показателей того, насколько каждая группа в среднем отклоняется от общего среднего. Здесь надо сделать замечание, что термин фактор и факторная изменчивость возникает здесь по причине того, что часть изменчивости данных объясняется действием некоторого фактора, который в терминах математических моделей мы называем предиктором.\nИ в данных также остается случайная изменчивость, или сумма квадратов ошибок (error sum of squares, \\(\\text{SS}_e\\)) — отклонения наблюдений от своих групповых средних:\n\n\n\n\n\n\n\n\n\nС точки зрения математической модели эти три изменчивости — \\(\\text{SS}_t\\), \\(\\text{SS}_X\\) и \\(\\text{SS}_e\\) — совпадают соответственно с \\(\\text{TSS}\\), \\(\\text{ESS}\\) и \\(\\text{RSS}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nМатематически эти изменчивости записываются так:\n\\[\n\\begin{split}\n\\text{TSS} &= \\text{SS}_t = \\displaystyle \\sum_{i=1}^n (\\bar y - y_i)^2, \\\\\n\\text{ESS} &= \\text{SS}_X = \\displaystyle \\sum_{j=1}^k n_j \\cdot (\\bar y - \\bar y_j)^2, \\\\\n\\text{RSS} &= \\text{SS}_e = \\displaystyle \\sum_{j=1}^k \\sum_{i=1}^{n_j} (\\bar y_j - \\bar y_{ji})^2,\n\\end{split}\n\\]\nгде \\(n\\) — общее количество наблюдений, \\(n_j\\) — количество наблюдений в конкретной \\(j\\)-ой группе, \\(k\\) — количество групп.\nОкей, с самой структурой изменчивости разобрались. Теперь вернемся к двум ситуациям, которые мы хотели описать:\n\nсвязь межде предиктором и целевой переменной есть:\n\n\n\n\n\n\n\n\n\n\n\nсвязи между предиктором и целевой переменной нет:\n\n\n\n\n\n\n\n\n\n\nПоскольку рассматривать общую изменчивость не слишком осмысленно, в том числе потому что выполяется соотношение \\(\\text{TSS} = \\text{ESS} + \\text{RSS}\\) или \\(\\text{SS}_t = \\text{SS}_X + \\text{SS}_e\\), рассмотрим, как в двух интересующих нас случаях соотносятся объясненная (факторная) и случайная (остаточная) изменчивости.\n\nВ первом случае, когда закономерность есть, мы получим ситуацию, когда факторная изменчивость будет больше, чем случайная, то есть \\(\\text{SS}_X &gt; \\text{SS}_e\\).\nВ втором случае, когда закономерности нет, мы получим ситуацию, когда факторная изменчивость будет меньше (или, по крайней мере, равна), чем случайная, то есть \\(\\text{SS}_X \\leq \\text{SS}_e\\).\n\nПолучается, используя эти изменчивости мы можем изучать гипотезы о связи между количественной и категориальной переменной.\n\n\n9.2.2 Тестирование гипотез в однофакторном дисперсионном анализе\nЧтобы перейти к тестированию гипотез, нам необходимо сами эти [статистические] гипотезы сформулировать. Исходя из внимательного рассмотрения картинок выше, мы можем получить следующие гипотезы для дисперсионного анализа:\n\\[\n\\begin{split}\nH_0&: \\mu_0 = \\mu_1 = \\mu_2 = \\ldots = \\mu_k \\\\\nH_1&: \\exists \\, j_1, j_2: \\mu_{j_1} \\neq \\mu_{j_2}\n\\end{split}\n\\]\nНулевая гипотеза говорит, что средние всех групп в генеральной совокупности равны между собой. Альтернативная гипотеза говорит, что существуют хотя бы две группы, cредние которых различаются. Для тестирования этой гипотезы мы могли бы посмотреть на отношение между объясненной и остаточной изменчивостями (суммами квадратов), но количество наблюдений в разных группах имеет право на совпадать, поэтому используют средние квадраты — иначе, дисперсии:\n\\[\n\\begin{split}\n\\text{MS}_t &= \\frac{\\text{SS}_t}{n-1} = \\frac{\\text{TSS}}{n-1} = \\frac{\\sum_{i=1}^n (\\bar y - y_i)}{n-1} \\\\\n\\text{MS}_X &= \\frac{\\text{SS}_X}{k-1} = \\frac{\\text{ESS}}{k-1} = \\frac{n_j \\cdot \\sum_{j=1}^k (\\bar y - \\bar y_j)}{k-1} \\\\\n\\text{MS}_e &= \\frac{\\text{SS}_e}{n-k} = \\frac{\\text{RSS}}{n-k} = \\frac{\\sum_{j=1}^k \\sum_{i=1}^{n_j} (\\bar y_j - \\bar y_{ji})}{n-k}\n\\end{split}\n\\]\nСобственно, поэтому анализ и называется дисперсионный (analysis of variance, ANOVA). Статистика, используемая для тестирования заявленной гипотезы — это F-статистика, значение которой рассчитывается так:\n\\[\nF = \\frac{\\text{MS}_X}{\\text{MS}_e} \\overset{H_0}{\\thicksim} F(\\text{df}_{\\text{MS}_X}, \\text{df}_{\\text{MS}_e})\n\\]\nЭта статистика подчиняется F-распределению со степенями свободы \\(\\text{df}_{\\text{MS}_X} = k - 1\\) и \\(\\text{df}_{\\text{MS}_e} = n - k\\). Таким образом, если\n\nне получено статистически значимого результата, то у нас нет оснований говорить, что между какими-либо группами есть различия, а если\nполучен статистически значимый результат, то мы можем говорить, что между какими-либо двумя группами есть различия.\n\n\n\n9.2.3 Размер эффекта\nКогда мы обсуждали тестирование статистических гипотез, мы также упоминали о размере эффекта — метрики, которая позволяет оценить силу связи между переменными. В случае корреляционного анализа нам повезло, и сам коэффициент корреляции был размером эффекта для себя же. В случае линейной регрессии мы вроде бы не говорили о размере эффекта, однако если мы внимательно всмотримся в \\(R^2\\), то осознаем, что он и является метрикой, позволяющей говорить о силе связи между переменными — в случае множественной линейной регрессии о силе связи между целевой переменной и несколькими предикторами сразу.\nДля дисперсионного анализа также необходима специальная метрика. Она придумана и называется \\(\\eta^2\\). Она считается следующим образом:\n\\[\n\\eta^2 = \\frac{\\text{ESS}}{\\text{TSS}} = \\frac{\\text{SS}_X}{\\text{SS}_e}\n\\]\nИз формулы видно, что размер эффекта — это доля объясненной фактором дисперсии от всей дисперсии данных, что в общем-то весьма логично.\nДля интерпретации значения размера эффекта предлагаются следующие пороговые значения:\n\n\n\nЗначение \\(\\eta^2\\)\nРазмер эффекта\n\n\n\n\n\\(0.01\\)\nМалый (small)\n\n\n\\(0.06\\)\nСредний (medium)\n\n\n\\(0.14\\)\nБольшой (large)\n\n\n\n\n\n9.2.4 Попарные сравнения\nСтатистически значимый результат дисперсионного анализа приводит нас к интересному выводу: хотя бы двумя групами есть различия — но между какими? Ответа на этот вопрос дисперсионный анализ не дает, а знать ответ критически хочется. Отсюда возникаются попарные сравнения.\nЗадача попарных сравнений, которую можно нехитро вывести из их названия — сравнить попарно все группы наблюдений друг с другом и выяснить, есть ли между ними различия. F-статистика для этой задачи уже не подходит, поэтому берется t-статистика.\nМы уже видели так называемый одновыборочный t-тест в корреляционном анализе (при тестировании статистической значимости коэффициента корреляции) и регрессии (при тестировании статистической значимости коэффициента регрессионной модели). Однако t-тест позволяет сравнить и две выборки друг с другом. Формула его расчета несколько изменится, но общая логика останется той же — в числителе стоят сравниваемые средние, а в знаменателе расположена их стандартная ошибка:\n\\[\nt = \\frac{\\bar X_1 - \\bar X_2}{\\displaystyle \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\overset{H_0}{\\thicksim} t(\\text{df}),\n\\]\nгде \\(\\bar X_1\\) и \\(\\bar X_2\\) — средние сравниваемых групп, \\(s_1^2\\) и \\(s_2^2\\) — дисперсии сравниваемых групп, \\(n_1\\) и \\(n_2\\) — количество наблюдений в сравниваемых группах.\nГипотезы для t-теста в этом случае будут формулироваться так:\n\\[\n\\begin{split}\nH_0 &: \\mu_1 = \\mu_2 \\\\\nH_1 &: \\mu_1 \\neq \\mu_2\n\\end{split}\n\\]\n\n\nСтепени свободы вычисляются весьма хитро\n\n\\[\n\\text{df} = \\frac{\\left(\\displaystyle \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} \\right)^2}\n{\\displaystyle \\frac{1}{n_1 - 1} \\left(\\frac{s_1^2}{n_1} \\right)^2 + \\frac{1}{n_2 - 1} \\left(\\frac{s_2^2}{n_2} \\right)^2}\n\\]\n\n\nПолучается, что мы может взять t-тест, сравнить попарно группы друг с другом и выяснить, между какими группами есть различия. Почти. Иначе нам бессмысленно было бы городить дисперсионный анализ.\nВ силу проблемы множественных сравнений, которую мы обсуждали в теме тестирования статистических гипотез, необходимо скорректировать уровень значимости. Способы существуют разные — поправки Бонферрони, Холма, Тьюки — но суть одна: необходимо понизить уровень значимости, чтобы избежать увеличения вероятности ложноположительного вывода.\nИтого,\n\nесли мы получили статистически значимый результат дисперсионного анализа, необходимо провести попарные сравнения (другое название — post hoc тесты), чтобы выяснить, между какими именно группами есть различия\nесли мы не получили статистически значимый результат дисперсионного анализа, проводить попарные сравнения не нужно, так как сама нулевая гипотеза дисперсионного анализа говорит о том, что различий между группами нет\n\n\n\n9.2.5 Допущения дисперсионного анализа\nКак и другие линейные модели, дисперсионному анализ выдвигает ряд требований к данным:\n\nКоличественная непрерывная зависимая переменная\nНезависимые между собой выборки\n\nА если зависимые, то надо это учесть в модели\n\nНормальное распределение признака в генеральных совокупностях, из которых извлечены выборки\nРавенство (гомогенность) дисперсий изучаемого признака в генеральных совокупностях из которых извлечены выборки\n\nПроверяется с помощью теста Левина\n\nНезависимые наблюдения в каждой из выборок\n\n\nПодведем промежуточные итоги. Всё, о чем мы говорили выше, относится к однофакторному дисперсиионному анализу (One-way ANOVA), так как мы рассматривали ситуацию, когда нас интересует связь между количественной и одной категориальной переменной. В этом случае с точки зрения математической модели в ней будет один предиктор.\nОтдельно отметим случай, когда у нас категориальная переменная задаёт только две группы. Можно и нужно ли в этом случае использовать дисперсионный анализ или достаточно только t-теста? Есть утверждение, что для сравнения двух групп нужно использовать двухвыборочный t-тест, и это правда, так как t-тест позволяет сравнить средние в двух группах. Использовать дисперсиионный анализ для сравнения двух групп также можно, однако попарные сравнения в этом случае бессмысленны, так как у нас всего две группы. Более того, для случая двух групп выполняется следующее соотношение между F и t-статистиками:\n\\[\nF = t^2\n\\]\nДоказательство того факта можно найти здесь.\n\nКонечно, мы крайне редко в рамках своих исследований мы редко вводим в их дизайн только одну [независимую] переменную, поэтому далее мы рассмотрим более сложные модели. Без погружения в математику, так как логика остается аналогичной обсужденной выше.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L9 // Дисперсионный анализ. Ковариационный анализ</span>"
    ]
  },
  {
    "objectID": "l9.html#дизайн-экспериментального-исследования",
    "href": "l9.html#дизайн-экспериментального-исследования",
    "title": "9  L9 // Дисперсионный анализ. Ковариационный анализ",
    "section": "9.3 Дизайн экспериментального исследования",
    "text": "9.3 Дизайн экспериментального исследования\nПрежде чем перейти к многофакторному дисперсионному анализу, нам необходимо сказать несколько слов о дизайне экспериментального исследования. Почему мы здесь говорим именно об эксперименте? Так сложилось, что ANOVA очень хорошо подходит для анализа экспериментальных данных, однако это не значит, что ею можно анализировать только экспериментальные данные. Тем не менее, для разных экспериментальных дизайнов нам будут нужны разные модели дисперсионного анализа.\nОпуская детали, скажем, что принципиально существуют два возможных экспериментальных плана: межгрупповой и внутригрупповой.\n\nПри межгрупповом плане у нас есть две или более групп, каждая из которых проходит различные экспериментальные условия, одно из которых может выступать в роли контрольного. Таким образом, один испытуемый проходит одно экспериментальное условие. Соответственно, разные испытуемые проходят разные экспериментальные условия.\nПри внутригрупповом плане у нас есть по сути одна группа испытуемых, которая проходит все экспериментальные условия. Получается, что каждый испытуемый проходит два или более экспериментальных условия, одно из которых также может являться контрольным.\n\nС точки зрения модели у нас появляются два разных типа эффектов:\n\nмежгрупповые эффекты (between-subject effects), выражающие эффекты межгрупповых переменных (межгрупповой план)\nвнутригрупповые эффекты (within-subject effects), выражающие эффекты внутригрупповых переменных (внутригрупповой план)\n\nЕсли же у нас смешанный экспериментальный план, то в дизайне есть и межгрупповые, и внутригрупповые переменные, то и в модели будут оба типа эффектов.\nВ зависимости от того, каков дизайн нашего исследования, у нас будут разные модели дисперсионного анализа.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L9 // Дисперсионный анализ. Ковариационный анализ</span>"
    ]
  },
  {
    "objectID": "l9.html#многофакторный-дисперсионный-анализ",
    "href": "l9.html#многофакторный-дисперсионный-анализ",
    "title": "9  L9 // Дисперсионный анализ. Ковариационный анализ",
    "section": "9.4 Многофакторный дисперсионный анализ",
    "text": "9.4 Многофакторный дисперсионный анализ\nПодойдем к изучению моделей с несколькими категориальными предикторами с графической стороны. Посколько в случае нескольких факторов картинка может значительно усложнится, не будет визуализировать отдельные наблюдения, как мы делали выше, а воспользуемся изображением средних с доверительными интервалами. Для начала посмотрим, как такие графики будут выглядеть в случае с одним предиктором (фактором).\nДля случая, когда связь есть, мы имеем что-то такое:\n\n\n\n\n\n\n\n\n\nДля случая, когда связи нет, картинка выглядит как-то так:\n\n\n\n\n\n\n\n\n\nПусть теперь у нас есть не один фактор, а два — \\(A\\) и \\(B\\). Пусть оба фактора имеют по два уровня (задают каждый по две группы) — \\(A_1\\), \\(A_2\\) и \\(B_1\\), \\(B_2\\). В этом случае принципиально возможны несколько ситуаций.\n\nнет эффекта для обоих факторов\n\n\n\n\n\n\n\n\n\n\n\nесть эффекта для одного из факторов (в первом случае для фактора \\(A\\), во втором — для фактора \\(B\\))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nесть эффекта для обоих факторов\n\n\n\n\n\n\n\n\n\n\n\nесть взаимодействие факторов\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nВзаимодействие факторов говорит о том, что один фактор влияет на целевую переменную по-разному в зависимости от уровня второго фактора.\nС одной стороны, это довольно мощная фича дисперсионного анализа — возможность учесть взаимодействие факторов. Она позволяет нам посмотреть на происходящее в данных более подробно. Так, если мы будем смотреть только на основные эффекты, то можем потерять важные части закономерности:\n\n\n\n\n\nС другой стороны, значимое взаимодействие затрудняет интерпретацию основных эффектов. Если взаимодействие не значимо, то с интерпретацией главных эффектов трудностей не возникает. Если взаимодействие значимо, то обсуждать главнные эффекты необходимо аккуратно, или не обсуждать вовсе. В частности, нижний ряд рисунков выше показывает, как эффект фактора A частично маскирует эффект фактора B, что отражается во взаимодействии.\nКонечно, в модель можно ввести и более двух факторов, и логика тестирования статистической значимости останется, как и всегда, та же самая. Но помните, что чем сложнее модель, тем сложнее её интерпретация. А интерпретируя взаимодействие трёх предикторов вовсе можно сойти с ума.\n\nВ связи с этим, есть следующий момент. Когда вы планируете ваше исследование, сразу подумайте, как вы будете анализировать данные — что будет входить в модель в качестве основных предикторов, что в качестве ковариат, и какие взаимодействия в ней будут. Иначе измерить кучу переменных вы построите модель, результаты которой невозможно будет понять. Дизайн исследования очень тесно связан с аналитикой.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L9 // Дисперсионный анализ. Ковариационный анализ</span>"
    ]
  },
  {
    "objectID": "l9.html#partial-eta2",
    "href": "l9.html#partial-eta2",
    "title": "9  L9 // Дисперсионный анализ. Ковариационный анализ",
    "section": "9.5 Partial \\(\\eta^2\\)",
    "text": "9.5 Partial \\(\\eta^2\\)\nПоскольку теперь в модель включены несколько предикторов, они могут по-разному вкладываться в объяснение дисперсии. В каком-то смысле аналогично скорректированному \\(R^\\) существует и более точная метрика для размера эффекта конкретного фактора в дисперсионном анализе — это partial \\(\\eta^2\\). Она показывает долю объясненной фактором дисперсии при исключении вариативности, объясненной другими факторами. Эта метрика является основной для оценки размера эффекта в многофакторной дисперсионном анализе и именно она описывается в результатах.\n\\[\n\\eta^2_p = \\frac{\\text{SS}_X}{\\text{SS}_X + \\text{SS}_e}\n\\]\n\n9.5.1 Повторные измерения\nОтдельно оговорим ситуацию, когда у нас возникают внутригрупповые эффекты. В этом случае, напомним, каждый респондент проходит все условия эксперимента, в результате чего наблюдения оказываются связанными друг с другом — то есть нарушается допущение о независимости наблюдений. В силу этого нам нужна модификация исходной модели — дисперсионный анализ с повторными измерениями (repeated measures ANOVA, rm ANOVA). У этой модифицированной модели есть ещё одно допущение/требование к данным — сферичность.\nСферичность (sphericity) — это допущение, согласно которому дисперсии разностей между всеми парами уровней фактора равны. Нарушение допущение о сферичности — то есть отсутствие равенства дисперсий между парами уровней фактора — является серьезной проблемой для ANOVA, так как при этом тест становится слишком либеральным, то есть увеличение вероятность ошибки I рода.\nК нашему счастью, если есть нарушение сферичности данных, то машина сама нам об этом сообщит, а также применит необходимые поправки, чтобы избежать рост вероятности ошибки I рода.\n\n\n9.5.2 Типы сумм квадратов\nКогда у нас в анализе появляется несколько факторов, мы можем тестировать их значимость различными способами. Способ тестирования определяет типа сумм квадратов, которых существует три.\n\nПервый (I) тип сумм квадратов проводит последовательные тесты значимости факторов. Величина эффекта фактора зависит от объёма выборки. Кроме того, результат вычислений зависит от порядка включения факторов в модель, что не очень хорошо.\n\nне используется\n\nВторой (II) тип сумм квадратов проводит иерархические тесты, поэтому в этом случае результаты не зависят от порядка включения факторов в модель, однако величина эффекта все ещё зависит от объема выборки.\n\nхорошо работает на сбалансированных данных\n\nТретий (III) тип сумм квадратов проводит частные тесты. К данному подходу есть некоторые статистические вопросы, однако результаты в этом случае не зависят ни от порядка включения факторов в модель, ни от объёма выборки.\n\nиспользуют в случае несбалансированных данных, когда объемы групп по факторам различаются\n\n\nЕсли у нас экспериментальный дизайн исследования, то мы, во-первых, на уровне планирования исследования делаем всё возможное, чтобы группы были уравнены, а также у нас, как правило, есть возможность добрать испытуемых, если в какой-то из групп из не хватает. Тогда мы по умолчанию используем II тип суммы квадратов.\nЕсли же у нас, например, опросниковое исследование или такой дизайн, где респонденты разбиваются на группы post factum, мы не можем гарантировать, что эти группы окажутся равными по численности. В этом случае нам может помочь III типа суммы квадратов.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L9 // Дисперсионный анализ. Ковариационный анализ</span>"
    ]
  },
  {
    "objectID": "l9.html#контрасты",
    "href": "l9.html#контрасты",
    "title": "9  L9 // Дисперсионный анализ. Ковариационный анализ",
    "section": "9.6 Контрасты",
    "text": "9.6 Контрасты\nИногда может случиться довольно экзотичная ситуация: даже при наличии нескольких групп наблюдений нас могут интересовать не все различия, а вполне определенные — то есть нам не нужно сравнивать все группы со всеми, а необходимо сравить, скажем, одну группу со всеми остальными сразу. Например, у нас ситуация исследования в области образования. Мы хотим понять, как сказывается на качестве образования комбинация разных форматов обучения. Пусть целевой переменной будет итоговый балл студента по курсу. Есть три академические группы студентов: в одной освоение теоретической части курса происходит в формате лекций (\\(\\text{L}\\)), во второй — в формате групповой дискуссии (\\(\\text{G}\\)), в третьей реализован комбинированный формат (и лекции, и групповая дискуссия — \\(\\text{C}\\)). Сделать настоящую контрольную группу здесь не получается, поскольку нет какого-то нейтрального формата освоения теоретическая части, да и исключить полностью теоретическую часть из курса тоже как-то программа не очень позволяет.\nПри такой постановке задачи нас не интересуют различия между группами \\(\\text{L}\\) и \\(\\text{G}\\). Наверное, они будут различаться, но нам эта разница совершенно не важна, так как для нас обе эти группы выступают как контрольные. Интересует же нас различие между группами \\(\\text{C}\\) и \\(\\text{L+G}\\).\nМы можем, безусловно, подойти к решению этой задачи так, как предполагает дисперсионный анализ — изучить структуру изменчивости данных. Однако мы понимаем, что просто взять и соединить две группы \\(\\text{L}\\) и \\(\\text{G}\\) было бы не очень правомерно — все жё (1) могут различаться их средние, (2) хотелось бы учесть «истинную» (с учетом разделения на эти группы) случайную изменчивость. Для этого существуют контрасты.\nЛогика статистического теста остается похожей. У нас есть статистические гипотезы — в данном случае такие:\n\\[\n\\begin{split}\nH_0 &: \\mu_{\\text{C}} = \\mu_{\\text{L+G}} \\\\\nH_0 &: \\mu_{\\text{C}} \\neq \\mu_{\\text{L+G}}\n\\end{split}\n\\]\nДля тестирования гипотезы также используется F-статистика, которая считается аналогично:\n\\[\nF = \\frac{\\text{MS}_\\text{cont}}{\\text{MS}_e},\n\\]\nгде \\(\\text{MS}_e\\) — это случайная изменчивость, а \\(\\text{MS}_\\text{cont}\\) — «контрастная» изменчивость.\nСлучайная изменчивость вычисляется точно так же, как и раньше:\n\\[\n\\text{SS}_e = \\displaystyle \\sum_{j = \\text{\\{L,G,C\\}}} \\sum_{i=1}^{n_j} (\\bar y_j - \\bar y_{ji}),\n\\]\nОсталось понять, как вычисляется \\(\\text{MS}_\\text{cont}\\). Общая формула должна сохраниться, поэтому можно записать, что\n\\[\n\\text{MS}_\\text{cont} = \\frac{\\text{SS}_\\text{cont}}{\\text{df}_\\text{cont}}\n\\]\nТак как нам нужно сравнить две группы — \\(\\text{C}\\) и \\(\\text{L+G}\\) — то \\(\\text{df}_\\text{cont} = 2-1 = 1\\), а вот \\(\\text{SS}_\\text{cont}\\) считается довольно хитро, хотя и всё ещё аналогично тому, что мы видели раньше:\n\\[\n\\begin{split}\n\\text{SS}_\\text{cont} & = \\displaystyle \\sum_{j=\\{\\text{C, L+G}\\}} n_j \\cdot (\\bar y - \\bar y_j)^2 = \\\\\n& = n_\\text{C} \\cdot (\\bar y - \\bar y_\\text{C})^2 + n_\\text{L+G} \\cdot (\\bar y - \\bar y_\\text{L+G})^2 = \\\\\n&= n_\\text{C} \\cdot (\\bar y - \\bar y_\\text{C})^2 + (n_\\text{L} + n_\\text{G}) \\cdot \\left(\\bar y - \\frac{\\bar y_\\text{L} + \\bar y_\\text{G}}{2} \\right)^2\n\\end{split}\n\\]\nПолучается, что мы по сути сравниваем, различается ли положение группы \\(\\text{C}\\) и усредненное положение групп \\(\\text{L}\\) и \\(\\text{G}\\) относительно общего среднего. При этом случайная изменчивость посчитана в заменателе F-статистики в сооветствии с тем, как распределены наблюдения по имеющимся в данных группам.\nАналогично можно сравнить и две группы в отдельности, например, \\(\\text{C}\\) и \\(\\text{G}\\) — будет отличаться только подсчет \\(\\text{SS}_\\text{cont}\\):\n\\[\n\\begin{split}\nH_0 &: \\mu_{\\text{C}} = \\mu_{\\text{G}} \\\\\nH_0 &: \\mu_{\\text{C}} \\neq \\mu_{\\text{G}}\n\\end{split}\n\\]\n\\[\nF = \\frac{\\text{MS}_\\text{cont}}{\\text{MS}_e},\n\\]\n\\[\n\\text{SS}_e = \\displaystyle \\sum_{j = \\text{\\{L,G,C\\}}} \\sum_{i=1}^{n_j} (\\bar y_j - \\bar y_{ji}),\n\\]\n\\[\n\\text{MS}_\\text{cont} = \\frac{\\text{SS}_\\text{cont}}{\\text{df}_\\text{cont}}\n\\]\n\\[\n\\begin{split}\n\\text{SS}_\\text{cont} & = \\displaystyle \\sum_{j=\\{\\text{C,G}\\}} n_j \\cdot (\\bar y - \\bar y_j)^2 = \\\\\n& = n_\\text{C} \\cdot (\\bar y - \\bar y_\\text{C})^2 + n_\\text{G} \\cdot (\\bar y - \\bar y_\\text{G})^2\n\\end{split}\n\\]\nПоскольку в случае контрастов мы делаем конкретные сравнения либо группами наблюдений, делать попарные сравнения нам не нужно — результаты контрастов уже показывают интересующие нас различия.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L9 // Дисперсионный анализ. Ковариационный анализ</span>"
    ]
  },
  {
    "objectID": "l9.html#ковариационный-анализ",
    "href": "l9.html#ковариационный-анализ",
    "title": "9  L9 // Дисперсионный анализ. Ковариационный анализ",
    "section": "9.7 Ковариационный анализ",
    "text": "9.7 Ковариационный анализ\nКогда мы обсуждали регрессионные модели с количественными и категориальными предикторами, мы говорили, что модель может включать взаимодействие количественного и категориального предиктора, а может не включать его. Если мы посмотрим на модель множественной линейной регрессии, в которой отсутствуют взаимодействия дискретных и непрерывных предикторов с точки зрения дисперсионного анализа, то получим модель ковариационного анализа (analysis of covariance, ANCOVA). Мы можем подойти с другой стороны и сказать, что модель ковариационного анализа — это модель дисперсионного анализа, в которую включён (включены) один или несколько непрерывных предикторов. Непрерывный предиктор называется ковариатой. Ковариата — это переменная, которая [потенциально] связана с нашей целевой переменной, но её влияние не является целью нашего анализа.\nМатематическая модель будет абсолютно та же самая, что и раньше:\n\\[\n\\hat y_i = b_0 + b_1 I + b_2 x_2 = b_0 + b_1 x_1 + b_2 x_2\n\\]\nЭто самый простой вариант. Здесь \\(x_1\\) — категориальный предиктор (принимает значения \\(0\\) и \\(1\\)), \\(x_2\\) — ковариата, или непрерывный предиктор. Как можно заметить, взаимодействие в модель не включено.\nЗачем вообще нам может понадобиться ковариата? На нашу целевую переменную могут влиять, помимо интересующих нас факторов, еще и разные другие. Вполне может случится такое, что различия, обнаруживаемые нами в ходе дисперсионного анализа, связаны не с влиянием фактора, а с действием каких-либо сторонних переменных. Введение ковариаты позволяет действие таких переменных учесть. Ковариата объяснит часть дисперсии данных, и вполне возможно, что значимость фактора пропадет. Пример такой ситуации мы рассмотрим на практике.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>L9 // Дисперсионный анализ. Ковариационный анализ</span>"
    ]
  },
  {
    "objectID": "l10.html",
    "href": "l10.html",
    "title": "10  L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия",
    "section": "",
    "text": "10.1 Ограничения общих линейных моделей\nМодели, которые мы изучали на предыдущих занятиях носят название общих линейных моделей (general linear models). Они достаточно просты и удобны в большинстве случаев, однако имеют существенное ограничений.\nВспомним, как выглядит уравнение такой модели:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\varepsilon\n\\]\nПредикторы в такой модели, как мы знаем, могут быть как дискретными, так и непрерывными. Однако ключевым допущением (и требованием) этой модели является распределение ошибки:\n\\[\n\\varepsilon \\thicksim \\mathcal{N}(0, \\sigma^2)\n\\]\nПоскольку ошибка модели должна быть распределена нормально, а моделируется среднее значение, то можно сформулировать более общее допущение/требование:\n\\[\ny \\thicksim \\mathcal{N}(\\mu, \\sigma^2)\n\\]\nТаким образом, общие линейные модели позволяют моделировать зависимости только для нормально-распределенных величин. Если же отклик модели (он же зависимая переменная) подчиняется другому распределению, эти модели не годятся.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия</span>"
    ]
  },
  {
    "objectID": "l10.html#идея-обобщенных-линейных-моделей",
    "href": "l10.html#идея-обобщенных-линейных-моделей",
    "title": "10  L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия",
    "section": "10.2 Идея обобщенных линейных моделей",
    "text": "10.2 Идея обобщенных линейных моделей\nТем не менее, мы из своего опыта знаем, что существует много величин, распределение которых отличается от нормального. И дело даже не столько в асимметрии или эксцессе — здесь разговор о самой природе величин. Так, например, есть величина «поступление в вуз», у которой только два возможных значения — «поступил» и «не поступил». Или же, скажем, количество детей в семьей, варьирующееся в небольших пределах и принимающее только положительные целочисленые значения.\nНам хочется моделировать связь таких величин с предикторами так же, как мы делали в случае общих линейных моделей, или хотя бы в той же логике, которая нам хороша знакома из линейной регрессии. Для того, чтобы этого достичь, в модель вводится новый компонент — функция связи (link function).\n\nПусть у нас есть некоторая переменная \\(y\\), которая подчиняется какому-то закону распределения \\(f(y|\\theta)\\), где \\(\\theta\\) — параметр(ы) распределения, и этот закон отличается от нормального.\nМы всё так же хотим моделировать «среднее» значение, а точнее, математическое ожидание, переменной \\(\\mathbb{E}(y)\\).\nДавайте возьмем некоторое преобразование (функцию) \\(g \\big( \\mathbb{E}(y) \\big) = \\eta\\), которое будет преобразовывать математическое ожидание нашей целевой переменной, линеаризуя его. Она и будет называться функцией связи.\nТеперь у нас есть линейная величина \\(\\eta\\), которую можно моделировать с помощью уже хорошо знакомой нам модели:\n\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\varepsilon\n\\]\n\nОднако нас всё же интересует с точки зрения изученя закономерностей не \\(\\eta\\), а \\(\\mathbb{E}(y)\\), но так как функция \\(g \\big( \\mathbb{E}(y) \\big)\\) известна, мы возьмем обратную функцию \\(g^{-1} (\\eta) = \\mathbb{E}(y)\\) и получим интересующие нас значения.\n\nТакова общая логика обобщенных линейных моделей (generalized linear models, GLM). Какая именно функция связи (и, соответственно, обратная функция) будет использоваться, зависит от распределения целевой переменной. Далее мы рассмотрим два конкретных примера. Но уже сейчас мы можем написать общее уравнения для таких моделей:\n\\[\ng \\big( \\mathbb{E}(y_i) \\big) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon_i\n\\]\nОдин из особых случаев возникает, когда к целевой переменной не применяется никакого преобразования — то есть используется \\(g \\big( \\mathbb{E}(y)) = \\mu\\) при \\(y \\thicksim \\mathcal{N}(\\mu, \\sigma^2)\\). Такая функция связи называется функцией индентичности (identity function). В этом случае получается следующее:\n\\[\ng \\big( \\mathbb{E}(y_i) \\big) = \\mu_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon_i\n\\]\nТо есть мы получили обычную хорошо нам знакомую линейную регрессию. В этом смысле обобщенные линейные модели действительно обобщают случай обычной линейной регрессии на другие случай распределения целевой переменной.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия</span>"
    ]
  },
  {
    "objectID": "l10.html#биноминальная-регрессия",
    "href": "l10.html#биноминальная-регрессия",
    "title": "10  L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия",
    "section": "10.3 Биноминальная регрессия",
    "text": "10.3 Биноминальная регрессия\nРассмотрение фреймворка обобщенных линейных моделей мы начнем с биномиальной, или логистической, регрессии, которая позволяет моделировать бинарные переменные.\n\n10.3.1 Бинарные переменные и биномиальное распределение\nБинарными называются переменные, которые могут принимать только два значения — вообще, любых, но так как нам нужнв такие обозначения, которые сможет переварить математика, используем \\(0\\) и \\(1\\). Для нас это будут просто лейблы классов, так как мы, заменив обозначения, остались в рамках номинальной шкалы.\nВ теме про случайные величины мы вычисляли вероятность случайного прохождения теста из \\(n\\) вопросов, каждый из которых мог быть решен правильно («успех», \\(1\\)) или неправильно («неудача», \\(0\\)), и описывали вероятность получить \\(k\\) «успехов» в \\(n\\) испытаниях как\n\\[\n\\mathbb{P}(X = k) = C_n^k \\, p^k \\, q^{n-k}\n\\]\nЭта формула и задает биномиальное распределение: \\(\\mathbb{P}(X = k) \\thicksim \\text{Bin}(n, p)\\):\n\n\n\n\n\n\n\n\n\nОт этого распределения и происходит название рассматриваемой регрессионной модели.\n\n\n10.3.2 Математическая модель\nТем не менее, хоть биномиальное распределения и существует, реализация \\(0\\) и \\(1\\) в отдельном наблюдении полчинается не ему1, поэтому первым концептуальным шагом построения модели биномиальной регрессии является переход от моделирования \\(0\\) и \\(1\\) к моделированию вероятности получения \\(1\\).\nРассмотрим картинку. Пусть мы пытаемся смоделировать связь между бинарной целевой переменной \\(Y\\) и количественным предиктором \\(X\\). Тогда её визуализация будет схематично выглядеть так:\n\n\n\n\n\nТогда мы можем посчитать доли единиц в общем количестве исходов при данном значении предиктора \\(p_{Y=1|x_i}\\) и использовать их как оценку вероятности \\(\\mathbb{P}(Y=1|x_i)\\):\n\n\n\n\n\nОтлично, мы получили непрерывную переменную! В принципе, можно попытаться смоделировать её с помощью линейной регрессии, однако возникнет некоторая проблема:\n\n\n\n\n\nПеременная, которую мы получили, определяет вероятность, а значит она ограничена — \\(0 \\leq \\mathbb{P}(Y=1|x_i) \\leq 1\\). К тому же, чисто визуально заметно, что эта вероятность изменяется нелинейной, а по некоторой кривой — примерно так:\n\n\n\n\n\nСупер… Теперь еще и искать кривую…\n\n10.3.2.1 Логистическая кривая\nК счаcтью, математики поработали за нас, и сообщили нам, что такая закономерность хорошо моделируется логистической кривой (logistic curve) — отсюда второй название рассматриваемой нами модели (логистическая регрессия):\n\n\n\n\n\n\n\n\n\nЭта прямая как раз зажата по \\(y\\) между нулем и единицей, а её изгибы хорошо подходят под связь вероятности \\(\\mathbb{P}(Y=1|x_i)\\) со значением предиктора \\(X\\).\nЕё формула такова:\n\\[\ny = \\frac{e^x}{1 + e^x}\n\\]\nОднако если мы хотим описывать зависимость вероятности от значения предиктора, то вместо \\(x\\) нам необходимо подставить \\(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots \\beta_p x_{ip}\\):\n\\[\n\\mathbb{P}(Y=1|x_i) = p_i = \\frac{e^{\n\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots \\beta_p x_{ip}}\n}\n{1 + e^{\n\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots \\beta_p x_{ip}}\n}\n\\]\nВыглядит, безусловно, страшно. Но наш путь еще не завершен…\n\n\n10.3.2.2 Шансы и логиты\nМы победили дискретность целевой переменной \\(y\\) и подобрали кривую, которая хорошо моделирует зависимость искомой вероятности от значений предиктора. Однако кривая всё ещё ограничена нулем и единицей, а значит смоделировать её м помощью линейной регрессии не получится.\nДля того, чтобы победить ограниченность логистической кривой, используются шансы.\nШанс (отношение шансов, odds, odds ratio) — это отношение вреоятности «успеха» (\\(1\\)) к вероятности «неудачи» (\\(0\\)). Эта величина хороша тем, что изменяется от \\(0\\) до \\(+\\infty\\). Получается,\n\\[\n\\text{odds}_i = \\frac{\\mathbb{P}(Y=1|x_i)}{1 - \\mathbb{P}(Y=1|x_i)} = \\frac{p_i}{1 - p_i}\n\\]\nОтлично, мы побороли ограниченность логистической кривой сверху — движемся к успеху.\n\n\n10.3.2.3 Logit-преобразование\nДля того, чтобы победить ограниченность кривой снизу, возьмем логарифм от шанса. Получим следующее:\n\\[\n\\text{logit}(p_i) = \\ln \\left(\\frac{p_i}{1 - p_i} \\right)\n\\]\nТакое преобразование вероятности разывается логит-преобразованием (logit-transformation). Значения логитов варьируются от \\(-\\infty\\) до \\(+\\infty\\), симметричны относительно нуля, и их удобно брать в каечестве целевой переменной для построения линейной модели. Кроме того, логит-преобразование еще и линеаризует логистическую кривую! Очень хорошее преобразование! Просто замечательное.\n\n\nЛинеаризация логистической кривой через logit-преобразование\n\nВозьмем случай в одним предиктором, чтобы было попроще. Логистическая кривая, моделирующая вероятность, имеет такой вид:\n\\[\np_i = \\frac{e^{\\beta_0 + \\beta_1 x_{i1}}}\n{1 + e^{\\beta_0 + \\beta_1 x_{i1}}}\n\\]\nОбозначим \\(\\beta_0 + \\beta_1 x_{i1} = t\\). Тогда необходимо показать, что логит-преобразование\n\\[\n\\text{logit} (p_i) = \\ln \\left(\\frac{p_i}{1 - p_i} \\right)\n\\]\nделает логистическую функцию линейной, то есть обычной прямой. Иначе говоря, необходимо показать, что\n\\[\n\\ln \\left(\\frac{p_i}{1 - p_i} \\right)= t_i\n\\]\nДоказывается это через расписывание формулы и раскрытие всех скобок и логарифмов (индекс \\(i\\) опущен для упрощения записи):\n\\[\n\\begin{split}\n\\ln \\left(\\frac{p}{1-p} \\right)&= \\\\\n&= \\ln \\left(\\frac{\\frac{e^t}{1 + e^t}}{1 - \\frac{e^t}{1 + e^t}} \\right)= \\\\\n&= \\ln \\left(\\frac{e^t}{1 + e^t} \\right)- \\ln \\left(1 - \\frac{e^t}{1 + e^t} \\right)= \\\\\n&= \\ln \\left(\\frac{e^t}{1 + e^t} \\right)- \\ln \\left(\\frac{1 + e^t - e^t}{1 + e^t} \\right)= \\\\\n&= \\ln \\left(\\frac{e^t}{1 + e^t} \\right)- \\ln \\left(\\frac{1}{1 + e^t} \\right)= \\\\\n&= \\ln (e^t) - \\ln (1 + e^t) - \\big(\\ln (1) - \\ln (1+e^t)\\big) = \\\\\n&= \\ln (e^t) - \\ln (1) = \\\\\n&= \\ln (e^t) = t\n\\end{split}\n\\]\n\n\n\n\n\n\n\n\nCаммари того, что происходило выше:\n\n\n\n\nОт дискретной оценки событий (0 и 1) переходим к оценке вероятностей.\nСвязь вероятностей с предиктором описывается логистической кривой.\nЕсли при помощи функции связи перейти от вероятностей к логитам, то связь будет описываться прямой линией.\nПараметры линейной модели для такой прямой можно оценить с помощью регрессионного анализа.\n\n\n\nВ итоге мы получаем, что математическая модель логистической регрессии выглядит так:\n\\[\np_i = \\frac{e^{\\beta_0 + \\beta_1 x_{i1}}}\n{1 + e^{\\beta_0 + \\beta_1 x_{i1}}}\n\\]\nФункция связи — логит:\n\\[\n\\text{logit} (p_i) = \\ln \\left(\\frac{p_i}{1 - p_i} \\right)= \\eta_i\n\\]\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip}\n\\]\nА это ровно то, что нам было нужно!\nДля перехода от логитов к вероятностям используется обратная функция вида\n\\[\np_i = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}}\n\\]\n\n\n\n10.3.3 Идентификация модели\nВсе эти модификации модели приводят к тому, что задача индентификации модели логистической регрессии — то есть вычисления значений коэффициентов модели — не имеет аналитического решения. То есть мы не можем вычислить коэффциент просто имея значения целевой переменной и предикторов. По этой причине используются численные методы для получения оценок коэффициентов.\n\n10.3.3.1 Метод максимального правдоподобия\nВ частности, метод максимального правдоподобия (maximuum likelihood), который позволяет получить несмешенные и состоятельные и эффективные оценки коэффициентов.\nПравдоподобие (likelihood) — это способ измерить соответствие имеющихся данных тому, что можно получить при определенных значениях параметров модели.\nВычисляется значение правдоподобия как произведение вероятностей получения каждой из точек данных:\n\\[\nL(\\theta|\\text{data}) = \\prod_{i=1}^n f(\\text{data}|\\theta),\n\\]\nгде \\(f(\\text{data}|\\theta)\\) — функция распределения с параметрами \\(\\theta\\).\nЗадача метода максимального правдоподобия — найти наиболее правдоподобное решение, иначе говоря, максимизировать значение функции правдоподобия:\n\\[\nL(\\theta|\\text{data}) \\to \\max_{\\mathbf{b}}\n\\]\nТак как правдоподобие — это произведение вероятностей, то функция правдоподобия принимает очень маленькие значение, поэтому работают с максимизацией логарифма правдоподобия (loglikelihood):\n\\[\n\\ln \\big( L(\\theta | \\text{data}) \\big) \\to \\max_{\\mathbf{b}}\n\\]\n\n\n\n10.3.4 Тестирование качества модели\nПриколы, связанные с преобразованиями на этом не заканчиваются. При переходе к обобщенным линейным моделям у нас пропадают две важные статистики: \\(F\\)-статистика и \\(R^2\\) — их теперь невозможно рассчитать, так как мы моделируем вероятность получения «единиц», а не сами «единицы» и «нули».\nЧто же делать?\n\n10.3.4.1 Девианса\nНам на помощь приходит девианса (deviance). Чтобы попробовать понять, что это такое, введем две теоретические модели:\n\nНасыщенная модель (saturated model) — модель, в которой каждое наблюдение (сочетание предикторов) описывается одним из \\(n\\) параметров. Для такой модели справедливо:\n\n\\[\n\\begin{split}\n& \\ln L_\\text{sat} = 0 \\\\\n& \\text{df}_\\text{sat} = n - p_\\text{sat} = n - n = 0\n\\end{split}\n\\]\n\nНулевая модель (null model) — модель, в которой все наблюдения описываются одним параметром (средним значением). Для такой модели справедливо:\n\n\\[\n\\begin{split}\n& \\eta_i = \\beta_0 \\\\\n& \\ln L_\\text{null} \\neq 0, \\; \\ln L_\\text{null} \\to -\\infty \\\\\n& \\text{df}_\\text{null} = n - p_\\text{null} = n - 1\n\\end{split}\n\\]\nНаша же [предложенная] модель, то есть та, с которой мы работаем, будем находится где-то между насыщенной и нулевой моделями:\n\\[\n\\begin{split}\n& \\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} \\\\\n& \\ln L_\\text{model} \\neq 0 \\\\\n& \\text{df}_\\text{model} = n - p_\\text{model}\n\\end{split}\n\\]\nДевианса является мерой различия правдоподобий двух моделей (оценка разницы логарифмов правдоподобий)\n\n\n\n\n\n\nОстаточная девианса: \\(d_\\text{resid} = 2(\\ln L_\\text{sat} - \\ln L_\\text{model}) = -2 \\ln L_\\text{model}\\)\nНулевая девианса: \\(d_\\text{null} = 2(\\ln L_\\text{sat} - \\ln L_\\text{null}) = -2 \\ln L_\\text{null}\\)\n\n\n\n10.3.4.2 Анализ девиансы\nСравнение нулевой и остаточной девианс позволяет судить о статистической значимости модели в целом. Такое сравнение проводится с помощью теста отношения правдоподобий (likelihood ratio test, LRT).\n\\[\n\\begin{split}\nd_\\text{null} - d_\\text{resid} &= \\\\\n&= -2 (\\ln L_\\text{null} - \\ln L_\\text{model}) = \\\\\n&= 2 (\\ln L_\\text{model} - \\ln L_\\text{null}) = \\\\\n&= 2 \\ln \\left(\\frac{L_\\text{model}}{L_\\text{null}} \\right)\n\\end{split}\n\\]\n\\[\n\\text{LRT} = 2 \\ln \\left(\\frac{L_\\text{M1}}{L_\\text{M2}} \\right)= 2 (\\ln L_\\text{M1} - \\ln L_\\text{M2}),\n\\]\nгде \\(\\text{M1}\\) и \\(\\text{M2}\\) — вложенные модели (\\(\\text{M1}\\) — более полная, \\(\\text{M2}\\) — сокращенная). Распределение разницы логарифмов правдоподобий аппроксимируется распределением \\(\\chi^2\\) со степенями свободы \\(\\text{df} = \\text{df}_\\text{M2} - \\text{df}_\\text{M1}\\).\nТо есть, в общем случае тест отношения правдоподобий позволяет статистически сравнить две модели друг с другом. В случае же тестирования значимости модели в целом получается следующее:\n\\[\n\\begin{split}\n& \\text{LRT} = 2 \\ln \\left(\\frac{L_\\text{model}}{L_\\text{null}} \\right)= 2 (\\ln L_\\text{model} - \\ln L_\\text{null}) = d_\\text{null} - d_\\text{model} \\\\\n& \\text{df} = p_\\text{model} - 1\n\\end{split}\n\\]\nКроме того, поскольку мы можем исключить из модели только один предиктор, тест отношения правдоподобий может быть использован и для тестирования значимости отдельных предикторов:\n\\[\n\\begin{split}\n& \\text{LRT} = 2 \\ln \\left(\\frac{L_\\text{model}}{L_\\text{reduced}} \\right)= 2 (\\ln L_\\text{model} - \\ln L_\\text{reduced}) \\\\\n& \\text{df} = p_\\text{model} - p_\\text{reduced}\n\\end{split}\n\\]\nИтак, тест отношения правдоподобий является аналогом F-статистики. Что же делать с отсутствием \\(R^2\\)?\nЗдесь также помогает девианса. Одним из вариантов оценки качества модели является расчет доли объясненной девиансы:\n\\[\n\\frac{d_\\text{null} - d_\\text{residual}}{d_\\text{null}}\n\\]\nЭта метрика — одна из разновидностей «псевдо-\\(R^2\\)».\n\n\n10.3.4.3 Информационные критерии\nПомимо статистических критериев, сравнить модели можно также с использованием информационных критериев. Их два — баейсовский (Bayesian information criterion, BIC) и информационный критерий Акаике (Akaike information criterion, AIC). Чем ниже значение информационного критерия, тем лучше модель описывает имеющиеся данные.\n\n\n10.3.4.4 Допущения логистической регрессии\nРяд допущений логистической регрессии совпадает с допущениями общих линейных моделей. В частности:\n\nнезависимость наблюдений\nлинейность связи целевой переменной и предикторов (с учетом функции связи)\nотсутствие коллинеарности предикторов\n\nОднако появляется еще одно важное допущение — отсутствие сверхдисперсии.\n\n\n10.3.4.5 Проверка на сверхдисперсию\nНовое допущение связано с тем, что для биномиального распределения характерна связт между математическим ожиданием и дисперсией — этого не было у нормального распределения. Для биномиального распределения:\n\\[\n\\begin{split}\n& \\mathbb{E}(X) = np \\\\\n& \\text{var}{X} = np(p-1)\n\\end{split}\n\\]\nгде \\(n\\) — количество испытаний, \\(p\\) — вероятность «успеха» в одном испытании.\nЕсли в модели обнаруживается свердисперсия, то мы на может гарантировать, что искомая закономерность смоделирована точно. В детали расчета и проверки гипотезы мы прогружать не будет, на практике воспользуемся специальной функцией для проверки этого допущения.\n\n\n\n10.3.5 Тестирование значимости предикторов\nС одним из способ тестирование статистической значимости предикторов мы уже познакомились — это тесты отношения правдоподобий. Однако существует и второй способ — в каком-то смысле более «базовый» — тесты Вальда.\n\n10.3.5.1 Тесты Вальда\nЭти тесты являются аналогом t-тестов для общих линейных моделей, однако являются менее точными, так как распределение их z-статистики только ассимптотически стремится к нормальному. Это значит, что на малых выборках эти тесты буду давать неточные результаты.\nТем не менее, они автоматически выводятся в аутпуте функций в R, поэтому посмотрим на них — они критически похожи на t-тесты:\n\\[\n\\begin{split}\nH_0 &: \\beta_k = 0 \\\\\nH_1 &: \\beta_k \\neq 0\n\\end{split}\n\\]\n\\[\nz = \\frac{b_k - \\beta_k}{\\text{se}_{b_k}} = \\frac{b_k}{\\text{se}_{b_k}} \\thicksim \\mathcal{N}(0, 1)\n\\]\n\n\n\n10.3.6 Интерпретация коэффициентов модели\nС интерпретацией значений коэффициентов тоже оказывается не все просто. Вспомним, коэффициенты какой модели мы в итоге получаем:\n\\[\n\\eta_i = \\hat \\beta_0 + \\hat \\beta_1 x_{i1} + \\hat \\beta_2 x_{i2} + \\dots + \\hat \\beta_p x_{ip}\n\\]\nНаша целевая переменная в модели, для которой мы получили значения предикторов — это логарифм отношения шансов. Довольно сложно понять, что это такой с содержательной стороны. Однако чисто технически получается следующее:\n\n\\(\\hat  \\beta_0\\), интерсепт, показывает логарифм отношения шансов для случая, когда значения всех предикторов равны нулю\n\\(\\hat \\beta_k\\) показывает, на сколько изменится логарифм отношения шансов при изменении значения предиктора на единицу\n\nКорректно, но всё её непонятно…\nДавайте посмотрим, что получится, если расписать изменение логарифма отношения шансов. Пусть для простоты у нас есть модель с одним непрерывным предиктором:\n\\[\n\\eta = b_0 + b_1 x\n\\]\nПри этом\n\\[\n\\eta = \\ln \\left(\\frac{p}{1-p} \\right)= \\ln (\\text{odds})\n\\]\nКак изменится предсказание модели при увеличении предиктора на единицу?\n\\[\n\\eta_{x+1} - \\eta_x = \\ln (\\text{odds}_{x+1}) - \\ln (\\text{odds}_x) = \\ln \\left(\\frac{\\text{odds}_{x+1}}{\\text{odds}_x} \\right)\n\\]\nС другой стороны:\n\\[\n\\begin{split}\n\\eta_{x+1} - \\eta_x &= \\big( b_0 + b_1(x+1) \\big) - \\big( b_0 + b_1 x \\big) = \\\\\n&= b_0 + b_1 x + b_1 - b_0 - b_1 x = b_1\n\\end{split}\n\\]\nПолучается, что\n\\[\n\\begin{split}\n\\ln \\left(\\frac{\\text{odds}_{x+1}}{\\text{odds}_x} \\right)&= b_1 \\\\\n\\frac{\\text{odds}_{x+1}}{\\text{odds}_x} = e^{b_1}\n\\end{split}\n\\]\nТаким образом, \\(e^{b_1}\\) показывает, во сколько раз изменится шанс того, что наблюдение принадлежит к группе «единиц» при увеличении предиктора на единицу. Для дискретных предикторов \\(e^{b_1}\\) покажет, во сколько раз различается отношение шансов для данного уровня предиктора по сравнению с базовым.\n\n\n10.3.7 Предсказательная сила модели\nОценить предсказательную силу общих линейных моделей не сложно — надо сравнить предсказанные значения с предскавленными в данных. С логистической моделью же есть некоторые тонкости.\nВ данных у нас лежат нули и единицы, а модель нам возвращает вероятность того, что отдельное наблюдение является единицей. Для того, чтобы нам получить предсказанные значения, которые мы будем сопоставлять с имеющимися в данных, нам необходимо перевести непрерывные предсказания в дискретные.\nДля этого необходимо выбрать порог — если значение вероятности выше него, мы будем считать, что модель предсказала \\(1\\), если ниже, то \\(0\\). Значение порога зависит от многих факторов и будет влиять на качество модели. Прежде всего стоит ориентироваться на сферу деятельности, в которой вы проводите анализ. Если у вас качественные чистые данные и вам важна высокая точность, то и порог для предсказаний должен быть высокий — \\(\\geq 0.9\\). Если же вы знаете, что вы работаете с зашумлёнными данными, и цена ошибки не так высока, то можете выбрать более либеральный критерий — \\(0.7\\)–\\(0.8\\). Самый либеральный критерий из возможных — \\(0.5\\), что по сути есть вероятность случайного угадывания.\n\n10.3.7.1 Confusion mattrix\nКогда мы перевели вероятности в нули и единицы, мы можем построить confusion matrix — — это таблица частот по модельным и реальным значениям нашей целевой переменной. Её общая структура выглядит так:\n\n\n\n\nПредсказания: \\(0\\)\nПредсказания: \\(1\\)\n\n\n\n\nДанные: \\(0\\)\n\\(\\text{TN}\\)\n\\(\\text{FP}\\)\n\n\nДанные: \\(1\\)\n\\(\\text{FN}\\)\n\\(\\text{TP}\\)\n\n\n\n\nTrue Positive (\\(\\text{TP}\\)) — верное предсказанные единицы\nTrue Negative (\\(\\text{TN}\\)) — верно предсказанные нули\nFalse Positive (\\(\\text{FP}\\)) — ложноположительные предсказания, ошибочно предсказанные единицы\nFalse Negative ($ — ложноотрицательные предсказания, ошибочно предсказанные нули\n\nНа основе данных значение можно расчитать несколько метрик качества модели.\n\n10.3.7.1.1 Accuracy\nЧаще всего эту метрику называют «точность». Она определяется по формуле\n\\[\n\\text{accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n\\]\nОна показывает долю верно предказанных значений и хорошо работает на сбалансированных данных, когда у вас одинаковое количество нулей и единиц в исходном датасете. Однако в случае несбалансированных данных даже самая плохая модель может иметь высокий показатель accuracy. Например, в данных такие значения целевой переменной:\n\\[\n\\begin{pmatrix}\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1\n\\end{pmatrix}\n\\]\nПусть у нас есть наиболее топорная модель, которая просто всегда предсказывает ноль — у такой модели accuracy окажется равной \\(0.73\\). Вроде бы высокий показатель, но совершенное неадекватно описывает качество модели.\nПоэтому были придуманы более точные метрики.\n\n\n10.3.7.1.2 Precision\nPrecision тоже переводится как «точность», поэтому лучше пользоваться английской терминологией во избежании путаницы. Эта метрика показывается долю верно предсказанных единиц, то есть сколько из предсказанных единиц предсказано верно:\n\\[\n\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\]\n\n\n10.3.7.1.3 Recall\nRecall можно перевести как «полнота», хотя по сути это снова «точность». Эта метрика показывается долю предсказанных единиц из всех единиц датасета, то есть сколько из всех единиц датасета модель предсказала верно:\n\\[\n\\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\nОчевидно, что чем ближе показатели всех метрик к единице, тем качество модели выше.\n\n\n10.3.7.1.4 F1-мера\nНа основе precision и recall вычисляется ещё одна метрика качества, которая является гармоническим средним этих двух метрик.\n\\[\n\\text{F1} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n\\]\n\n\n\n10.3.7.2 ROC-AUC\nВсе описанные выше метрики качестве основывались на confusion matrix — их значения будут зависеть от выбранного порога при переводе непрерывных предсказаний в дискретные. А можно как-то без это зависимости?\nМожно. Напоследок ещё одна метрика качества модели. Однако логика её расчёта не такая простая.\nЧем хороша эта метрика? Тем, что она работает не с предсказанными значениями (0 и 1), а с предсказанными вероятностями. Таким образом, она избавляет нас от вмешательства нас же в предсказания, ведь когда мы выбираем порог, мы хотя и опираемся на какое-то содержательное основание, тем не менее, глобально выбираем его условно произвольно.\nЧто рассчитать значение этой метрики нужно сделать следующее (разберём на некотором вымышленном примере):\n\nУпорядочить объекты по убыванию значения предсказанной вероятности:\n\n\n\n\n№ наблюдения\nВероятность\n\n\n\n\n1\n1.00\n\n\n2\n0.90\n\n\n3\n0.80\n\n\n4\n0.75\n\n\n5\n0.60\n\n\n6\n0.50\n\n\n7\n0.43\n\n\n8\n0.32\n\n\n9\n0.20\n\n\n10\n0.15\n\n\n\n\nДобавить столбец истинных значений (0 и 1)\n\n\n\n\n№ наблюдения\nВероятность\nЗначение\n\n\n\n\n1\n1.00\n1\n\n\n2\n0.90\n1\n\n\n3\n0.80\n0\n\n\n4\n0.75\n1\n\n\n5\n0.60\n0\n\n\n6\n0.50\n1\n\n\n7\n0.43\n0\n\n\n8\n0.32\n0\n\n\n9\n0.20\n0\n\n\n10\n0.15\n0\n\n\n\nОтметим, что если модель идеально справляется с предказаниями, то в упорядоченном по значению предсказанной вероятности наборе данных сначала будут идти все наблюдения с истинным значением \\(1\\), а потом с \\(0\\).\n\nПостроим кривую, которая будет описывать качество нашей модели:\n\n\nСтартуем из точки \\((0,0)\\) и хотим прийти в точку \\((1,1)\\):\n\n\n\n\n\n\n\nОсь \\(y\\) делим на равные части, число которых равно количеству \\(1\\) в датасете:\n\n\n\n\n\n\n\nОсь \\(x\\) делим на равные части, число которых равно количеству \\(0\\) в датасете:\n\n\n\n\n\n\n\nИдём по нашим данным сверху вниз, и когда встречаем наблюдение со значением \\(1\\), поднимаемся на графике на одно деление вверх; когда встречаем наблюдение со значением \\(0\\), сдвигаемся на одно деление вправо.\nВ итоге получится такая кривая:\n\n\n\n\n\n\nКаждая точка на этой кривой соответствует некоторому порогу вероятности отсечения объектов. Например, выделенная точка c координатами \\((\\frac{1}{6}), \\frac{3}{4})\\) соответствует порогу вероятности \\(0.6\\). Это означает, что если при переходе к предсказаниями модели мы будем использовать порог \\(0.6\\), то доля True Positive, или True Positive Rate (TRP) окажется равной \\(\\frac{3}{4}\\), а доля False Positive, или False Positive Rate (FPR) окажется равной \\(\\frac{1}{6}\\).\nПостроенная кривая называется ROC-кривой (receiver operating characteristic).\n\nОпределяем площадь фигуры под ROC-кривой — это и будет значением метрики ROC-AUC (AUC — area under a curve).\n\n\n\n\n\n\nВ случае идеального упорядочивания наблюдений по предсказанной вероятности площадь под кривой будет равна единице. Чем ближе значение ROC-AUC к единице, тем модель работает лучше. Значение \\(0.5\\) указывает на то, что модель совсем не ухватывает закономерность и точность её предсказаний на уровне случайного угадывания — ROC-кривая в этом случае будет проходить близко к диагонали \\([(0,0),(1,1)]\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия</span>"
    ]
  },
  {
    "objectID": "l10.html#пуассоновская-регрессия",
    "href": "l10.html#пуассоновская-регрессия",
    "title": "10  L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия",
    "section": "10.4 Пуассоновская регрессия",
    "text": "10.4 Пуассоновская регрессия\n\n10.4.1 Счетные данные\nВ жизни и практике мы часто сталкиваемся с так называемыми счётными величинами. Например, число комнат в квартире, количество детей в семье, число книг на полке, число людей, прошедших через турникет и т.д. Глобально — любые количества. Также как счётные величины можно рассмотреть шкалу Лайкерта или оценки по десятибалльной шкале — по сути, это количество набранных баллов2.\nКакими свойствами обладают такие величины?\n\nОни могут принимать только неотрицательные целочисленные значения (\\(x_i \\in \\mathbb{N}_{0}\\))\nРазброс значений зависит от среднего значения (\\(\\text{var}(X) \\propto \\mathbb{E}(X)\\))\n\nОбщие линейные модели, строго говоря, применимые только к непрерывным величинам1. Иногда свойства данных позволяют использовать такие методы для моделирования счётных величин, однако так бывает далеко не всегда.\nЗдесь мы обсудим самый простой подход к моделированию счетных величин, а также одну его модификацию, которая может быть полезна.\nМодели для счётных данных базируются на распределении Пуассона.\n\n\n10.4.2 Распределение Пуассона\nОпределяется следующим образом:\n\\[\nY \\thicksim \\text{Poisson} (\\mu)\n\\]\n\\[\nf(y) = \\frac{\\mu^y e - \\mu}{y!}\n\\]\nЕдинственный его параметр — это \\(\\mu\\). Он задает и математические ожидание, и дисперсию:\n\\[\n\\begin{split}\n\\mathbb{E}(Y) = \\mu \\\\\n\\text{var}(Y) = \\mu\n\\end{split}\n\\]\nВ зависимости от того, какие значения принимает этот параметр, распределение принимает достаточно сильно различающиеся формы. При малых значениях (\\(\\mu = 1\\), \\(\\mu = 2\\)) в распределении присутствует сильная левосторонняя асимметрия, при больших (\\(\\mu = 10\\)) — распределение становится симметричным и очень похожим на нормальное распределение.\n\n\n\n\n\n\n\n\n\nНеобходимо отметить, что пуассоновское распределение предполагает, что дисперсия связана с математическим ожидание через функцию идентичности, то есть с увеличением математического ожидания дисперсия возрастает ровно так же, как и само математичекое ожидание. Это факт нам будет важен далее.\n\n\n10.4.3 Почему общие линейные модели плохо работают?\nДва свойства счетны случайных величин, оговоренные в самом начале раздела, объясняют, почему общие линейные модели плохо работают (или, по крайне мере, могут плохо работать) на счетных данных.\nПервое свойство нам говорит о том, что количество не может быть отрицательным, и это логично. Обчная линейнная регрессия не имеет подобных ограничений, поэтому в предсказаниях будут появляться отрицательные значения. Что с ними делать не очень понятно.\nВторое свойство говорит нам о том, что изначально не будет выполнено допущение гомоскедастичности остатков, так как чем выше математичекое ожидание, тем выше дисперсия. В итоге мы будем наблюдать воронкообразный паттер в распределении остатков.\nТаким образом, оценки коэффициентов модели будут неточны, ошибки завышены, а следовательно, и результатам тестирования статистической значимости параметров модели доверять нельзя.\nЧто делать? Можно пойти простым путём — логарифмировать целевую переменную и построить модель для получившейся величины.\nНо более корректным вариантом будет построить модель, основанную на распределении, подходящем для счётных данных. В частности, пуассоновском распределении.\n\n\n10.4.4 Математическая модель\nТак как мы находимся во фреймворке обобщенных линейных моделей, значит ключевой момент с формулировании модели — функция связи. У пуассоновской регрессии она очень проста — это всего лишь логарифм:\n\\[\n\\eta_i = \\ln (y_i)\n\\]\nПрименяя её к нашей переменной, мы получаем величину \\(\\eta_i\\), которую можем моделировать линейной моделью. Соответственно, идентифицируемая модель будет такой же, как и прежде:\n\\[\n\\eta_i = \\hat \\beta_0 + \\hat \\beta_1 x_{i1} + \\hat \\beta_2 x_{i2} + \\dots + \\hat \\beta_p x_{ip}\n\\]\n\n\n10.4.5 Результаты моделирования\nАбсолютно аналогично биномиальной регрессии для пуассоновской будет:\n\nотсутствовать \\(F\\)-статистика\nотсутствовать \\(R^2\\)\nнеобходим анализ девиансы\nзначимость коэффициентов тестироваться z-тестами Вальда\nнеобходима проверка на сверхдисперсию\n\nТак как в биномиальной регрессии мы обсудили это достаточно подробно, не будем на этом останавливаться.\n\n\n10.4.6 Квазипуассоновские модели\nОднако по сравнению с биномиальной регрессией у пуассоновской есть одна особенность — а именно, возможность работать со сверхдисперсией.\nТак как пуассоновская модель исходит из предположения равенства дисперсии и математического ожидания, если мы не обнаруживаем сверхдисперсии, то все хорошо. Если же это допущение не выполнено, то\n\nоценки стандартных ошибок коэффициентов будут занижены\nтесты Вальда для коэффициентов модели дадут неправильные результаты — из-за того, что оценки стандартных ошибок занижены— уровень значимости будет занижен\nтесты, основанные на сравнении правдоподобий, дадут смещенные результаты, так как соотношение девианс уже не будет подчиняться распределению \\(\\chi^2\\)\n\nПроисходят все эти проблемы из-за того, что для распределения Пуассона справедливы следующие соотношения:\n\\[\n\\begin{split}\n& \\text{var}(y_i) = \\mu_i \\\\\n& \\text{var}(\\mathbb{E}(y_i)) = \\frac{\\mu}{n} \\\\\n& \\text{se}_{\\mathbb{E}(y_i)} = \\sqrt{\\text{var}\\big( \\mathbb{E}(y_i) \\big)}\n\\end{split}\n\\]\nЕсли обнаружена сверхдисперсия, то данные не подчиняются распределению Пуассона, и дисперсия в \\(\\phi\\) раз больше среднего (\\(\\phi &gt; 1\\)). Тогда,\n\\[\n\\begin{split}\n& \\text{var}(y_i) = \\phi \\mu_i \\\\\n& \\text{var}(\\mathbb{E}(y_i)) = \\frac{\\phi \\mu}{n} \\\\\n& \\text{se}_{\\mathbb{E}(y_i)} = \\sqrt{\\phi \\text{var}\\big( \\mathbb{E}(y_i) \\big)}\n\\end{split}\n\\]\nКаковы могут быть причины сверхдисперсии? Перечислим основные:\n\nв данных есть выбросы\nв модель не включен важный предиктор или взаимодействие предикторов\nнарушена независимость выборок (есть внутригрупповые корреляции)\nвыбрана неподходящая функция связи\nвыбрана неподходящая функция распределения для целевой переменной\n\nА что же делать? Пусти есть разные. Мы рассмотрим наболее простой — построение квазипуассоновских моделей.\nНе стоит думать, что существует «квази-пуассоновское» распределение —- эти модели также основываются на распределение Пуассона, но учитывают тот самый коэффициет \\(\\phi\\), описывающий сверхдисперсию. Это, по сути, поправка на сверхдисперсию.\nСама модель не меняется по сравнению с пуассоновской регрессией, коэффициент \\(\\phi\\) оценивается по данным. Кроме того, оценки параметром пуассоновских и квазипуассоновских моделей совпадают, однако в квазипуассоновских моделях:\n\nстандартные ошибки коэффициентов домножатся на \\(\\sqrt{\\phi}\\)\nдоверительные интервалы коэффициентов домножаются на \\(\\sqrt{\\phi}\\)\nлогарифмы правдоподобий, используемые для тестирования статистической значимости моделей, уменьшаются в \\(\\phi\\) раз\n\nА так как тестирование статистической значимости работает со стандартными ошибками, то и статистическая значимость также изменяется.\nТакже есть отличия и в статистических тестах:\n\nдля анализа девиансы используются F-тесты\nдля тестирования эначимости коэффициентов используются t-тесты\n\nЭто ключевые отличия квазипуассоновских моделей от пуассоновских — в остальном они полностью вписываются во фреймворк GLM.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия</span>"
    ]
  },
  {
    "objectID": "l10.html#footnotes",
    "href": "l10.html#footnotes",
    "title": "10  L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия",
    "section": "",
    "text": "А распределению Бернулли.↩︎\nХотя тут можно поспорить.↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия</span>"
    ]
  },
  {
    "objectID": "l12.html",
    "href": "l12.html",
    "title": "11  L12 // Линейные модели со смешанными эффектами",
    "section": "",
    "text": "11.1 Ограничения изученных ранее линейных моделей\nПри построении обычных линейных моделей мы предполагаем, что наши наблюдения независимы. Однако зачастую это требование не выполняется в полной мере, и наши данные имеют некоторую группировку (clustered data), которую, возможно, мы даже не планировали.\nПримеры возможных группировок:\nВозникают внутригрупповые корреляции (intraclass correlations) — наблюдения из одной группы более похожи друг на друга, чем наблюдениях из разных групп.\nЧтобы решить возникающие трудности, необходимо ввести в модель случайные факторы.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L12 // Линейные модели со смешанными эффектами</span>"
    ]
  },
  {
    "objectID": "l12.html#ограничения-изученных-ранее-линейных-моделей",
    "href": "l12.html#ограничения-изученных-ранее-линейных-моделей",
    "title": "11  L12 // Линейные модели со смешанными эффектами",
    "section": "",
    "text": "измерения в разные периоды времени (разные партии химических реактивов, разные настройки аппаратуры)\nизмерения в разных участках пространства (мониторы компьютеров могут различаться)\nповторные измерения (испытуемые / респонденты различаются между собой)\nизмерения на разных группах испытуемых / респондентов (школьники разных классов в одной параллели могут различаться)\n\n\n\nПодобную структуру данных некорректно игнорировать — увеличивается вероятность ошибиться с выводами.\nИнтуитивное решение: включить группирующие факторы в модель в качестве предикторов — технически так, безусловно, сделать можно, однако такой подход,\n\nво-первых, значительно усложняет модель (чрезмерно увеличивается количество параметров),\nа во-вторых, ограничивает широту обобщения результатов (интерпретаировать параметры модели можно только для конкретных испытуемых / респондентов / учебных классов).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L12 // Линейные модели со смешанными эффектами</span>"
    ]
  },
  {
    "objectID": "l12.html#случайные-vs-фиксированные-факторы",
    "href": "l12.html#случайные-vs-фиксированные-факторы",
    "title": "11  L12 // Линейные модели со смешанными эффектами",
    "section": "11.2 Случайные vs фиксированные факторы",
    "text": "11.2 Случайные vs фиксированные факторы\nДо сих пор мы работали только с фиксированными факторами — моделировали среднее значение для каждго уровня фактора.\n\n\n\n\n\n\n\n\n\nЕсли групп возникает много, то и моделируемых средных значений также много. Кроме того, когда мы задавали фиксированные факторы, мы считали, что сравниваемые группы фиксированные, и нас интересуют сравнения именно между ними.\nОднако когда группировка возникает не как результат дизайна исследования, а как некоторый побочный результат (то есть, мы не планировали изучать фактор, по которому разделилась наша выборка), нам не важны конкретные значения интерсептов в каждой из групп. Мы можем представить данный фактор как случайную величину (величину «поправки»), и оценить дисперсию между уровнями группирующего фактора. Это и есть случайные факторы.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nСвойства\nФиксированные факторы\nСлучайные факторы\n\n\n\n\nУровни фактора\nФиксированные, заранее определенные, потенциально воспроизводимые\nСлучайная выборка из всех возмоных уровней\n\n\nИспользуются для тестирвоания гипотез\nО средних значения ЗП на раных уровнях фактора \\(H_0:\\mu_1 = \\mu_2 = \\dots = \\mu\\)\nО дисперсии ЗП между уровнями фактора \\(H_0:\\sigma_r^2 = 0\\)\n\n\nВыводы можно экстраполировать\nТолько на уровни анализа\nНа все возможные уровни\n\n\n\nНа один и тот же фактор можно смотреть и как на случайный, и как на фиксированный в зависимости от задач исследователя. Так как мы хотим моделировать дисперсию, то у случайного фактора должно быть минимум пять градаций.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L12 // Линейные модели со смешанными эффектами</span>"
    ]
  },
  {
    "objectID": "l12.html#виды-glmm-и-их-математическая-формулировка",
    "href": "l12.html#виды-glmm-и-их-математическая-формулировка",
    "title": "11  L12 // Линейные модели со смешанными эффектами",
    "section": "11.3 Виды GLMM и их математическая формулировка",
    "text": "11.3 Виды GLMM и их математическая формулировка\nМы строим прямые — модели же линейные. А прямая задается двумя параметрами — интерсептом и углом наклона (slope). Эти параметры мы оцениваем в качестве фиксированных факторов. Но так как других у нас нет, то эти же параметры оцениваются и как случайные факторы. То есть, случайные факторы как бы дополняют фиксированные.\n\nМодель со случайным интерсептом\n\n\\[\n\\begin{split}\ny_{ij} &= \\beta_0 + \\beta_1 x_{ij} + \\eta_i + \\varepsilon_{ij} \\\\\n\\eta_i &\\thicksim \\mathcal{N}(0, \\sigma^2_\\eta) \\\\\n\\varepsilon_i &\\thicksim \\mathcal{N}(0, \\sigma^2)\n\\end{split}\n\\]\n\\(i\\) — наблюдение (респондент), \\(j\\) — уровни (значения) предиктора.\n\n\n\n\n\n\n\n\n\n\nМодель со случайным интерсептом и углом наклона\n\n\\[\n\\begin{split}\ny_{ij} &= \\beta_0 + \\beta_1 x_{ij} + \\eta_{0i} + \\eta_{1ij} x_{ij} + \\varepsilon_{ij} \\\\\n\\eta_{0i} &\\thicksim \\mathcal{N}(0, \\sigma^2_{\\eta_0}) \\\\\n\\eta_{1ij} &\\thicksim \\mathcal{N}(0, \\sigma^2_{\\eta_1}) \\\\\n\\varepsilon_{ij} &\\thicksim \\mathcal{N}(0, \\sigma^2)\n\\end{split}\n\\]\n\\(i\\) — наблюдение (респондент), \\(j\\) — уровни (значения) предиктора.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L12 // Линейные модели со смешанными эффектами</span>"
    ]
  },
  {
    "objectID": "l12.html#методы-подбора-параметров-в-смешанных-моделях",
    "href": "l12.html#методы-подбора-параметров-в-смешанных-моделях",
    "title": "11  L12 // Линейные модели со смешанными эффектами",
    "section": "11.4 Методы подбора параметров в смешанных моделях",
    "text": "11.4 Методы подбора параметров в смешанных моделях\nПараметры в смешанных моделях могут подбираться двумя методами. Первый из них — метод максимального правдоподобия (maximum likelihood, ML).\nОднако когда мы пытаемся оценить дисперсию методом максимального правдоподобия, оценки получаются смещенными. Это происходит потому, что сразу приходится оценивать и \\(\\beta\\), и дисперсии.\nЭтого можно избежать, применяя метод ограниченного максимального правдоподобия (restricted maximum likelihood, REML). Данный метод позволяет с помощью математических преобразований занулить \\(\\beta\\) и получить несмещенные оценки дисперсий.\nREML-оценки \\(\\beta\\) стремятся к ML-оценкам при увеличении объема выборки.\nТак что же использовать, REML или ML?\n\nЕсли нужны точные оценки фиксированных эффектов — ML.\nЕсли нужны точные оценки случайных эффектов — REML.\nЕсли нужно работать с правдоподобиями — следите, чтобы в моделях, подобранных REML была одинаковая фиксированная часть.\nДля обобщенных негауссовских смешанных линейных моделей REML не определен — там используется ML.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L12 // Линейные модели со смешанными эффектами</span>"
    ]
  },
  {
    "objectID": "l12.html#диагностика-модели",
    "href": "l12.html#диагностика-модели",
    "title": "11  L12 // Линейные модели со смешанными эффектами",
    "section": "11.5 Диагностика модели",
    "text": "11.5 Диагностика модели\n\n11.5.1 Индуцированные корреляции\nПоявление в модели случайного фактора позволяет учесть взаимосвязь наблюдений для каждого из респондентов — «индуцированные» корреляции.\nПосмотрим внимательно на случайную часть модели.\nСлучайные эффекты, распределенные нормально со средним 0 и некоторой дисперсией\n\\[\n\\eta \\thicksim \\mathcal{N}(0, \\sigma^2_\\eta)\n\\]\nОстатки модели, независимые распределенные нормально со средним 0 и некоторой дисперсией\n\\[\n\\varepsilon \\overset{\\text{i.i.d}}{\\thicksim} \\mathcal{N}(0, \\sigma^2)\n\\]\nПутем математических преобразований матриц ковариаций можно получить, что корреляция между наблюдениями одного субъекта равна следующему выражению.\n\\[\n\\text{ICC} = \\frac{\\sigma^2_\\eta}{\\sigma^2_\\eta + \\sigma^2}\n\\]\nЭта характеристика называется коэффициент внутриклассовой корреляции (intra-class correlation, ICC).\nТаким образом, ICC — это способ измерить, насколько коррелируют друг с другом наблюдения из одной и той же группы, заданной случайным фактором. Значения ICC интерпретируются аналогично коэффициенту корреляции.\nЕсли ICC низкий, то наблюдения очень разные внутри каждой из групп. Значит, чтобы надежно оценить эффект этого случайного фактора, нужно брать больше наблюдений в группе.\nЕсли ICC высокий, то наблюдения очень похожи внутри каждой из групп, заданных случайным фактором. Значит, можно брать меньше наблюдений в группе.\nЭто можно использовать при определении объема выборки (при анализе пилотных данных).\n\n\n11.5.2 Pseudo-\\(R^2\\)\nДля смешанных моделей существует аналог коэффициента детерминации, только теперь их два:\n\nMarginal \\(R^2\\) — доля дисперсии, объясненной фиксированными факторами\nConditional \\(R^2\\) — доля дисперсии, объясненной моделью в целом (и фиксированными, и случайными факторами)\n\n\n\n11.5.3 Анализ остатков модели\nОстается ряд допущений, классических для линейных моделей:\n\nлинейность связи\nраспределение остатков\n(не)зависимость остатков от предсказанных значений\n\nНо так как в модели темерь есть и случайные факторы, мы можем изучать зависимость остатков от случайных факторов.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L12 // Линейные модели со смешанными эффектами</span>"
    ]
  },
  {
    "objectID": "l12.html#тестирование-гипотез",
    "href": "l12.html#тестирование-гипотез",
    "title": "11  L12 // Линейные модели со смешанными эффектами",
    "section": "11.6 Тестирование гипотез",
    "text": "11.6 Тестирование гипотез\nС помощью смешанных моделей можно тестировать статистические гипотезы. Но есть проблема: тесты, применяемые для GLM (t- и z-тесты Вальда, LRT) дают приблизительные оценки. Для отбора моделей используют информационные критерии (AIC).\n\n11.6.1 t-тесты Вальда\n\\[\n\\begin{split}\n&H_0: \\beta_k = 0 \\\\\n&H_1: \\beta_k \\neq 0 \\\\\nt &= \\frac{b_k - \\beta_k}{\\text{se}_{b_k}} = \\frac{b_k}{\\text{se}_{b_k}} \\thicksim t(n-p)\n\\end{split}\n\\]\n\nТак как эти тесты дают лишь приблизительные оценки, их значения не приводятся в аутпутах функций.\nИх можно вернуть, однако при интерпретации получаемых результатов всегда нужно помнить, что это только приблизительные оценки.\n\n\n\n11.6.2 Тесты отношения правдоподобий\n\\[\n\\text{LRT} = 2 \\ln \\Big( \\frac{L_{\\text{M}_1}}{L_{\\text{M}_2}} \\Big ) = 2 (\\ln L_{\\text{M}_1} - L_{\\text{M}_2})\n\\]\nLRT для случайных эффектов\n\nТребуются модели с одинаковой фиксированной частью, подобранные REML\nУровни значимости будут завышены\nОбычно тесты не делают, а набор случайных эффектов определяется структурой данных\n\nLRT для фиксированных эффектов\n\nТребуются модели с одинаковой случайной частью, подобранные ML\nУровни значимости будут занижены",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L12 // Линейные модели со смешанными эффектами</span>"
    ]
  },
  {
    "objectID": "l12.html#сравнение-моделей",
    "href": "l12.html#сравнение-моделей",
    "title": "11  L12 // Линейные модели со смешанными эффектами",
    "section": "11.7 Сравнение моделей",
    "text": "11.7 Сравнение моделей\nДля сравнения моделей актуальны ранее изученные варианты:\n\nтесты отношения правдоподобий\nинформационные критерии (AIC, BIC)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L12 // Линейные модели со смешанными эффектами</span>"
    ]
  },
  {
    "objectID": "l12.html#модели-для-других-распределений",
    "href": "l12.html#модели-для-других-распределений",
    "title": "11  L12 // Линейные модели со смешанными эффектами",
    "section": "11.8 Модели для других распределений",
    "text": "11.8 Модели для других распределений\nСовместив идеи GLM и mixed models можно получить модели для бинарных переменных.\n\nВ основе анализа стоит подбор параметров логистической регрессионной модели\nПараметры логистической регрессии подбираются методом максимального правдоподобия\nУгловые коэффициенты логистической регрессии позволяют сказать, во сколько раз изменяется соотношение шансов для события при увеличении предиктора на единицу (или при переходе от базового уровня фактора к данному уровню)\nДля визуализации результатов лучше проводить обратное логит-преобразование и отражать зависимую переменную в терминах вероятностей\nГруппирующие факторы могут определять дисперсию свободного члена модели или дисперсию углового коэффициента и свободного члена модели\nВнутриклассовые корреляции могут быть вычислены лишь приблизительно, но их рассмотрение дает важную информацию. Если ICC равен нулю, то лучше отказаться от рассмотрения случайных факторов и применить обычную GLM.\n\nАналогично можно получить смешанные модели для счетных переменных.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>L12 // Линейные модели со смешанными эффектами</span>"
    ]
  },
  {
    "objectID": "l13.html",
    "href": "l13.html",
    "title": "12  L13 // Кластерный анализ",
    "section": "",
    "text": "12.1 Геометрическая интерпретация задачи кластеризации\nНапомним себе, что\nТеперь посмотрим на данные. Как мы знаем,\nЛюбой объект мы можем описать числовым вектором, где числа задают значение характеристик объектов. Если это количественные характеристики, то тут всё понятно — это сами числовые значения переменных. А если характеристики качественные? Никаких проблем — мы их перекодируем в числа! Если это бинарные переменные (например, пол или ступень обучения «бакалавр»/«магистр»), то одну категорию обозначим 0, другую — 1. Если категорий больше, то у нас просто будет больше чисел-индикаторов. Итого, каждое наблюдение описывается числовым вектором, а следовательно, и некоторой точкой в пространстве.\nКаково это пространство? Оси — это переменные, то есть характеристики объектов. Измерений в этом признаковом пространстве столько, сколько переменных в нашем датасете.\nНаша задача — объединить похожие наблюдения в группы. А какие наблюдения являются похожими? Логично допустить, что те, которые обдалают схожими характеристиками. Если характеристики объектов схожи, то в признаковом пространстве они будут располагаться близко друг к другу.\nИтого, summary геометрической задачи:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#геометрическая-интерпретация-задачи-кластеризации",
    "href": "l13.html#геометрическая-интерпретация-задачи-кластеризации",
    "title": "12  L13 // Кластерный анализ",
    "section": "",
    "text": "компьютер умеет работать только с числами\nупорядоченое множество объектов одного типа есть вектор\nкаждый вектор мы [когда-то давно] договаривались начинать из начала координат, а значит, может описать его только координатами конца\n\n\n\nстолбцы — это переменные, или характеристики объектов\nстроки — это сами объекты.\n\n\n\n\n\n\nкаждый из \\(n\\) рассматриваемых объектов — это точка в некотором \\(p\\)-мерном признаковом пространстве;\nпохожие объекты будут располагаться «близко» друг с другу;\nразличающиеся объекты будут располагаться «далеко» друг от друга;\nскопления точек — это искомые кластеры.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#проблема-кластеризации",
    "href": "l13.html#проблема-кластеризации",
    "title": "12  L13 // Кластерный анализ",
    "section": "12.2 Проблема кластеризации",
    "text": "12.2 Проблема кластеризации\nПосмотрим на простейший вариант — двухмерное признаковое пространство. Пусть у нас есть некоторые два признака, которые будут задавать два соответствующих измерения, и некоторое количество точек, которые располагаются примерно так:\n\n\n\n\n\nСколько здесь кластеров? Кто-то скажет, что их три:\n\n\n\n\n\nКто-то скажет, что их четыре:\n\n\n\n\n\nКто-то скажет, что их всё же три, но выглядят они по-другому:\n\n\n\n\n\nЧто мы здесь наблюдаем? Проблему. Разные методы кластеризации могут давать разные результаты. Какой из них верный? Неясно… так как истинная группировка данных нам неизвестна. Но мы будем пытаться как-то выживать в ситуаций такой неопределённости.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#расстояние-между-объектами",
    "href": "l13.html#расстояние-между-объектами",
    "title": "12  L13 // Кластерный анализ",
    "section": "12.3 Расстояние между объектами",
    "text": "12.3 Расстояние между объектами\nОбратим внимание на следующий важный момент. Мы оперируем терминами «близко» и «далеко» — но как мы определаем расстояние между объектами? Рассмотрим самые популярные и важные для нас варианты.\n\n12.3.1 Евклидово расстояние\nС одним из них мы знакомы ещё со школы — это евклидово расстояние между точками. По смыслу это длина отрезка, соединяющего две точки. Оно определяется как корень из суммы квадратов покоординатных разностей. Пусть у нас есть две точки — \\((x_1, x_2, \\dots, x_p)\\) и \\((y_1, y_2, \\dots, y_p)\\). Евклидово расстояние будет определяться по формуле:\n\\[\nd_{\\text{Eucl},XY} = \\sqrt{\\sum_{j=1}^p (x_j - y_j)^2}\n\\]\nИногда также используется квадрат евклидова расстояния1.\n\n\n12.3.2 Манхэттеновское расстояние\nОно же блок-расстояние, или расстояние таксиста, или Майкопское расстояние.\nСлавный российский город Майкоп — а именно его улицы — устроен так:\n\n\n\n\n\nКакое расстояние проедет таксист из точки \\(A\\) в точку \\(B\\)?\n\n\n\n\n\nПравильно, вот такое2:\n\n\n\n\n\nСхожая логика может быть использована и при расчёте расстояния между точками3:\n\n\n\n\n\nМатематически это будет определено так:\n\\[\nd_{\\text{Manh},XY} = \\sum^p_{j=1} |x_j - y_j|\n\\]\n\n\n12.3.3 Евклид vs Манхэттен\nКогда какое расстояние выбирать? Здесь два важных момента.\nПервый — математический. Как и в случае с дисперсией, мы возводим покоординатные разности в квадрат. Если переменные измерены в различных единицах, то вклад одной из них в суммарное расстояние может быть значительно выше, чем других. По этой причине необходимо принять решение: является ли большая разница значений по одной из переменных достаточным основанием для отнесения наблюдений к различным кластерам? Если да, то можно использовать такое расстояние, если нет, то либо необходима стандартизация переменных, либо использование расстояния Манхэттен.\nВторой — измерительный. Если переменные, по которым вы кластеризуете наблюдения, непрерывные, то можно использовать евклидово расстояние. Если переменные дискретные, то более логичным вариантом будет манхэттеновское расстояние.\n\n\n12.3.4 Другие виды расстояний\nОтметим, что есть и другие виды расстояний, когда мы работает не с числовыми объектами. Например, мы можем пытаться кластеризовать слова — задача непростая, но её можно пытаться решить. Например, с помощью расстояний Хэмминга или Левенштейна. Для более специфичных объектов могут понадобиться и более изощрённые метрики расстояний. Да и вообще «никакое время, потраченное на раздумья, какое расстояние выбрать, не будет потрачено зря»4.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#проблема-операционализации-расстояния",
    "href": "l13.html#проблема-операционализации-расстояния",
    "title": "12  L13 // Кластерный анализ",
    "section": "12.4 Проблема операционализации расстояния",
    "text": "12.4 Проблема операционализации расстояния\nНо вообще нам надо здесь поговорить ещё вот о чём: как вообще мы определяем, что есть расстояние между объектами? То есть как мы его операционализируем?\nНапример, мы хотим кластеризовать наших испытуемых на «эффективных решателей задачи» и «неэффективных решателей задачи». По каким параметрам мы это будем делать? Как вариант — время решения и число ошибок в ходе решения. А как мы будем замерять эти переменные? Первую, видимо, в непрерывной шкале, вторую — в дискретной. Далее будем решать вопрос о выборе конкретной метрики расстояния.\nХорошо, а как нам кластеризовать менеджеров на «хороших», «плохих» и «средненьких продажников»? Можем использовать разные подходы: оценку 360, показатели KPI и т. д.\nА как нам определять расстояние между сайтами? По каким показателям? Здесь вариантов ещё больше и всё зависит от конкретной аналитической задачи.\nЭто всё о чем? О том, что операционализировать расстояние не так-то просто и для разных задач расстояние между одними и теми же объектами может быть операционализировано по-разному.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#субъективность-кластерного-анализа",
    "href": "l13.html#субъективность-кластерного-анализа",
    "title": "12  L13 // Кластерный анализ",
    "section": "12.5 Субъективность кластерного анализа",
    "text": "12.5 Субъективность кластерного анализа\nМы плавно подъехали к ещё одной проблеме-особенности. Какова роль аналитика в кластерном анализе? Достаточно велика:\n\nотбор переменных для анализа\nкакие переменные включать в анализ? все? или необходимо проводить отбор?\nвозможно наличие переменных, которые будут хорошо работать с точки зрения поиска схожих объектов, но это не то сходство, которое мы ищем\nодни переменные могут быть косвенно заменены другими (уровень дохода — профессия, образование, стаж работы)\n«ковариаты» могут быть важны при формировании кластеров (число учащихся и учителей в школах)\nправильный выбор переменных крайне важен\nкритерием при отборе переменных выступает ясность интерпретации полученного результата и «интуиция исследователя»\nметод стандартизации\n\nкачество кластеризации может зависит от выбранного метода стандартизации\n\nвыбор метрики для расстояния между объектами\nвыбор метрики для расстояния между кластерами\n\nесли кластеры выражены, то метрика не важна\nесли появляются кластеры сложной формы (например, ленточные), то всё становится сложнее\n\n[иногда] определение числа кластеров\nинтерпретация результатов\n\nрезультаты кластерного анализа нуждаются в интерпретации (если он не решает чисто техническую задачу сокращения размерности данных)\nлучший результат кластеризации — это тот, который вы смогли понять и проинтерпретировать",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#расстояние-между-кластерами",
    "href": "l13.html#расстояние-между-кластерами",
    "title": "12  L13 // Кластерный анализ",
    "section": "12.6 Расстояние между кластерами",
    "text": "12.6 Расстояние между кластерами\nХорошо, мы поговорили о том, как считать расстояние между объектами, но нам надо понять, насколько (не)похожи получившиеся группы объектов. Для этого придется считать расстояние между кластерами. Варинатов, как обычно, масса.\n\n12.6.1 Среднее невзвешенное расстояние\nСреднее невзвешенное рассрояние (Average linkage clustering) определяется так:\n\nнаходим расстояния между всеми парами объектов двух кластеров\nусредняем их\n\n\n\n\n\n\n\n\n12.6.2 Центроидный метод\nРанее был самым популярным из-за вычислительной простоты. Определяется расстояние между центрами тяжести двух кластеров. В настоящее время используется крайне мало.\n\n\n\n\n\n\n\n12.6.3 Метод дальнего соседа\nРасстояние между кластерами определяется как расстояние между наиболее удалёнными объектами кластеров.\n\n\n\n\n\n\n\n12.6.4 Метод ближайшего соседа\nРасстояние между кластерами определяется как расстояние между наиболее близкими объектами кластеров. Хорошо работает с ленточными кластерами.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#иерархическая-кластеризация",
    "href": "l13.html#иерархическая-кластеризация",
    "title": "12  L13 // Кластерный анализ",
    "section": "12.7 Иерархическая кластеризация",
    "text": "12.7 Иерархическая кластеризация\n\n12.7.1 Алгоритм иерархического кластерного анализа\n\nКаждый объект объявляется кластером — из \\(n\\) наблюдений получается \\(n\\) кластеров.\nВыбираются два ближайших кластера — они объединяются.\nВыбираются два ближайших кластера — они объединяются [2].\nВыбираются два ближайших кластера — они объединяются [3].\nТак происходит до тех пор, пока не остается два кластера.\nОставшиеся два кластера являются ближайшими друг с другу — поэтому объединяются в один.\n\nЗвучит, как какой-то сюр — начали с \\(n\\) кластеров по одному объекту, закончили один кластером, содержащим все объекты… Да, в таком исполнении, действительно, странная процедура. Однако если мы на каком-то этапе её прервём, то получим желаемый результат.\nНа каком этапе стоит остановиться? Когда расстояния между объединяемыми кластерами становится большим, так как большое расстояние говорит о том, что мы объединяем непохожие объекты.\n\n\n12.7.2 Дендрограмма\nВ иерархическом кластерном анализе есть удобный инструмент для определения момента, когда стоит остановиться в объединении кластеров. Он называется дендрограмма. По своей сути, это визуализация алгоритма иерархического кластерного анализа.\nПринцип построения дендрограммы следующий:\n\nНа прямой располагаются все наблюдения как отдельные кластеры.\nКаждому кластеру соответствует вертикальная линия.\nКаждому объединению кластеров соответствует горизонтальная линия.\nВысота, на которой кластеры соединяются, отражает расстояние между кластерами.\n\nРазберемся с этим на примере.\n\n\n\n\n\n\nУ нас есть пять наблюдений. Первоначально мы объявляем все их кластерами — получаем пять кластеров. - - Договоримся, что расстояние между наблюдениями у нас манхэттеновское, а расстояние между кластерами — среднее невзвешенное, ибо так проще считать.\n[Итерация 1]\n\nДалее ищем два ближайших — это кластеры 1 и 2.\nОбъединяем их.\nОтображаем это на дендрограмме — соединяем линии 1 и 2 между собой на высоте 1, так как расстояние между объединяемым кластерами равно единице.\nТеперь у нас четыре кластера.\n\n[Итерация 2]\n\nСнова ищем два ближайших кластера — это 4 и 5.\nОбъединяем их на высоте 2, так как расстояние между ними равно двум.\nОстаётся три кластера.\n\n[Итерация 3]\n\nСнова ищем два ближайщих кластера — на этот раз это 3 и 4-5.\nОбъединяем их на высоте 3, так как расстояние между ними равно трём.\n\n[Итерация 4]\n\nОстаётся только два кластера — соединяем их на каком-то большом расстоянии.\n\n\n\n\n\n\n\nПри анализе дендрограммя мы ищем скачок расстояний. Он обозначает момент, когда мы перешли к объединению непохожих (далёких друг от друга кластеров). Собственно, это и есть тот момент, когда необхожимо было прервать алгоритм и оставить те кластеры, которые образовались на текущий момент.\n\n\n12.7.3 Каменистая осыпь\nЕщё один способ определить число кластеров — это график «каменистая осыпь».\nВ данном случае по оси \\(x\\) располагаются шаги объединения, по оси \\(y\\) — расстояние между кластерами в момент объединения. Как именно нам помогает такой график?\n\n\n\n\n\nМы видим, что сначала расстояние между объединяемыми кластерами растёт медленно, а затем происходит излом линнии, и после него расстояние начинает расти быстро. Это является указанием, что на шаге, где происходит излом линии, необходимо прервать процедуру объединения.\n\n\n12.7.4 Когда кластеризации нет?\nКак вы понимаете, паттерны дендрограммы и каменистой осыпи могут быть крайне разнообразны в зависимости от того, что есть в данных. Однако главный момент, который нам говорит о том, что кластеризаци нет — это отсутствие скачка расстояний на дендрограмме и/или отсутствие излома линии на графике «каменистая осыпь».\n\n\n12.7.5 Чем плох иерархический кластерный анализ?\nУ иерархического кластерного анализа практически нет недостатков, за исключением одного очень важного технического — он требует, чтобы в оперативной памяти хранилась матрица попарных расстояний. Если у нас порядка ста объектов, то проблем никаких, а вот если объектов 100 000, уже возникают трудности. Невозможность работать с очень большими датасетами — основная проблема этого вида кластерного анализа.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#метод-k-средних-k-means",
    "href": "l13.html#метод-k-средних-k-means",
    "title": "12  L13 // Кластерный анализ",
    "section": "12.8 Метод k-средних (k-means)",
    "text": "12.8 Метод k-средних (k-means)\n\n12.8.1 Алгоритм метода k-средних\nПроцедура кластерного анализа этим методом значительно отличается.\n\nЗаранее определяется число кластеров \\(k\\). Хотя вообще-то это невозможно, однако уже найдены способы, чтобы обойти это ограничение.\nДля анализа выбирается \\(k\\) точек — центры кластеров.\nОбъект приписывается к тому кластеру, чей центр ближайший.\nЦентр кластера — центр тяжести объектов кластера.\n\n\nцентр тяжести множества точек с координатами \\((x_{i1}, x_{i2}, \\dots, x_{ip})\\) — это точка с координатами \\((\\bar x_1, \\bar x_2, \\dots, \\bar x_p)\\).\n\n\nПовторяем поочерёдно пункты 3 и 4 до тех пор, пока центры кластеров не перестанут двигаться.\n\nВизуализацию можно посмотреть тут.\n\n\n\n12.8.2 Ограничения k-means\n\nНеобходимо заранее определить число кластеров\nИспользуется только евклидово растояние\n\nхотя этот недостаток исправляется в других модификациях метода\n\nРезультат зависит от начальных центров кластеров\n\n\n\n12.8.3 Начальное положение кластеров\nЕсли «бросать» центроиды совсем случайно, то это может привести к тому, что некоторые из них буду, например, слишком далеко от скопления точек — в результате работы алгоритма образуются пустые кластеры. Это нехорошо, поэтому есть два наиболее популярных подхода.\n\nForgy — Случайным образом выбираются \\(k\\) наблюдений. Они объявляются начальными центрами кластеров.\nСлучайное разбиение (Random Partition) — Каждое наблюдение случайным образом приписывается к одному из кластеров. Находятся центры тяжести кластеров. Они объявляются начальными центрами кластеров.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#метрики-качества-кластеризации",
    "href": "l13.html#метрики-качества-кластеризации",
    "title": "12  L13 // Кластерный анализ",
    "section": "12.9 Метрики качества кластеризации",
    "text": "12.9 Метрики качества кластеризации\nВообще оценка качества кластеризации — задача крайне сложная и в строгом математическом смысле невыполнимая. Однако всякие разные метрики, которые позволяют приблизиться к такой оценки всё же были придуманы. Они делятся на внешние и внутренние. Мы рассмотрим наиболее простые.\n\n12.9.1 Внешние метрики\nВнешние метрики используют дополнитльную информацию о кластеризуемом множестве объектов. Например, распределение объектов по кластерам. То есть, чтобы посчитать метрику, мы должны знать, как данные распределяются на кластеры перед тем, как будем проводить кластерный анализ.\nНо зачем нам проводить кластерный анализ, если нам уже известны кластеры? Ведь это тогда не кластеры, а классы! И мы можем строить классификатор. Так-то оно, конечно, так — и нам, аналитикам, эти метрики не очень интересны. Для разработчиков алгоритмов же они могут быть очень полезны.\nМы не будем на них останавливаться. Если хочется почитать и вникнуть, то вот.\n\n\n12.9.2 Внутренние метрики\nЭти метрики не используют внешний информации о датасете, а опираются только на результаты кластеризации.\n\n12.9.2.1 Компактность кластеров (cluster cohesion)\nПомним, что мы хотим собрать похожие наблюдения вместе, а похожие — это те, которые располагаются близко друг к другу. Соответственно, разделение на кластеры тем лучше, чем ближе объекты кластера находятся к его центру. Поэтому необхожимо минимизировать внутрикластерное расстояние:\n\\[\n\\text{WSS} = \\sum_{j=1}^k \\sum^{|C_j|}_{i=1} (x_{ij} − \\bar x_j)^2,\n\\]\nгде \\(k\\) — число кластеров, \\(|C_j|\\) — количество объектов в данном кластере.\nЭто самая популярная метрика качества кластеризации.\n\n\n12.9.2.2 Отделимость кластеров (cluster separation)\nПомним, что мы хотим собрать в разные кластеры непохожие друг на друга наблюдения — это те, которые располагаются далеко друг от друга. Соответственно, чем дальше находятся друг от друга центры кластеров, тем лучше. Поэтому необхожимо максимизировать межкластерное расстояние:\n\\[\n\\text{BSS} = n \\cdot \\sum_{j=1}^k (\\bar x_j - \\bar x)^2,\n\\]\nгде \\(k\\) — число кластеров.\nО других внутренних метриках кластеризации можно посмотреть тут.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#нечеткая-кластеризация",
    "href": "l13.html#нечеткая-кластеризация",
    "title": "12  L13 // Кластерный анализ",
    "section": "12.10 Нечеткая кластеризация",
    "text": "12.10 Нечеткая кластеризация\nДва рассмотренных выше алгоритма кластеризации позволяют приписать каждому наблюдению некоторый единственный кластер. Такие подходы относятся к чёткой кластеризации. Но что если мы хотим получить какое-то более общее решение? И, скажем, оценить степень принадлежности наблюдений к каждому кластеру?\nВозникает справедливый вопрос — зачем? если чёткая кластеризации даёт более однозначные результаты? Решение, возвращающее степень принадлежности наблюдения к определенному кластеру, даёт нам больше возможностей для интерпретации результатов анализа, а также позволяет получить более подробную картину ситуации, которая происходит где-то «между кластерами», или когда явная кластеризация не выявляется.\nДля решения задачи нечёткой кластеризации (fuzzy clustering) используется метод C-средних (C-means), который является усовершенствованием метода k-средних. Он позволяет разбить наблюдения на заданное число \\(k\\) нечетких кластеров. Нечеткость выражается как раз в том, что в ходе алгоритма рассчитывается степень принадлежности (membership value) наблюдения к каждому из кластеров.\nРассмотрим шаги алгоритма для простейшего случая. Пусть исходно имеются четыре наблюдения в двумерном признаковом пространстве с координатами \\((1,3), (2,5), (6,8), (7,9)\\). Эти наблюдения мы будем разбивать на два кластера.\n\nКаждому наблюдению присваивается случайное число, задающее его принадлежность к кластеру.\n\n\n\n\nКластер\n\\((1,3)\\)\n\\((2,5)\\)\n\\((4,8)\\)\n\\((7,9)\\)\n\n\n\n\n1\n0.8\n0.7\n0.2\n0.1\n\n\n2\n0.2\n0.3\n0.8\n0.9\n\n\n\n\nВычисляются центры (центроиды) кластеров.\n\n\\[\nV_{ij} = \\frac{\\sum_{k=1}^n \\gamma_{ik}^m x^{(j)}_k}{\\sum_{k=1}^n \\gamma_{ik}^m},\n\\]\nгде \\(\\gamma\\) — membership value, \\(m\\) — fuzziness parameter (степень нечеткости кластеров, стандартное значение — от 1.2 до 2), \\(x^{(j)}_k\\) — координата наблюдения, \\(i\\) — номер кластера, \\(j\\) — номера координаты.\nДля представленной таблицы координаты центроидов будут таковы:\n\\[\n\\begin{split}\n\\mathbf{V}_1 &= (V_{11}, V_{12}) = (1.568, 4.051) \\\\\n\\mathbf{V}_1 &= (V_{21}, V_{22}) = (5.350, 8.215)\n\\end{split}\n\\]\n\nРасчитываются расстояния от каждого наблюдения до центров (центроидов) кластеров.\n\nЕсли использовать евклидово расстояние, то для рассматриваемого примера они будут такими:\n\n\n\n\n\n\n\n\n\n\nКластер\n\\((1,3)\\)\n\\((2,5)\\)\n\\((4,8)\\)\n\\((7,9)\\)\n\n\n\n\n1\n\\(d_{11} = 1.2\\)\n\\(d_{21} = 1.04\\)\n\\(d_{31} = 4.63\\)\n\\(d_{41} = 7.34\\)\n\n\n2\n\\(d_{12} = 6.79\\)\n\\(d_{22} = 4.64\\)\n\\(d_{32} = 1.36\\)\n\\(d_{42} = 1.82\\)\n\n\n\n\nОбновляются значения membership values для наблюдений по следующей формуле:\n\n\\[\n\\gamma_{pi} = \\Bigg( \\sum_{j=1}^J \\Big( \\frac{d_{pi}^2}{d_{pj}^2} \\Big) ^{\\frac{1}{m-1}} \\Bigg)^{-1},\n\\]\nгде \\(i\\) — номер кластера, \\(p\\) — номер наблюдения, \\(d\\) — расстояние между наблюдением и центром кластера, \\(J\\) — количество кластеров, \\(m\\) — fuzziness parameter.\nВ рассматриваемом примере получатся такие значения:\n\n\n\nКластер\n\\((1,3)\\)\n\\((2,5)\\)\n\\((4,8)\\)\n\\((7,9)\\)\n\n\n\n\n1\n0.97\n0.95\n0.08\n0.06\n\n\n2\n0.03\n0.05\n0.92\n0.94\n\n\n\n\nПовторяются шаги 2–4 до тех пор, пока значения membership values не перестанут меняться.\n\nПо полученой таблице и определяется структура данных — или её отсутствие.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l13.html#footnotes",
    "href": "l13.html#footnotes",
    "title": "12  L13 // Кластерный анализ",
    "section": "",
    "text": "Хотя математически эта метрика не является расстоянием, так как может нарушаться неравенство треугольника. Однако для задач кластерного анализа это не имеет большого значения.↩︎\nЗаметьте, что в количестве кварталов все расстояния равны.↩︎\nЗаметьте, что длина всех траекторий также будет одинакова.↩︎\nВадим Аббакумов, лекция в Computer Science Center↩︎",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>L13 // Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "l14.html",
    "href": "l14.html",
    "title": "13  L14 // Анализ главных компонент и эксплораторный факторный анализ",
    "section": "",
    "text": "13.1 Проклятие размености (curse of dimensionality)\nС данными могут случиться две проблемы: либо их слишком мало, либо их слишком много. С первой проблемой многие так или иначе сталкивались — во-первых, не достигается требуемая статистическая мощность анализа (мало наблюдений), во-вторых, упускаем какие-либо закономерности (мало переменных). А в чем может быть проблема, если данных много?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L14 // Анализ главных компонент и эксплораторный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l14.html#проклятие-размености-curse-of-dimensionality",
    "href": "l14.html#проклятие-размености-curse-of-dimensionality",
    "title": "13  L14 // Анализ главных компонент и эксплораторный факторный анализ",
    "section": "",
    "text": "13.1.1 Геометрическая вероятность\nВозьмем квадрат, в который вписан круг:\n\n\n\n\n\n\n\n\n\nБудем бросать в этот квадрат точки случайным образом:\n\n\n\n\n\n\n\n\n\nВопрос: какова вероятность, что случайно брошенная точка попадет в круг (событие \\(A\\))?\nМы можем воспользоваться статистическим подходом к вероятности, набросать побольше точек и посчитать, сколько из них попало в пределы круга:\n\n\n\n\n\n\n\n\n\nИз визуализации видно, что в конечном итоге при очень большом количестве бросаний точек они заполнят всю площадь квадрата, а значит, и всю площадь круга, поэтому вероятность попадания случайно прошенной точки в круг равняется отношению площади круга к площади квадрата, то есть:\n\\[\n\\mathbb{P}(A) = \\lim_{N \\rightarrow \\infty} \\frac{n}{N} = \\frac{S_\\text{circle}}{S_\\text{square}}\n\\]\nМожно расписать точнее, если вспомнить геометрические формулы:\n\\[\n\\mathbb{P}(A) = \\frac{S_\\text{circle}}{S_\\text{square}} = \\frac{\\pi r^2}{a^2} = \\frac{\\pi \\big(\\frac{1}{2}a\\big)^2}{a^2} = \\frac{1}{4}\\pi \\approx 0.785\n\\]\nОкей, это для \\(\\mathbb{R}^2\\), то есть для пространства размерности два (оно же — плоскость). Теперь рассмотрим ситуацию для \\(\\mathbb{R}^3\\), то есть для пространства размерности три.\n\\[\n\\mathbb{P}(A) = \\frac{V_\\text{ball}}{V_\\text{cube}} = \\frac{\\frac{4}{3}\\pi r^3}{a^3} = \\frac{\\frac{4}{3}\\pi \\big(\\frac{1}{2}a\\big)^3}{a^3} \\approx 0.523\n\\]\nВ общем случае, для \\(\\mathbb{R}^k\\) объем шара равен:\n\\[\n\\begin{split}\nk = 2n &, V = \\frac{\\pi^2}{n!}r^{2n} \\\\\nk = 2n+1 &, V = \\frac{2 \\cdot (2\\pi)^n}{(2n+1)!!} r^{2n+1}\n\\end{split}\n\\]\nМожно аналитически доказать, что при \\(k \\to \\infty : V \\to 0\\).\n\nТо есть, говоря более осязаемо, места вне шара становится больше. Набирая переменных, мы делаем расстояния между точками больше, из-за объем выборки должен расти экспоненциально, чтобы сохранялась адекватная точность оценки параметров. Это и называется проклятием размерности.\nЧто делать? Снижать размерность.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L14 // Анализ главных компонент и эксплораторный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l14.html#факторный-анализ-vs-анализ-главных-компонент",
    "href": "l14.html#факторный-анализ-vs-анализ-главных-компонент",
    "title": "13  L14 // Анализ главных компонент и эксплораторный факторный анализ",
    "section": "13.2 Факторный анализ vs анализ главных компонент",
    "text": "13.2 Факторный анализ vs анализ главных компонент\nТак сложилось, что под термином «факторный анализ» часто смешиваются две вещи: анализ главных компонент и собственно факторный анализ. Кроме того, собственно факторный анализ также подразделяется на эксплораторный — когда мы ищем факторы — и конфирматорный — когда у нас есть гипотеза о структуре факторов, и нам необходимо ее подтвердить.\nАнализ главных компонент и факторный анализ различаются в своей математической модели, своей изначальной идеологии, но дают сходные результаты и решают сходные задачи.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L14 // Анализ главных компонент и эксплораторный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l14.html#задачи-факторного-анализа-и-анализа-главных-компонент",
    "href": "l14.html#задачи-факторного-анализа-и-анализа-главных-компонент",
    "title": "13  L14 // Анализ главных компонент и эксплораторный факторный анализ",
    "section": "13.3 Задачи факторного анализа и анализа главных компонент",
    "text": "13.3 Задачи факторного анализа и анализа главных компонент\n\nСокращение числа переменных\nИзмерение неизмеримого (построение новых обобзеных показателей)\nНаглядное представление многомерных наблюдений\nОписание структуры взаимных связей между переменными\nПреодоление мультиколлинеарности (в регрессионном анализе)\nЗаполнение пропущенных значений (при работе с разряженными матрицами)\n\nи т.д.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L14 // Анализ главных компонент и эксплораторный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l14.html#анализ-главных-компонент-principal-component-analysis",
    "href": "l14.html#анализ-главных-компонент-principal-component-analysis",
    "title": "13  L14 // Анализ главных компонент и эксплораторный факторный анализ",
    "section": "13.4 Анализ главных компонент (Principal Component Analysis)",
    "text": "13.4 Анализ главных компонент (Principal Component Analysis)\n\n13.4.1 Математическая модель анализа главных компонент\nРассмотрим случайный вектор (матрицу) \\((\\mathbf{X}_1, \\mathbf{X}_2, \\dots, \\mathbf{X}_k)\\) (\\(\\mathbf{X}_i\\) — некоторый столбец [числовых] данных). Мы хотим найти такую линейную комбинацию наших данных, у которой дисперсия максимальна. При это мы хотим домножать матрицу данных на вектор коэффициентов единичной длины, чтобы искусственно не раздувать дисперсию.\nФормально,\n\\[\n\\begin{split}\n&\\mathbf{Y}_1 = a_{11} \\mathbf{X}_1 + a_{12} \\mathbf{X}_2 + \\dots + a_{1k} \\mathbf{X}_k \\\\\n&\\text{var}(\\mathbf{Y}_1) \\to \\max \\\\\n&\\mathbf{a}_1 \\mathbf{a}_1^\\top = 1, \\, \\mathbf{a}_1 =\n\\begin{pmatrix} a_{11} & a_{12} \\dots a_{1k} \\end{pmatrix}\n\\end{split}\n\\]\nДалее, находим вторую линейную комбинацию наших данных с максимальной дисперсией, независимую (некоррелированную) с первой. Требования к вектору коэффициентов оставляем те же.\n\\[\n\\begin{split}\n&\\mathbf{Y}_2 = a_{21} \\mathbf{X}_1 + a_{22} \\mathbf{X}_2 + \\dots + a_{2k} \\mathbf{X}_k \\\\\n&\\text{var}(\\mathbf{Y}_2) \\to \\max \\\\\n&\\mathbf{a}_2 \\mathbf{a}_2^\\top = 1, \\, \\mathbf{a}_2 =\n\\begin{pmatrix} a_{21} & a_{22} \\dots a_{2k} \\end{pmatrix} \\\\\n&\\text{cor}(\\mathbf{Y}_2, \\mathbf{Y}_1) = 0\n\\end{split}\n\\]\nДалее, находим третью линейную комбинацию, аналогичную предыдущим и некоррелированную с ними.\n\\[\n\\begin{split}\n&\\mathbf{Y}_3 = a_{31} \\mathbf{X}_1 + a_{32} \\mathbf{X}_2 + \\dots + a_{3k} \\mathbf{X}_k \\\\\n&\\text{var}(\\mathbf{Y}_3) \\to \\max \\\\\n&\\mathbf{a}_3 \\mathbf{a}_3^\\top = 1, \\, \\mathbf{a}_3 =\n\\begin{pmatrix} a_{31} & a_{32} \\dots a_{3k} \\end{pmatrix} \\\\\n&\\text{cor}(\\mathbf{Y}_3, \\mathbf{Y}_1) = 0, \\, \\text{cor}(\\mathbf{Y}_2, \\mathbf{Y}_1) = 0\n\\end{split}\n\\]\nТак как у нас всего \\(k\\) переменных, то мы можем найти \\(k\\) таких линейных комбинаций.\nВ общем виде, они будут выглядеть следующим образом:\n\\[\n\\begin{split}\n&\\mathbf{Y}_k = a_{k1} \\mathbf{X}_1 + a_{k2} \\mathbf{X}_2 + \\dots + a_{kk} \\mathbf{X}_k \\\\\n&\\text{var}(\\mathbf{Y}_k) \\to \\max \\\\\n&\\mathbf{a}_k \\mathbf{a}_k^\\top = 1, \\, \\mathbf{a}_k =\n\\begin{pmatrix} a_{k1} & a_{k2} \\dots a_{kk} \\end{pmatrix} \\\\\n&\\text{cor}(\\mathbf{Y}_k, \\mathbf{Y}_i) = 0, \\, i = 1, 2, \\ldots, k\n\\end{split}\n\\]\nПолученные \\(Y_i\\) и будут искомые главные компоненты (principal components) — наши новые оси, с помощью которых мы будем смотреть на данные и описывать их.\nЕсли мы в качестве информативности компоненты используем её дисперсию, то можно сказать, что мы ищем наиболее информативные линейные комбинации. То есть, мы ищем некоторую «правильную систему координат».\n\n\n13.4.2 Поиск главных компонент геометрически\nЧтобы лучше понять, что все это значит, посмотрим на картинку.\n\n\n\n\n\nРассмотрим самый простой случай. Пусть у нас есть две переменные (\\(\\mathbf{X}_1\\), \\(\\mathbf{X}_2\\)) и мы подбираем их линейную комбинацию с наибольшей дисперсией. Графически это то же самое, что провести прямую \\(Y_1\\), то есть такую прямую, по которой наши данные будут максимально «растянуты».\n\n\n\n\n\nДалее ищем вторую линейную комбинацию. Прицнип тот же: необходима прямая, по которой данные макисмально растянуты. Однако эта прямая должна быть перпендикулярная \\(Y_1\\), чтобы выполнялось условие \\(\\text{cor}(Y_1, Y_2) = 0\\) (в двумерном пространстве отсутствие корреляции эквивалентно ортогональности, то есть пересечению прямых под прямым углом).\n\n\n\n\n\nПолучаем что-то такое. Красные оси и есть наши главные компоненты. Их столько же, сколько изначально было переменных. Теперь мы можем рассматривать наши наблюдения относительно новой системы координат \\(Y_1 O^′ Y_2\\).\n\n\n13.4.3 Сокращение размерности признакового пространства\nИтак, мы нашли новые координаты, но что-то переменных у нас меньше не стало — мы нашли \\(k\\) новых переменных взамен \\(k\\) старых. Мы же хотели, чтобы осей, то есть измерений признакового пространства, стало меньше.\nТеперь нам пригодится следующая идея лаваша.\n\n\n\n\n\nЛаваш, это, безусловно, трехмерный объект, однако практически вся его «изменчивость» сосредоточена по двум измерениям:\n\n\n\n\n\nТолщина лаваша практически «неинформативна» для нас1, поэтому мы можем игнорировать его изменчивость по третьей оси.\nТаким образом, чтобы снизить размерность, мы волей, данной нам нами, выбрасываем малоинформативные главные компоненты.\n\n\n13.4.4 Информативность компонент\nЧто же такое малоинформативные компоненты? И как нам решить, какие из компонент конкретного решения малоинформативны?\nВновь рассмотрим картинку. В данном случае мы имеем дело с ситуацией, когда все наблюдения расположены более-менее вокруг одной прямой — первой главной компоненты \\(\\text{PC1}\\):\n\n\n\n\n\nТаким образом, изменчивостью по второй главной компоненте \\(\\text{PC2}\\) можно пренебречь.\nКаждая из главных компонент объясняет какую-то часть дисперсии данных, значит мы можем воспользоваться следующей идеей: если новая переменная (главная компонента) объясняет меньше дисперсии, чем исходная переменная, то она не информативна. Дисперсия исходных переменных может быть какой угодно, поэтому перед проведением анализа переменные стандартизируют — их дисперсия становится равной единице. Это значит, что если дисперсия главной компоненты меньше единицы, то можно считать её неинформативной.\nКроме того, есть ещё один способ отбора главных компонент. Можно вычислить, какую долю дисперсии данных объясняет каждая компонента, и упорядочить компоненты по возрастанию доли объясняемой дисперсии. Затем, рассчитав кумулятивную долю объясняемой дисперсии, выяснить, какого количества главных компонент достаточно, чтобы объяснить не менее 80% дисперсии данных. Это некий конвенциональный порог, поэтому его можно изменять в зависимости от решаемой задачи.\nРезультаты обычно представляются в виде таблицы:\n\n\n\n\nPC1\nPC2\nPC3\n…\nPCk\n\n\n\n\nStandard Deviation\n2.214\n1.501\n0.622\n…\n0.003\n\n\nProportion of Variance\n0.596\n0.327\n0.059\n…\n0.000\n\n\nCumulative Proportion\n0.596\n0.923\n0.982\n…\n1.000\n\n\n\n\n\n13.4.5 Интерпретация главных компонент\nВ принципе, анализ главных компонент может решать чисто техническую задачу сокращения размерности признакового пространства, и нас может и не интересовать содержательная интерпретация новых переменных.\nОднако решая аналитические задачи, мы часто заинтересованы в том, чтобы хотя бы как-то понимать, что же мы в итоге получили. Для этого воспользуемся матрицой нагрузок (matrix of variable loadings).\n\n\n\n\nPC1\nPC2\nPC3\n…\nPCk\n\n\n\n\nX1\n0.06\n−0.62\n0.42\n…\n−0.42\n\n\nX2\n0.38\n−0.27\n−0.74\n…\n−0.27\n\n\nX3\n0.44\n−0.24\n0.19\n…\n−0.38\n\n\n…\n…\n…\n…\n…\n−0.05\n\n\nXk\n0.24\n0.57\n−0.11\n…\n−0.77\n\n\n\nНагрузки — это не что иное как коэффициенты \\(a_{ij}\\) из математической модели. Чем они больше, тем сильнее исходные переменные связаны с главными компонентами. Обобщив связи, можно попробовать придумать адеватную исследовательской области интерпретацию.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L14 // Анализ главных компонент и эксплораторный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l14.html#эксплораторный-факторный-анализ-exploratory-factor-analysis",
    "href": "l14.html#эксплораторный-факторный-анализ-exploratory-factor-analysis",
    "title": "13  L14 // Анализ главных компонент и эксплораторный факторный анализ",
    "section": "13.5 Эксплораторный факторный анализ (Exploratory Factor Analysis)",
    "text": "13.5 Эксплораторный факторный анализ (Exploratory Factor Analysis)\n\n13.5.1 Математическая модель эксплораторного факторного анализа\nУ нас есть все та же матрица наблюдений \\(X^\\top = \\pmatrix{X_1 & X_2 & \\ldots X_k}\\), только мы ее транспонировали для будущего удобства. Мы предполагаем, что под нашими наблюдениями спрятаны некие факторы в количестве \\(p\\) штук, \\(p &lt; k\\). Их мы также можем оформить в матрицу \\(F^\\top = \\pmatrix{F_1 & F_2 & \\ldots & F_p}\\). Эти факторы объясняют имеющиеся переменные. Делают это они следующим образом:\n\\[\n\\begin{split}\n&\\mathbf{X}_i = a_{i1} \\mathbf{F}_1 + a_{i2} \\mathbf{F}_2 + \\dots + a_{ip} \\mathbf{F}_p + \\mathbf{U}_i, \\, i = 1, 2, \\ldots, k \\\\\n& \\mathbf{X} = \\mathbf{A} \\mathbf{F} + \\mathbf{U}, \\\\\n& \\mathbf{A} = (a_{ij}), \\, i = 1,2,\\ldots,k, \\, j = 1,2,\\dots,p \\\\\n& \\mathbf{U}^\\top = \\pmatrix{\\mathbf{U}_1 & \\mathbf{U}_2 & \\dots & \\mathbf{U}_k}\n\\end{split}\n\\]\nЗдесь \\(\\mathbf{U}\\) — то, что не удалось объяснить факторами (остатки, уникальность, uniqueness).\nТакже выдвигаем ряд дополнительных предположения для формализации требования и упрощения вычислений:\n\n\\(\\mathbb{E}\\mathbf{X} = 0\\).\n\nТак сделать можно, так как мы все равно будем стандартизировать переменные, и математическое ожидание обратиться в ноль.\n\n\\(\\text{cor}(\\mathbf{F}_j, \\mathbf{F}_t) = 0, \\, \\forall j \\forall t, \\, j \\neq t, j = 1,2,\\ldots,p, \\, t = 1,2,\\ldots,p\\).\n\nТо есть хотим, чтобы факторы были независимы (некоррелированы).\n\n\\(\\text{var}(\\mathbf{F}) = \\mathbf{I}\\).\n\nЕсли «истинная» дисперсия факторов будет отличаться от единицы, то разница уйдет в матрицу \\(\\mathbf{A}\\).\n\n\\(\\text{cor}(\\mathbf{U}_i, \\mathbf{U}_r) = 0, \\, \\text{cor}(\\mathbf{U}_i, \\mathbf{F}_j) = 0, \\, \\forall i \\forall r \\forall j, i \\neq r, \\, i = 1,2,\\ldots,k, \\, r = 1,2,\\ldots,k, \\, j = 1,2,\\ldots,p\\).\n\nНу, они же все-таки уникальности.\n\n\nЭлементы матрицы \\(\\mathbf{A}\\) называются факторными нагрузками (factor loadings). Элементы вектора \\(\\mathbf{U}\\) называются уникальными факторами (specific variates).\nТеперь об информативности. Аналогично PCA, возьмем в качестве меры информативности дисперсию. Мы хотим узнать, насколько хорошо факторы объясняют исходные переменные. Дисперсия переменных будет складываться из следующего:\n\\[\n\\text{var}\\mathbf{X}_i = \\sum_{j=1}^p a^2_{ij} + \\text{var}\\mathbf{U}_i\n\\]\nТо есть, чем больше уникальность (uniqueness) — часть дисперсии переменной, объясненной уникальными факторами — тем хуже наши факторы объясняют переменную. Что делать? Либо подобрать другую модель (изменить количество факторов), либо не брать такую переменную в факторный анализ…\nОднако в факторном анализе есть еще одна интересная деталь. Мы работаем с выборочными корреляциями. По этой причине мы не можем подобрать единственно верное решение задачи факторного анализа. Матрица факторных нагрузок определена с точностью до ортогонального преобразования (по-русски, вращения). То есть у нас есть множество решений, которые отличаются друг от друга поворотом, и мы можем выбирать самое «симпатишное».\n\nЗа такую неопределенность факторный анализа часто критикуют. Да и вообще, как вы могли заметить, в факторном анализе субъективность чуть ли не на каждом шагу.\n\n\n\n13.5.2 Вращение факторов (factor rotation)\nВращать вообще-то не обязательно. Если хочется, чтобы все было похоже на PCA, то можно ввести некоторое дополнительное условие и получить единственное решение.\nОднако вращение факторов позволяет получить иногда более интересные результаты. Самые используемые вращения таковы:\n\nvarimax — находит наиболее «контрастное» решение, еще более «изолируя» друг от друга факторы\nquartimax — минимизирует количество факторов, необходимых для объяснения каждой переменной\nequamax — количество переменных, сильно нагружающих фактор, и количество факторов для объяснения переменных минимальны\npromax — наклонное вращение, позволяющее коррелировать факторы\noblimin — косоугольное вращение, позволяющее коррелировать факторы",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L14 // Анализ главных компонент и эксплораторный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l14.html#footnotes",
    "href": "l14.html#footnotes",
    "title": "13  L14 // Анализ главных компонент и эксплораторный факторный анализ",
    "section": "",
    "text": "Хотя это, бесспорно, определяется видом лаваша — их популяция весьма разнообразна.↩︎",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>L14 // Анализ главных компонент и эксплораторный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l15.html",
    "href": "l15.html",
    "title": "14  L15 // Структурное моделирование и конфирматорный факторный анализ",
    "section": "",
    "text": "14.1 Крайне краткое введение в структурное моделирование\nВспомним, кто мы… Мы же с вами исследователи из области социальных и/или гуманитарных наук. А чем мы богаты? Теоретическими моделями, на которых мы основываем наши исследования! А какая теоретическая модель хороша? Та, которая получила эмпирическое подтверждение1!\nА как это?…\nМы научились искать факторы, скрытые за наблюдаемыми переменными — это уже большой шаг. Однако недостаточный. И главное ограничение PCA и EFA — это то, что мы ищем независимые факторы/главные компоненты, в то время как структура взаимосвязей между латентными переменными может быть гораздо сложнее. Кроме того, эти скрытые переменные могут предсказывать другие наблюдаемые переменные: например, уровень вовлеченности сотрудников [возможно] будет предсказывать выполнение KPI, что в свою очередь предсказывает заработную плату. То есть в данным случае мы движемся не от наблюдаемых переменных к латентным, а в обратном направлении. А ведь влияние одних латентных конструктов могут быть опосредованы другими… И в итоге получается что-то такое:\nмодель может быть сколь угодно сложна — и надо её каким-то образом эмпирически проверять. Это позволяют делать структурное моделирование (structural equation modelling, SEM).\nЭто группа методов, которые позволяют проверять модели (гипотезы), описывающие наши данные. Проверка состоит из двух больших шагов:\nНо, если вдуматься, то так работают все статистические методы — даже в линейной регрессии мы сначала задаём линейную модель, а потом проверяем, насколько хорошо она описывает взаимосвязи, представленные в данных. В чем же особенность SEM?\nВо-первых, в модель включаются латентные переменные — мы получаем возможность их моделировать и использовать для предсказаний. Этого не может делать, например, хорошо знакомая нам линейная регрессия, так как она работает только с наблюдаемыми переменными.\nВо-вторых, в модель могут быть включены косвенные связи — связи между латентными переменными. То есть мы можем моделировать связи между переменными, которые мы даже не можем измерить!\nКроме того, модели удобно визуализируются с помощью диаграмм, аналогично рисунку выше.\nВозникает вопрос: сколько нужно данных для такого?\nМожно ли рассчитать объем выборки, который необходим для проверки подобных гипотез? Можно, однако здесь есть некоторая проблемка.\nСтатистические расчеты дадут некоторый результат, но количество респондентов (наблюдений) часто оказывается далеко от необходимого —- статистика расходится к алгоритмами. Задача подбора SEM-моделей, конечно же, не имеет аналитического решения — с ним мы попрощались ещё на общих линейных моделях. Для того, чтобы параметры модели были достаточно точно подобраны,как правило, необходимо больше данных, чем говорят статистические расчеты.\nЕсть мнение, что «структурное моделирование требует большого количества данных». Не очень понятно, что считать большим количеством, однако эвристика следующая: на один оцениваемый параметр нужно не менее 10–15 наблюдений. Выглядит как что-то приемлемое.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>L15 // Структурное моделирование и конфирматорный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l15.html#крайне-краткое-введение-в-структурное-моделирование",
    "href": "l15.html#крайне-краткое-введение-в-структурное-моделирование",
    "title": "14  L15 // Структурное моделирование и конфирматорный факторный анализ",
    "section": "",
    "text": "задание теоретической модели генерации данных;\nпроверка того, насколько предложенная модель хорошо подходит под наши данные.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>L15 // Структурное моделирование и конфирматорный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l15.html#модель-конфирматорного-факторного-анализа",
    "href": "l15.html#модель-конфирматорного-факторного-анализа",
    "title": "14  L15 // Структурное моделирование и конфирматорный факторный анализ",
    "section": "14.2 Модель конфирматорного факторного анализа",
    "text": "14.2 Модель конфирматорного факторного анализа\nМы кратко взглянули на то, что такое структурное моделирование. Если мы возьмем от него только часть, то получим конфирматорный факторый анализ (confirmatory factor analysis). Осталось понять, какую часть надо взять.\nНаша задача — проверить факторную структуру данных, которую мы взяли из теории или нашли с помощью эксплораторного факторного анализа. То есть, в общем виде что-то такое:\n\n\n\n\n\nВыглядит уже не так страшно и запутанно.\nВ теоретической модели могут присутствовать различные типы связей:\n\nизмерения — связи между наблюдаемыми и латентными переменными (латентные → наблюдаемые)\nи корреляции — связи между наблюдаемыми или латентными переменными (наблюдаемые → наблюдаемые, латентные → латентные)\n\nТакже, как и вообще в любой модели, сущесвуют остатки — вариантивность переменных, которую не удалось смоделировать.\nВ SEM также есть регрессии — связи от наблюдаемых переменных к латентным (наблюдаемые → латентные)\nКак это всё работает внутри? Сложно. Но, на самом деле, за всем стоит привычная нам множественная линейная регрессия и уже знакомый нам метод максимального правдоподобия, ведь мы всё ещё в рамках линейных моделей.\nЧуть выше мы обсуждали, сколько нужно данных, и сказали, что «10 наблюдений на один параметр». Однако тактично умолчали, что есть такое параметр. Так вот параметры — это, проще говоря, стрелочки на схеме выше. Это либо факторные нагрузки, так же как и в EFA, или коэффициенты косвенных связей между латентными переменными.\nКонфирматорный факторный анализ в некотором смысле дополняет эксплораторный. Так, он позволяет уточнить и дополнить результаты последнего, а именно ответить на вопросы:\n\nпересекаются ли факторы? — действительно ли каждый переменная обусловлена влиянием одного фактора?\nдостоверны ли статистически факторные нагрузки?\nкак коррелируют сами факторы и как это влияет на факторные нагрузки?\nотличается ли дисперсия фактора от нуля? — ведь если нет, тогда этот фактор не информативен, то есть не дифференцирует респондентов",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>L15 // Структурное моделирование и конфирматорный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l15.html#оценка-качества-модели",
    "href": "l15.html#оценка-качества-модели",
    "title": "14  L15 // Структурное моделирование и конфирматорный факторный анализ",
    "section": "14.3 Оценка качества модели",
    "text": "14.3 Оценка качества модели\nЧтобы проверить, согласуются ли эмпирические данные с теоретической моделью, хочется использовать какой-то статистический тест. Однако и тут есть проблема…\nТакой тест есть — это chi-squared for model fit. Его нулевая гипотеза гласит, что модель соответствует данным, а альтернативная — что модель не соответствует им. Проблема с этим тестом в том, что на больших выборках он всегда дает статистически значимые результаты, а значит мы всегда будем вынуждены отклонять нулевую гипотезу, чего делать нам крайне не хотелось бы. Поэтому на практике результаты этого статистического теста даже не приводятся, а вместо него используют метрики качества модели.\nИх много, но самых популярных шесть. Для каждой из них определены пороговые значения.\n\n\n\nМетрика\nЗначение\n\n\n\n\nGFI, goodnes of fit (аналог \\(R^2\\))\n\\(&gt;0.95\\)\n\n\nAGFI, adjusted goodnes of fit\n\\(&gt;0.90\\)\n\n\nCFI, comparative fit index\n\\(&gt;0.95\\)\n\n\nTLI, Tucker Lewis index\n\\(&gt;0.95\\)\n\n\nRMSEA, Root Mean Square Error of Approximation\n\\(&lt;0.05\\)\n\n\nSRMR, Standardized Root Mean Square Residual\n\\(&lt;0.05\\)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>L15 // Структурное моделирование и конфирматорный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l15.html#изучение-параметров-модели",
    "href": "l15.html#изучение-параметров-модели",
    "title": "14  L15 // Структурное моделирование и конфирматорный факторный анализ",
    "section": "14.4 Изучение параметров модели",
    "text": "14.4 Изучение параметров модели\nКФА позволяет прямо статистически протестировать гипотезы о значимости параметров модели (факторных нагрузок, корреляций). Для этого используется следующая нехитрая таблица:\n\n\n\nСвязь\nОценка параметра\nSE\nz\np\nДоверительный интервал\n\n\n\n\nF1 =~ q1\n0.787\n0.020\n39.421\n0.000\n[0.748, 0.827]\n\n\nF1 ~~ F2\n0.674\n0.032\n21.123\n0.000\n[0.611, 0.736]\n\n\n\nАналогично можно протестировать и значимость остатков.\nКроме статистической значимости, разумно посмотреть и на само значение оценки параметра. Так, если факторная нагрузка меньше 0.4, то можно рассмотреть вариант исключения вопроса из опросника, так как он не очень сильно связан с фактором (латентной переменной).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>L15 // Структурное моделирование и конфирматорный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l15.html#модификация-модели",
    "href": "l15.html#модификация-модели",
    "title": "14  L15 // Структурное моделирование и конфирматорный факторный анализ",
    "section": "14.5 Модификация модели",
    "text": "14.5 Модификация модели\nКонфирматорный факторный анализ предполагает наличие некоторой модели, структуру которой мы проверяем на данных. Однако вполне может быть, что наша изначальная модель не учитывыает что-то, что присутствует в данных.\nДопустим, наша изначальная модель выглядела так:\n\n\n\n\n\nКФА позволяет проверить, насколько связи, не включенные в модель, потенциально улучшают качество модели. Для этого существуют индексы модификации (modification indices). Это относительная мера того, насколько улучшится наша модель, если мы включим в неё данную связь. Индексы модификации рассчитываются для всех связей, которые могут быть включены в модель.\nЕсть большой соблазн включить в изначальную модель связи с высокими индексами модификации, чтобы улучшить модель — особенно в случае, когда метрики качества модели не очень хороши. Так делать не надо. Это p-hacking.\nКак же нам тогда их использовать на благо аналитики? Схема такова:\n\nизучить индексы модификации и выявить связи с наибольшими индексами модификации\nпопробовать найти теоретическое основание таких связей\nвключить их в теоретическую модель\nсобрать новые данные\nпроверить обновленную модель на новых данных\n\nИ по результатам анализа сделать вывод о новой модели.\nВ частности, у нас может получиться такая модель, если мы включим корреляции с высокими индексами модификации между факторами и пунктами опросника:",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>L15 // Структурное моделирование и конфирматорный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l15.html#сравнение-моделей",
    "href": "l15.html#сравнение-моделей",
    "title": "14  L15 // Структурное моделирование и конфирматорный факторный анализ",
    "section": "14.6 Сравнение моделей",
    "text": "14.6 Сравнение моделей\nДля сравнения моделей, во-первых, подойдут метрики качества моделей, а во-вторых, можно использоваться хи-квадрат тест, аналогично тому, как мы делали в обобщенных и смешанных линейных моделях. Также рассчитываются и информационные критерии, которые также позволяют сравнивать модели друг с другом.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>L15 // Структурное моделирование и конфирматорный факторный анализ</span>"
    ]
  },
  {
    "objectID": "l15.html#footnotes",
    "href": "l15.html#footnotes",
    "title": "14  L15 // Структурное моделирование и конфирматорный факторный анализ",
    "section": "",
    "text": "Это, конечно, не единственный критерий, но, пожалуй, один из самых важных.↩︎",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>L15 // Структурное моделирование и конфирматорный факторный анализ</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Источники",
    "section": "",
    "text": "Barrett, P. 2005. “What If There Were No Psychometrics?:\nConstructs, Complexity, and Measurement.” Journal of\nPersonality Assessment 85 (2): 134–40.\n\n\nMessick, S. 1993. “Foundations of Validity: Meaning and\nConsequences in Psychological Assessment.” Ets Research\nReport Series 1993 (2): i–18.\n\n\nЗегет, В. 1985. Элементарная Логика. Москва: Высшая школа.",
    "crumbs": [
      "Источники"
    ]
  },
  {
    "objectID": "l2.html#группировка-и-агрегация-данных",
    "href": "l2.html#группировка-и-агрегация-данных",
    "title": "2  L2 // Предобработка данных. Дата и время. Визуализация данных",
    "section": "2.19 Группировка и агрегация данных",
    "text": "2.19 Группировка и агрегация данных\n\nds\n\n# A tibble: 7 × 6\n     id var1   var2 var3  var4   var5\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 Abc       5 TRUE  cond1  12.8\n2     2 Def      16 FALSE cond1  14.2\n3     3 Ghi      94 FALSE cond2  32.5\n4     4 Jkl      28 FALSE cond2   9.4\n5     5 Mno      11 TRUE  cond3   6.3\n6     6 Pqr     100 TRUE  cond3  11.7\n7     7 Stu      96 FALSE cond1  95.5\n\nds %&gt;% \n  summarise(v5_mean = mean(var5),\n            v2_median = median(var2))\n\n# A tibble: 1 × 2\n  v5_mean v2_median\n    &lt;dbl&gt;     &lt;dbl&gt;\n1    26.1        28\n\nds %&gt;% \n  group_by(var4) %&gt;% \n  summarise(n = n(),\n            v5_mean = mean(var5),\n            v2_median = median(var2))\n\n# A tibble: 3 × 4\n  var4      n v5_mean v2_median\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 cond1     3    40.8      16  \n2 cond2     2    21.0      61  \n3 cond3     2     9        55.5\n\nds %&gt;% \n  summarise(n = n(),\n            v5_mean = mean(var5),\n            v2_median = median(var2),\n            .by = var4)\n\n# A tibble: 3 × 4\n  var4      n v5_mean v2_median\n  &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 cond1     3    40.8      16  \n2 cond2     2    21.0      61  \n3 cond3     2     9        55.5\n\n\n\nungroup()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>L2 // Предобработка данных. Дата и время. Визуализация данных</span>"
    ]
  }
]