<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>L // WLM 2023 - 10&nbsp; L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./l12.html" rel="next">
<link href="./l9.html" rel="prev">
<link href="./pics/favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Ничего не найдено",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Очистить",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Отмена",
    "search-submit-button-title": "Отправить",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./pics/logo.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">WLM 2023 // HSE UX LAB</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./../hub/index.html"> 
<span class="menu-text">Хаб</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./../lectures/index.html"> 
<span class="menu-text">Лекции</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./../practice/index.html"> 
<span class="menu-text">Практики</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./../homeworks/index.html"> 
<span class="menu-text">Домашки</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./../projects/index.html"> 
<span class="menu-text">Проекты</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./l10.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Вступление</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">R</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">L1 // Основы R. Типы и структура данных. Функции и управляющие конструкции</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">L2 // Предобработка данных. Дата и время Визуализация данных</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Математика и статистика</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">L3 // Математика для анализа данных</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">L4 // Теория измерений</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">L5 // Введение в статистику. Случайный эксперимент и случайные величины</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">L6 // Оценивание параметров в практике статистического анализа. Тестирование статистических гипотез</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Анализ данных</span></span>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">L7 // Описательные статистики. Корреляционный анализ</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">L8 // Общие линейные модели. Простая и множественная линейная регрессия</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">L9 // Дисперсионный анализ. Ковариационный анализ</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l10.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">L12 // Линейные модели со смешанными эффектами</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L13 // Кластерный анализ</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">L14 // Анализ главных компонент и эксплораторный факторный анализ</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./l15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">L15 // Структурное моделирование и конфирматорный факторный анализ</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Источники</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Содержание</h2>
   
  <ul>
  <li><a href="#ограничения-общих-линейных-моделей" id="toc-ограничения-общих-линейных-моделей" class="nav-link active" data-scroll-target="#ограничения-общих-линейных-моделей"><span class="header-section-number">10.1</span> Ограничения общих линейных моделей</a></li>
  <li><a href="#идея-обобщенных-линейных-моделей" id="toc-идея-обобщенных-линейных-моделей" class="nav-link" data-scroll-target="#идея-обобщенных-линейных-моделей"><span class="header-section-number">10.2</span> Идея обобщенных линейных моделей</a></li>
  <li><a href="#биноминальная-регрессия" id="toc-биноминальная-регрессия" class="nav-link" data-scroll-target="#биноминальная-регрессия"><span class="header-section-number">10.3</span> Биноминальная регрессия</a>
  <ul class="collapse">
  <li><a href="#бинарные-переменные-и-биномиальное-распределение" id="toc-бинарные-переменные-и-биномиальное-распределение" class="nav-link" data-scroll-target="#бинарные-переменные-и-биномиальное-распределение"><span class="header-section-number">10.3.1</span> Бинарные переменные и биномиальное распределение</a></li>
  <li><a href="#математическая-модель" id="toc-математическая-модель" class="nav-link" data-scroll-target="#математическая-модель"><span class="header-section-number">10.3.2</span> Математическая модель</a></li>
  <li><a href="#идентификация-модели" id="toc-идентификация-модели" class="nav-link" data-scroll-target="#идентификация-модели"><span class="header-section-number">10.3.3</span> Идентификация модели</a></li>
  <li><a href="#тестирование-качества-модели" id="toc-тестирование-качества-модели" class="nav-link" data-scroll-target="#тестирование-качества-модели"><span class="header-section-number">10.3.4</span> Тестирование качества модели</a></li>
  <li><a href="#тестирование-значимости-предикторов" id="toc-тестирование-значимости-предикторов" class="nav-link" data-scroll-target="#тестирование-значимости-предикторов"><span class="header-section-number">10.3.5</span> Тестирование значимости предикторов</a></li>
  <li><a href="#интерпретация-коэффициентов-модели" id="toc-интерпретация-коэффициентов-модели" class="nav-link" data-scroll-target="#интерпретация-коэффициентов-модели"><span class="header-section-number">10.3.6</span> Интерпретация коэффициентов модели</a></li>
  <li><a href="#предсказательная-сила-модели" id="toc-предсказательная-сила-модели" class="nav-link" data-scroll-target="#предсказательная-сила-модели"><span class="header-section-number">10.3.7</span> Предсказательная сила модели</a></li>
  </ul></li>
  <li><a href="#пуассоновская-регрессия" id="toc-пуассоновская-регрессия" class="nav-link" data-scroll-target="#пуассоновская-регрессия"><span class="header-section-number">10.4</span> Пуассоновская регрессия</a>
  <ul class="collapse">
  <li><a href="#счетные-данные" id="toc-счетные-данные" class="nav-link" data-scroll-target="#счетные-данные"><span class="header-section-number">10.4.1</span> Счетные данные</a></li>
  <li><a href="#распределение-пуассона" id="toc-распределение-пуассона" class="nav-link" data-scroll-target="#распределение-пуассона"><span class="header-section-number">10.4.2</span> Распределение Пуассона</a></li>
  <li><a href="#почему-общие-линейные-модели-плохо-работают" id="toc-почему-общие-линейные-модели-плохо-работают" class="nav-link" data-scroll-target="#почему-общие-линейные-модели-плохо-работают"><span class="header-section-number">10.4.3</span> Почему общие линейные модели плохо работают?</a></li>
  <li><a href="#математическая-модель-1" id="toc-математическая-модель-1" class="nav-link" data-scroll-target="#математическая-модель-1"><span class="header-section-number">10.4.4</span> Математическая модель</a></li>
  <li><a href="#результаты-моделирования" id="toc-результаты-моделирования" class="nav-link" data-scroll-target="#результаты-моделирования"><span class="header-section-number">10.4.5</span> Результаты моделирования</a></li>
  <li><a href="#квазипуассоновские-модели" id="toc-квазипуассоновские-модели" class="nav-link" data-scroll-target="#квазипуассоновские-модели"><span class="header-section-number">10.4.6</span> Квазипуассоновские модели</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">L10 // Обобщенные линейные модели. Логистическая регрессия. Пуассоновская регрессия</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!--- {{< include ../other/_symbols.qmd >}} --->
<!--- Logic --->
<!--- Number sets--->
<!--- Linear Algebra --->
<!--- vectors and matrices --->
<!--- transposed matrix --->
<!--- Probability, Random Vars --->
<!--- Distributions --->
<!--- Other --->
<!--- Trig --->
<!--- Stats --->
<section id="ограничения-общих-линейных-моделей" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="ограничения-общих-линейных-моделей"><span class="header-section-number">10.1</span> Ограничения общих линейных моделей</h2>
<p>Модели, которые мы изучали на предыдущих занятиях носят название <strong>общих линейных моделей (general linear models)</strong>. Они достаточно просты и удобны в большинстве случаев, однако имеют существенное ограничений.</p>
<p>Вспомним, как выглядит уравнение такой модели:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon
\]</span></p>
<p>Предикторы в такой модели, как мы знаем, могут быть как дискретными, так и непрерывными. Однако ключевым допущением (и требованием) этой модели является распределение ошибки:</p>
<p><span class="math display">\[
\varepsilon \thicksim \mathcal{N}(0, \sigma^2)
\]</span></p>
<p>Поскольку ошибка модели должна быть распределена нормально, а моделируется среднее значение, то можно сформулировать более общее допущение/требование:</p>
<p><span class="math display">\[
y \thicksim \mathcal{N}(\mu, \sigma^2)
\]</span></p>
<p>Таким образом, общие линейные модели позволяют моделировать зависимости только для нормально-распределенных величин. Если же отклик модели (он же зависимая переменная) подчиняется другому распределению, эти модели не годятся.</p>
</section>
<section id="идея-обобщенных-линейных-моделей" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="идея-обобщенных-линейных-моделей"><span class="header-section-number">10.2</span> Идея обобщенных линейных моделей</h2>
<p>Тем не менее, мы из своего опыта знаем, что существует много величин, распределение которых отличается от нормального. И дело даже не столько в асимметрии или эксцессе — здесь разговор о самой природе величин. Так, например, есть величина «поступление в вуз», у которой только два возможных значения — «поступил» и «не поступил». Или же, скажем, количество детей в семьей, варьирующееся в небольших пределах и принимающее только положительные целочисленые значения.</p>
<p>Нам хочется моделировать связь таких величин с предикторами так же, как мы делали в случае общих линейных моделей, или хотя бы в той же логике, которая нам хороша знакома из линейной регрессии. Для того, чтобы этого достичь, в модель вводится новый компонент — <strong>функция связи (link function)</strong>.</p>
<ul>
<li>Пусть у нас есть некоторая переменная <span class="math inline">\(y\)</span>, которая подчиняется какому-то закону распределения <span class="math inline">\(f(y|\theta)\)</span>, где <span class="math inline">\(\theta\)</span> — параметр(ы) распределения, и этот закон отличается от нормального.</li>
<li>Мы всё так же хотим моделировать «среднее» значение, а точнее, <em>математическое ожидание</em>, переменной <span class="math inline">\(\mathbb{E}(y)\)</span>.</li>
<li>Давайте возьмем некоторое преобразование (функцию) <span class="math inline">\(g \big( \mathbb{E}(y) \big) = \eta\)</span>, которое будет преобразовывать математическое ожидание нашей целевой переменной, <em>линеаризуя его</em>. Она и будет называться <strong>функцией связи</strong>.</li>
<li>Теперь у нас есть линейная величина <span class="math inline">\(\eta\)</span>, которую можно моделировать с помощью уже хорошо знакомой нам модели:</li>
</ul>
<p><span class="math display">\[
\eta_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon
\]</span></p>
<ul>
<li>Однако нас всё же интересует с точки зрения изученя закономерностей не <span class="math inline">\(\eta\)</span>, а <span class="math inline">\(\mathbb{E}(y)\)</span>, но так как функция <span class="math inline">\(g \big( \mathbb{E}(y) \big)\)</span> известна, мы возьмем <em>обратную функцию</em> <span class="math inline">\(g^{-1} (\eta) = \mathbb{E}(y)\)</span> и получим интересующие нас значения.</li>
</ul>
<p>Такова общая логика <strong>обобщенных линейных моделей (generalized linear models, GLM)</strong>. Какая именно функция связи (и, соответственно, обратная функция) будет использоваться, зависит от распределения целевой переменной. Далее мы рассмотрим два конкретных примера. Но уже сейчас мы можем написать общее уравнения для таких моделей:</p>
<p><span class="math display">\[
g \big( \mathbb{E}(y_i) \big) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \varepsilon_i
\]</span></p>
<p>Один из особых случаев возникает, когда к целевой переменной не применяется никакого преобразования — то есть используется <span class="math inline">\(g \big( \mathbb{E}(y)) = \mu\)</span> при <span class="math inline">\(y \thicksim \mathcal{N}(\mu, \sigma^2)\)</span>. Такая функция связи называется <em>функцией индентичности (identity function)</em>. В этом случае получается следующее:</p>
<p><span class="math display">\[
g \big( \mathbb{E}(y_i) \big) = \mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \varepsilon_i
\]</span></p>
<p>То есть мы получили обычную хорошо нам знакомую линейную регрессию. В этом смысле <em>обобщенные</em> линейные модели действительно <em>обобщают</em> случай обычной линейной регрессии на другие случай распределения целевой переменной.</p>
</section>
<section id="биноминальная-регрессия" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="биноминальная-регрессия"><span class="header-section-number">10.3</span> Биноминальная регрессия</h2>
<p>Рассмотрение фреймворка обобщенных линейных моделей мы начнем с биномиальной, или логистической, регрессии, которая позволяет моделировать бинарные переменные.</p>
<section id="бинарные-переменные-и-биномиальное-распределение" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="бинарные-переменные-и-биномиальное-распределение"><span class="header-section-number">10.3.1</span> Бинарные переменные и биномиальное распределение</h3>
<p>Бинарными называются переменные, которые могут принимать только два значения — вообще, любых, но так как нам нужнв такие обозначения, которые сможет переварить математика, используем <span class="math inline">\(0\)</span> и <span class="math inline">\(1\)</span>. Для нас это будут просто лейблы классов, так как мы, заменив обозначения, остались в рамках номинальной шкалы.</p>
<p>В теме про случайные величины мы вычисляли вероятность случайного прохождения теста из <span class="math inline">\(n\)</span> вопросов, каждый из которых мог быть решен правильно («успех», <span class="math inline">\(1\)</span>) или неправильно («неудача», <span class="math inline">\(0\)</span>), и описывали вероятность получить <span class="math inline">\(k\)</span> «успехов» в <span class="math inline">\(n\)</span> испытаниях как</p>
<p><span class="math display">\[
\mathbb{P}(X = k) = C_n^k \, p^k \, q^{n-k}
\]</span></p>
<p>Эта формула и задает биномиальное распределение: <span class="math inline">\(\mathbb{P}(X = k) \thicksim \text{Bin}(n, p)\)</span>:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="l10_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>От этого распределения и происходит название рассматриваемой регрессионной модели.</p>
</section>
<section id="математическая-модель" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="математическая-модель"><span class="header-section-number">10.3.2</span> Математическая модель</h3>
<p>Тем не менее, хоть биномиальное распределения и существует, реализация <span class="math inline">\(0\)</span> и <span class="math inline">\(1\)</span> в отдельном наблюдении полчинается не ему<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, поэтому первым концептуальным шагом построения модели биномиальной регрессии является переход от моделирования <span class="math inline">\(0\)</span> и <span class="math inline">\(1\)</span> к моделированию вероятности получения <span class="math inline">\(1\)</span>.</p>
<p>Рассмотрим картинку. Пусть мы пытаемся смоделировать связь между бинарной целевой переменной <span class="math inline">\(Y\)</span> и количественным предиктором <span class="math inline">\(X\)</span>. Тогда её визуализация будет схематично выглядеть так:</p>
<center>
<figure class="figure">
<img src="pics/bin_y.jpg" class="figure-img">
</figure>
</center>
<p>Тогда мы можем посчитать доли единиц в общем количестве исходов <em>при данном значении предиктора</em> <span class="math inline">\(p_{Y=1|x_i}\)</span> и использовать их как оценку вероятности <span class="math inline">\(\mathbb{P}(Y=1|x_i)\)</span>:</p>
<center>
<figure class="figure">
<img src="pics/bin_prob.jpg" class="figure-img">
</figure>
</center>
<p>Отлично, мы получили непрерывную переменную! В принципе, можно попытаться смоделировать её с помощью линейной регрессии, однако возникнет некоторая проблема:</p>
<center>
<figure class="figure">
<img src="pics/bin_prob_line.jpg" class="figure-img">
</figure>
</center>
<p>Переменная, которую мы получили, определяет <em>вероятность</em>, а значит она ограничена — <span class="math inline">\(0 \leq \mathbb{P}(Y=1|x_i) \leq 1\)</span>. К тому же, чисто визуально заметно, что эта вероятность изменяется нелинейной, а по некоторой кривой — примерно так:</p>
<center>
<figure class="figure">
<img src="pics/bin_prob_curve.jpg" class="figure-img">
</figure>
</center>
<p>Супер… Теперь еще и искать кривую…</p>
<section id="логистическая-кривая" class="level4" data-number="10.3.2.1">
<h4 data-number="10.3.2.1" class="anchored" data-anchor-id="логистическая-кривая"><span class="header-section-number">10.3.2.1</span> Логистическая кривая</h4>
<p>К счаcтью, математики поработали за нас, и сообщили нам, что такая закономерность хорошо моделируется <strong>логистической кривой (logistic curve)</strong> — отсюда второй название рассматриваемой нами модели (логистическая регрессия):</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="l10_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Эта прямая как раз зажата по <span class="math inline">\(y\)</span> между нулем и единицей, а её изгибы хорошо подходят под связь вероятности <span class="math inline">\(\mathbb{P}(Y=1|x_i)\)</span> со значением предиктора <span class="math inline">\(X\)</span>.</p>
<p>Её формула такова:</p>
<p><span class="math display">\[
y = \frac{e^x}{1 + e^x}
\]</span></p>
<p>Однако если мы хотим описывать зависимость вероятности от значения предиктора, то вместо <span class="math inline">\(x\)</span> нам необходимо подставить <span class="math inline">\(\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots \beta_p x_{ip}\)</span>:</p>
<p><span class="math display">\[
\mathbb{P}(Y=1|x_i) = p_i = \frac{e^{
\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots \beta_p x_{ip}}
}
{1 + e^{
\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots \beta_p x_{ip}}
}
\]</span></p>
<p>Выглядит, безусловно, страшно. Но наш путь еще не завершен…</p>
</section>
<section id="шансы-и-логиты" class="level4" data-number="10.3.2.2">
<h4 data-number="10.3.2.2" class="anchored" data-anchor-id="шансы-и-логиты"><span class="header-section-number">10.3.2.2</span> Шансы и логиты</h4>
<p>Мы победили дискретность целевой переменной <span class="math inline">\(y\)</span> и подобрали кривую, которая хорошо моделирует зависимость искомой вероятности от значений предиктора. Однако кривая всё ещё ограничена нулем и единицей, а значит смоделировать её м помощью линейной регрессии не получится.</p>
<p>Для того, чтобы победить ограниченность логистической кривой, используются шансы.</p>
<p><strong>Шанс (отношение шансов, odds, odds ratio)</strong> — это отношение вреоятности «успеха» (<span class="math inline">\(1\)</span>) к вероятности «неудачи» (<span class="math inline">\(0\)</span>). Эта величина хороша тем, что изменяется от <span class="math inline">\(0\)</span> до <span class="math inline">\(+\infty\)</span>. Получается,</p>
<p><span class="math display">\[
\text{odds}_i = \frac{\mathbb{P}(Y=1|x_i)}{1 - \mathbb{P}(Y=1|x_i)} = \frac{p_i}{1 - p_i}
\]</span></p>
<p>Отлично, мы побороли ограниченность логистической кривой сверху — движемся к успеху.</p>
</section>
<section id="logit-преобразование" class="level4" data-number="10.3.2.3">
<h4 data-number="10.3.2.3" class="anchored" data-anchor-id="logit-преобразование"><span class="header-section-number">10.3.2.3</span> Logit-преобразование</h4>
<p>Для того, чтобы победить ограниченность кривой снизу, возьмем логарифм от шанса. Получим следующее:</p>
<p><span class="math display">\[
\text{logit}(p_i) = \ln \left(\frac{p_i}{1 - p_i} \right)
\]</span></p>
<p>Такое преобразование вероятности разывается логит-преобразованием (logit-transformation). Значения логитов варьируются от <span class="math inline">\(-\infty\)</span> до <span class="math inline">\(+\infty\)</span>, симметричны относительно нуля, и их удобно брать в каечестве целевой переменной для построения линейной модели. Кроме того, логит-преобразование еще и <em>линеаризует логистическую кривую</em>! Очень хорошее преобразование! Просто замечательное.</p>
<details>
<summary>
Линеаризация логистической кривой через logit-преобразование
</summary>
<p>Возьмем случай в одним предиктором, чтобы было попроще. Логистическая кривая, моделирующая вероятность, имеет такой вид:</p>
<p><span class="math display">\[
p_i = \frac{e^{\beta_0 + \beta_1 x_{i1}}}
{1 + e^{\beta_0 + \beta_1 x_{i1}}}
\]</span></p>
<p>Обозначим <span class="math inline">\(\beta_0 + \beta_1 x_{i1} = t\)</span>. Тогда необходимо показать, что логит-преобразование</p>
<p><span class="math display">\[
\text{logit} (p_i) = \ln \left(\frac{p_i}{1 - p_i} \right)
\]</span></p>
<p>делает логистическую функцию линейной, то есть обычной прямой. Иначе говоря, необходимо показать, что</p>
<p><span class="math display">\[
\ln \left(\frac{p_i}{1 - p_i} \right)= t_i
\]</span></p>
<p>Доказывается это через расписывание формулы и раскрытие всех скобок и логарифмов (индекс <span class="math inline">\(i\)</span> опущен для упрощения записи):</p>
<p><span class="math display">\[
\begin{split}
\ln \left(\frac{p}{1-p} \right)&amp;= \\
&amp;= \ln \left(\frac{\frac{e^t}{1 + e^t}}{1 - \frac{e^t}{1 + e^t}} \right)= \\
&amp;= \ln \left(\frac{e^t}{1 + e^t} \right)- \ln \left(1 - \frac{e^t}{1 + e^t} \right)= \\
&amp;= \ln \left(\frac{e^t}{1 + e^t} \right)- \ln \left(\frac{1 + e^t - e^t}{1 + e^t} \right)= \\
&amp;= \ln \left(\frac{e^t}{1 + e^t} \right)- \ln \left(\frac{1}{1 + e^t} \right)= \\
&amp;= \ln (e^t) - \ln (1 + e^t) - \big(\ln (1) - \ln (1+e^t)\big) = \\
&amp;= \ln (e^t) - \ln (1) = \\
&amp;= \ln (e^t) = t
\end{split}
\]</span></p>
<hr>
</details>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Cаммари того, что происходило выше:
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>От дискретной оценки событий (0 и 1) переходим к оценке вероятностей.</li>
<li>Связь вероятностей с предиктором описывается логистической кривой.</li>
<li>Если при помощи функции связи перейти от вероятностей к логитам, то связь будет описываться прямой линией.</li>
<li>Параметры линейной модели для такой прямой можно оценить с помощью регрессионного анализа.</li>
</ul>
</div>
</div>
<p>В итоге мы получаем, что математическая модель логистической регрессии выглядит так:</p>
<p><span class="math display">\[
p_i = \frac{e^{\beta_0 + \beta_1 x_{i1}}}
{1 + e^{\beta_0 + \beta_1 x_{i1}}}
\]</span></p>
<p>Функция связи — логит:</p>
<p><span class="math display">\[
\text{logit} (p_i) = \ln \left(\frac{p_i}{1 - p_i} \right)= \eta_i
\]</span></p>
<p><span class="math display">\[
\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip}
\]</span></p>
<p>А это ровно то, что нам было нужно!</p>
<p>Для перехода от логитов к вероятностям используется обратная функция вида</p>
<p><span class="math display">\[
p_i = \frac{e^{\eta_i}}{1 + e^{\eta_i}}
\]</span></p>
</section>
</section>
<section id="идентификация-модели" class="level3" data-number="10.3.3">
<h3 data-number="10.3.3" class="anchored" data-anchor-id="идентификация-модели"><span class="header-section-number">10.3.3</span> Идентификация модели</h3>
<p>Все эти модификации модели приводят к тому, что задача индентификации модели логистической регрессии — то есть вычисления значений коэффициентов модели — <strong>не имеет аналитического решения</strong>. То есть мы не можем вычислить коэффциент просто имея значения целевой переменной и предикторов. По этой причине используются численные методы для получения оценок коэффициентов.</p>
<section id="метод-максимального-правдоподобия" class="level4" data-number="10.3.3.1">
<h4 data-number="10.3.3.1" class="anchored" data-anchor-id="метод-максимального-правдоподобия"><span class="header-section-number">10.3.3.1</span> Метод максимального правдоподобия</h4>
<p>В частности, метод <strong>максимального правдоподобия (maximuum likelihood)</strong>, который позволяет получить несмешенные и состоятельные и эффективные оценки коэффициентов.</p>
<p><strong>Правдоподобие (likelihood)</strong> — это способ измерить соответствие имеющихся данных тому, что можно получить при определенных значениях параметров модели.</p>
<p>Вычисляется значение правдоподобия как произведение вероятностей получения каждой из точек данных:</p>
<p><span class="math display">\[
L(\theta|\text{data}) = \prod_{i=1}^n f(\text{data}|\theta),
\]</span></p>
<p>где <span class="math inline">\(f(\text{data}|\theta)\)</span> — функция распределения с параметрами <span class="math inline">\(\theta\)</span>.</p>
<p>Задача метода максимального правдоподобия — найти наиболее правдоподобное решение, иначе говоря, максимизировать значение функции правдоподобия:</p>
<p><span class="math display">\[
L(\theta|\text{data}) \to \max_{\mathbf{b}}
\]</span></p>
<p>Так как правдоподобие — это произведение вероятностей, то функция правдоподобия принимает очень маленькие значение, поэтому работают с максимизацией логарифма правдоподобия (loglikelihood):</p>
<p><span class="math display">\[
\ln \big( L(\theta | \text{data}) \big) \to \max_{\mathbf{b}}
\]</span></p>
</section>
</section>
<section id="тестирование-качества-модели" class="level3" data-number="10.3.4">
<h3 data-number="10.3.4" class="anchored" data-anchor-id="тестирование-качества-модели"><span class="header-section-number">10.3.4</span> Тестирование качества модели</h3>
<p>Приколы, связанные с преобразованиями на этом не заканчиваются. При переходе к обобщенным линейным моделям у нас пропадают две важные статистики: <span class="math inline">\(F\)</span>-статистика и <span class="math inline">\(R^2\)</span> — их теперь невозможно рассчитать, так как мы моделируем вероятность получения «единиц», а не сами «единицы» и «нули».</p>
<p>Что же делать?</p>
<section id="девианса" class="level4" data-number="10.3.4.1">
<h4 data-number="10.3.4.1" class="anchored" data-anchor-id="девианса"><span class="header-section-number">10.3.4.1</span> Девианса</h4>
<p>Нам на помощь приходит <strong>девианса (deviance)</strong>. Чтобы попробовать понять, что это такое, введем две теоретические модели:</p>
<ul>
<li><strong>Насыщенная модель (saturated model)</strong> — модель, в которой каждое наблюдение (сочетание предикторов) описывается одним из <span class="math inline">\(n\)</span> параметров. Для такой модели справедливо:</li>
</ul>
<p><span class="math display">\[
\begin{split}
&amp; \ln L_\text{sat} = 0 \\
&amp; \text{df}_\text{sat} = n - p_\text{sat} = n - n = 0
\end{split}
\]</span></p>
<ul>
<li><strong>Нулевая модель (null model)</strong> — модель, в которой все наблюдения описываются одним параметром (средним значением). Для такой модели справедливо:</li>
</ul>
<p><span class="math display">\[
\begin{split}
&amp; \eta_i = \beta_0 \\
&amp; \ln L_\text{null} \neq 0, \; \ln L_\text{null} \to -\infty \\
&amp; \text{df}_\text{null} = n - p_\text{null} = n - 1
\end{split}
\]</span></p>
<p>Наша же <strong>[предложенная] модель</strong>, то есть та, с которой мы работаем, будем находится где-то между насыщенной и нулевой моделями:</p>
<p><span class="math display">\[
\begin{split}
&amp; \eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} \\
&amp; \ln L_\text{model} \neq 0 \\
&amp; \text{df}_\text{model} = n - p_\text{model}
\end{split}
\]</span></p>
<p><strong>Девианса</strong> является мерой различия правдоподобий двух моделей (оценка разницы логарифмов правдоподобий)</p>
<center>
<figure class="figure">
<img src="pics/deviance.jpg" class="figure-img">
</figure>
</center>
<ul>
<li><strong>Остаточная девианса</strong>: <span class="math inline">\(d_\text{resid} = 2(\ln L_\text{sat} - \ln L_\text{model}) = -2 \ln L_\text{model}\)</span></li>
<li><strong>Нулевая девианса</strong>: <span class="math inline">\(d_\text{null} = 2(\ln L_\text{sat} - \ln L_\text{null}) = -2 \ln L_\text{null}\)</span></li>
</ul>
</section>
<section id="анализ-девиансы" class="level4" data-number="10.3.4.2">
<h4 data-number="10.3.4.2" class="anchored" data-anchor-id="анализ-девиансы"><span class="header-section-number">10.3.4.2</span> Анализ девиансы</h4>
<p>Сравнение нулевой и остаточной девианс позволяет судить о статистической значимости модели в целом. Такое сравнение проводится с помощью <strong>теста отношения правдоподобий (likelihood ratio test, LRT).</strong></p>
<p><span class="math display">\[
\begin{split}
d_\text{null} - d_\text{resid} &amp;= \\
&amp;= -2 (\ln L_\text{null} - \ln L_\text{model}) = \\
&amp;= 2 (\ln L_\text{model} - \ln L_\text{null}) = \\
&amp;= 2 \ln \left(\frac{L_\text{model}}{L_\text{null}} \right)
\end{split}
\]</span></p>
<p><span class="math display">\[
\text{LRT} = 2 \ln \left(\frac{L_\text{M1}}{L_\text{M2}} \right)= 2 (\ln L_\text{M1} - \ln L_\text{M2}),
\]</span></p>
<p>где <span class="math inline">\(\text{M1}\)</span> и <span class="math inline">\(\text{M2}\)</span> — вложенные модели (<span class="math inline">\(\text{M1}\)</span> — более полная, <span class="math inline">\(\text{M2}\)</span> — сокращенная). Распределение разницы логарифмов правдоподобий аппроксимируется распределением <span class="math inline">\(\chi^2\)</span> со степенями свободы <span class="math inline">\(\text{df} = \text{df}_\text{M2} - \text{df}_\text{M1}\)</span>.</p>
<p>То есть, в общем случае тест отношения правдоподобий позволяет статистически сравнить две модели друг с другом. В случае же тестирования значимости модели в целом получается следующее:</p>
<p><span class="math display">\[
\begin{split}
&amp; \text{LRT} = 2 \ln \left(\frac{L_\text{model}}{L_\text{null}} \right)= 2 (\ln L_\text{model} - \ln L_\text{null}) = d_\text{null} - d_\text{model} \\
&amp; \text{df} = p_\text{model} - 1
\end{split}
\]</span></p>
<p>Кроме того, поскольку мы можем исключить из модели только один предиктор, тест отношения правдоподобий может быть использован и для тестирования значимости отдельных предикторов:</p>
<p><span class="math display">\[
\begin{split}
&amp; \text{LRT} = 2 \ln \left(\frac{L_\text{model}}{L_\text{reduced}} \right)= 2 (\ln L_\text{model} - \ln L_\text{reduced}) \\
&amp; \text{df} = p_\text{model} - p_\text{reduced}
\end{split}
\]</span></p>
<p>Итак, тест отношения правдоподобий является аналогом F-статистики. Что же делать с отсутствием <span class="math inline">\(R^2\)</span>?</p>
<p>Здесь также помогает девианса. Одним из вариантов оценки качества модели является расчет <strong>доли объясненной девиансы</strong>:</p>
<p><span class="math display">\[
\frac{d_\text{null} - d_\text{residual}}{d_\text{null}}
\]</span></p>
<p>Эта метрика — одна из разновидностей «псевдо-<span class="math inline">\(R^2\)</span>».</p>
</section>
<section id="информационные-критерии" class="level4" data-number="10.3.4.3">
<h4 data-number="10.3.4.3" class="anchored" data-anchor-id="информационные-критерии"><span class="header-section-number">10.3.4.3</span> Информационные критерии</h4>
<p>Помимо статистических критериев, сравнить модели можно также с использованием <em>информационных критериев</em>. Их два — баейсовский (Bayesian information criterion, BIC) и информационный критерий Акаике (Akaike information criterion, AIC). Чем ниже значение информационного критерия, тем лучше модель описывает имеющиеся данные.</p>
</section>
<section id="допущения-логистической-регрессии" class="level4" data-number="10.3.4.4">
<h4 data-number="10.3.4.4" class="anchored" data-anchor-id="допущения-логистической-регрессии"><span class="header-section-number">10.3.4.4</span> Допущения логистической регрессии</h4>
<p>Ряд допущений логистической регрессии совпадает с допущениями общих линейных моделей. В частности:</p>
<ul>
<li>независимость наблюдений</li>
<li>линейность связи целевой переменной и предикторов (с учетом функции связи)</li>
<li>отсутствие коллинеарности предикторов</li>
</ul>
<p>Однако появляется еще одно важное допущение — отсутствие сверхдисперсии.</p>
</section>
<section id="проверка-на-сверхдисперсию" class="level4" data-number="10.3.4.5">
<h4 data-number="10.3.4.5" class="anchored" data-anchor-id="проверка-на-сверхдисперсию"><span class="header-section-number">10.3.4.5</span> Проверка на сверхдисперсию</h4>
<p>Новое допущение связано с тем, что для биномиального распределения характерна связт между математическим ожиданием и дисперсией — этого не было у нормального распределения. Для биномиального распределения:</p>
<p><span class="math display">\[
\begin{split}
&amp; \mathbb{E}(X) = np \\
&amp; \text{var}{X} = np(p-1)
\end{split}
\]</span></p>
<p>где <span class="math inline">\(n\)</span> — количество испытаний, <span class="math inline">\(p\)</span> — вероятность «успеха» в одном испытании.</p>
<p>Если в модели обнаруживается свердисперсия, то мы на может гарантировать, что искомая закономерность смоделирована точно. В детали расчета и проверки гипотезы мы прогружать не будет, на практике воспользуемся специальной функцией для проверки этого допущения.</p>
</section>
</section>
<section id="тестирование-значимости-предикторов" class="level3" data-number="10.3.5">
<h3 data-number="10.3.5" class="anchored" data-anchor-id="тестирование-значимости-предикторов"><span class="header-section-number">10.3.5</span> Тестирование значимости предикторов</h3>
<p>С одним из способ тестирование статистической значимости предикторов мы уже познакомились — это тесты отношения правдоподобий. Однако существует и второй способ — в каком-то смысле более «базовый» — <strong>тесты Вальда</strong>.</p>
<section id="тесты-вальда" class="level4" data-number="10.3.5.1">
<h4 data-number="10.3.5.1" class="anchored" data-anchor-id="тесты-вальда"><span class="header-section-number">10.3.5.1</span> Тесты Вальда</h4>
<p>Эти тесты являются аналогом t-тестов для общих линейных моделей, однако являются менее точными, так как распределение их z-статистики только <em>ассимптотически</em> стремится к нормальному. Это значит, что на малых выборках эти тесты буду давать неточные результаты.</p>
<p>Тем не менее, они автоматически выводятся в аутпуте функций в R, поэтому посмотрим на них — они критически похожи на t-тесты:</p>
<p><span class="math display">\[
\begin{split}
H_0 &amp;: \beta_k = 0 \\
H_1 &amp;: \beta_k \neq 0
\end{split}
\]</span></p>
<p><span class="math display">\[
z = \frac{b_k - \beta_k}{\text{se}_{b_k}} = \frac{b_k}{\text{se}_{b_k}} \thicksim \mathcal{N}(0, 1)
\]</span></p>
</section>
</section>
<section id="интерпретация-коэффициентов-модели" class="level3" data-number="10.3.6">
<h3 data-number="10.3.6" class="anchored" data-anchor-id="интерпретация-коэффициентов-модели"><span class="header-section-number">10.3.6</span> Интерпретация коэффициентов модели</h3>
<p>С интерпретацией значений коэффициентов тоже оказывается не все просто. Вспомним, коэффициенты какой модели мы в итоге получаем:</p>
<p><span class="math display">\[
\eta_i = \hat \beta_0 + \hat \beta_1 x_{i1} + \hat \beta_2 x_{i2} + \dots + \hat \beta_p x_{ip}
\]</span></p>
<p>Наша целевая переменная в модели, для которой мы получили значения предикторов — это логарифм отношения шансов. Довольно сложно понять, что это такой с содержательной стороны. Однако чисто технически получается следующее:</p>
<ul>
<li><span class="math inline">\(\hat  \beta_0\)</span>, интерсепт, показывает логарифм отношения шансов для случая, когда значения всех предикторов равны нулю</li>
<li><span class="math inline">\(\hat \beta_k\)</span> показывает, на сколько изменится логарифм отношения шансов при изменении значения предиктора на единицу</li>
</ul>
<p>Корректно, но всё её непонятно…</p>
<p>Давайте посмотрим, что получится, если расписать изменение логарифма отношения шансов. Пусть для простоты у нас есть модель с одним непрерывным предиктором:</p>
<p><span class="math display">\[
\eta = b_0 + b_1 x
\]</span></p>
<p>При этом</p>
<p><span class="math display">\[
\eta = \ln \left(\frac{p}{1-p} \right)= \ln (\text{odds})
\]</span></p>
<p>Как изменится предсказание модели при увеличении предиктора на единицу?</p>
<p><span class="math display">\[
\eta_{x+1} - \eta_x = \ln (\text{odds}_{x+1}) - \ln (\text{odds}_x) = \ln \left(\frac{\text{odds}_{x+1}}{\text{odds}_x} \right)
\]</span></p>
<p>С другой стороны:</p>
<p><span class="math display">\[
\begin{split}
\eta_{x+1} - \eta_x &amp;= \big( b_0 + b_1(x+1) \big) - \big( b_0 + b_1 x \big) = \\
&amp;= b_0 + b_1 x + b_1 - b_0 - b_1 x = b_1
\end{split}
\]</span></p>
<p>Получается, что</p>
<p><span class="math display">\[
\begin{split}
\ln \left(\frac{\text{odds}_{x+1}}{\text{odds}_x} \right)&amp;= b_1 \\
\frac{\text{odds}_{x+1}}{\text{odds}_x} = e^{b_1}
\end{split}
\]</span></p>
<p>Таким образом, <span class="math inline">\(e^{b_1}\)</span> показывает, во сколько раз изменится шанс того, что наблюдение принадлежит к группе «единиц» при увеличении предиктора на единицу. Для дискретных предикторов <span class="math inline">\(e^{b_1}\)</span> покажет, во сколько раз различается отношение шансов для данного уровня предиктора по сравнению с базовым.</p>
</section>
<section id="предсказательная-сила-модели" class="level3" data-number="10.3.7">
<h3 data-number="10.3.7" class="anchored" data-anchor-id="предсказательная-сила-модели"><span class="header-section-number">10.3.7</span> Предсказательная сила модели</h3>
<p>Оценить предсказательную силу общих линейных моделей не сложно — надо сравнить предсказанные значения с предскавленными в данных. С логистической моделью же есть некоторые тонкости.</p>
<p>В данных у нас лежат нули и единицы, а модель нам возвращает вероятность того, что отдельное наблюдение является единицей. Для того, чтобы нам получить предсказанные значения, которые мы будем сопоставлять с имеющимися в данных, нам необходимо перевести непрерывные предсказания в дискретные.</p>
<p>Для этого необходимо выбрать <strong>порог</strong> — если значение вероятности выше него, мы будем считать, что модель предсказала <span class="math inline">\(1\)</span>, если ниже, то <span class="math inline">\(0\)</span>. Значение порога зависит от многих факторов и будет влиять на качество модели. Прежде всего стоит ориентироваться на сферу деятельности, в которой вы проводите анализ. Если у вас качественные чистые данные и вам важна высокая точность, то и порог для предсказаний должен быть высокий — <span class="math inline">\(\geq 0.9\)</span>. Если же вы знаете, что вы работаете с зашумлёнными данными, и цена ошибки не так высока, то можете выбрать более либеральный критерий — <span class="math inline">\(0.7\)</span>–<span class="math inline">\(0.8\)</span>. Самый либеральный критерий из возможных — <span class="math inline">\(0.5\)</span>, что по сути есть вероятность случайного угадывания.</p>
<section id="confusion-mattrix" class="level4" data-number="10.3.7.1">
<h4 data-number="10.3.7.1" class="anchored" data-anchor-id="confusion-mattrix"><span class="header-section-number">10.3.7.1</span> Confusion mattrix</h4>
<p>Когда мы перевели вероятности в нули и единицы, мы можем построить <strong>confusion matrix</strong> — — это таблица частот по модельным и реальным значениям нашей целевой переменной. Её общая структура выглядит так:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">Предсказания: <span class="math inline">\(0\)</span></th>
<th style="text-align: center;">Предсказания: <span class="math inline">\(1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Данные: <span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\text{TN}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\text{FP}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Данные: <span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\text{FN}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\text{TP}\)</span></td>
</tr>
</tbody>
</table>
<ul>
<li><em>True Positive</em> (<span class="math inline">\(\text{TP}\)</span>) — верное предсказанные единицы</li>
<li><em>True Negative</em> (<span class="math inline">\(\text{TN}\)</span>) — верно предсказанные нули</li>
<li><em>False Positive</em> (<span class="math inline">\(\text{FP}\)</span>) — ложноположительные предсказания, ошибочно предсказанные единицы</li>
<li><em>False Negative</em> ($ — ложноотрицательные предсказания, ошибочно предсказанные нули</li>
</ul>
<p>На основе данных значение можно расчитать несколько <strong>метрик качества модели</strong>.</p>
<section id="accuracy" class="level5" data-number="10.3.7.1.1">
<h5 data-number="10.3.7.1.1" class="anchored" data-anchor-id="accuracy"><span class="header-section-number">10.3.7.1.1</span> Accuracy</h5>
<p>Чаще всего эту метрику называют «точность». Она определяется по формуле</p>
<p><span class="math display">\[
\text{accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
\]</span></p>
<p>Она показывает долю верно предказанных значений и хорошо работает на сбалансированных данных, когда у вас одинаковое количество нулей и единиц в исходном датасете. Однако в случае несбалансированных данных даже самая плохая модель может иметь высокий показатель accuracy. Например, в данных такие значения целевой переменной:</p>
<p><span class="math display">\[
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1
\end{pmatrix}
\]</span></p>
<p>Пусть у нас есть наиболее топорная модель, которая просто всегда предсказывает ноль — у такой модели accuracy окажется равной <span class="math inline">\(0.73\)</span>. Вроде бы высокий показатель, но совершенное неадекватно описывает качество модели.</p>
<p>Поэтому были придуманы более точные метрики.</p>
</section>
<section id="precision" class="level5" data-number="10.3.7.1.2">
<h5 data-number="10.3.7.1.2" class="anchored" data-anchor-id="precision"><span class="header-section-number">10.3.7.1.2</span> Precision</h5>
<p>Precision тоже переводится как «точность», поэтому лучше пользоваться английской терминологией во избежании путаницы. Эта метрика показывается <strong>долю верно предсказанных единиц</strong>, то есть сколько из предсказанных единиц предсказано верно:</p>
<p><span class="math display">\[
\text{precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\]</span></p>
</section>
<section id="recall" class="level5" data-number="10.3.7.1.3">
<h5 data-number="10.3.7.1.3" class="anchored" data-anchor-id="recall"><span class="header-section-number">10.3.7.1.3</span> Recall</h5>
<p>Recall можно перевести как «полнота», хотя по сути это снова «точность». Эта метрика показывается <strong>долю предсказанных единиц из всех единиц датасета</strong>, то есть сколько из всех единиц датасета модель предсказала верно:</p>
<p><span class="math display">\[
\text{recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]</span></p>
<p>Очевидно, что чем ближе показатели всех метрик к единице, тем качество модели выше.</p>
</section>
<section id="f1-мера" class="level5" data-number="10.3.7.1.4">
<h5 data-number="10.3.7.1.4" class="anchored" data-anchor-id="f1-мера"><span class="header-section-number">10.3.7.1.4</span> F1-мера</h5>
<p>На основе precision и recall вычисляется ещё одна метрика качества, которая является гармоническим средним этих двух метрик.</p>
<p><span class="math display">\[
\text{F1} = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
\]</span></p>
</section>
</section>
<section id="roc-auc" class="level4" data-number="10.3.7.2">
<h4 data-number="10.3.7.2" class="anchored" data-anchor-id="roc-auc"><span class="header-section-number">10.3.7.2</span> ROC-AUC</h4>
<p>Все описанные выше метрики качестве основывались на confusion matrix — их значения будут зависеть от выбранного порога при переводе непрерывных предсказаний в дискретные. А можно как-то без это зависимости?</p>
<p>Можно. Напоследок ещё одна метрика качества модели. Однако логика её расчёта не такая простая.</p>
<p>Чем хороша эта метрика? Тем, что она работает не с предсказанными значениями (0 и 1), а с предсказанными вероятностями. Таким образом, она избавляет нас от вмешательства <em>нас же</em> в предсказания, ведь когда мы выбираем порог, мы хотя и опираемся на какое-то содержательное основание, тем не менее, глобально выбираем его условно произвольно.</p>
<p>Что рассчитать значение этой метрики нужно сделать следующее (разберём на некотором вымышленном примере):</p>
<ol type="1">
<li>Упорядочить объекты по убыванию значения предсказанной вероятности:</li>
</ol>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">№ наблюдения</th>
<th style="text-align: center;">Вероятность</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.90</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr class="odd">
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr class="even">
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.32</td>
</tr>
<tr class="odd">
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.20</td>
</tr>
<tr class="even">
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.15</td>
</tr>
</tbody>
</table>
<ol start="2" type="1">
<li>Добавить столбец истинных значений (0 и 1)</li>
</ol>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;">№ наблюдения</th>
<th style="text-align: center;">Вероятность</th>
<th style="text-align: center;">Значение</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">7</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: center;">9</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Отметим, что если модель идеально справляется с предказаниями, то в упорядоченном по значению предсказанной вероятности наборе данных сначала будут идти все наблюдения с истинным значением <span class="math inline">\(1\)</span>, а потом с <span class="math inline">\(0\)</span>.</p>
<ol start="3" type="1">
<li>Построим кривую, которая будет описывать качество нашей модели:</li>
</ol>
<ul>
<li>Стартуем из точки <span class="math inline">\((0,0)\)</span> и хотим прийти в точку <span class="math inline">\((1,1)\)</span>:</li>
</ul>
<center>
<figure class="figure">
<img src="pics/auc1.jpg" class="figure-img">
</figure>
</center>
<ul>
<li>Ось <span class="math inline">\(y\)</span> делим на равные части, число которых равно количеству <span class="math inline">\(1\)</span> в датасете:</li>
</ul>
<center>
<figure class="figure">
<img src="pics/auc2.jpg" class="figure-img">
</figure>
</center>
<ul>
<li>Ось <span class="math inline">\(x\)</span> делим на равные части, число которых равно количеству <span class="math inline">\(0\)</span> в датасете:</li>
</ul>
<center>
<figure class="figure">
<img src="pics/auc3.jpg" class="figure-img">
</figure>
</center>
<ul>
<li>Идём по нашим данным сверху вниз, и когда встречаем наблюдение со значением <span class="math inline">\(1\)</span>, поднимаемся на графике на одно деление вверх; когда встречаем наблюдение со значением <span class="math inline">\(0\)</span>, сдвигаемся на одно деление вправо.</li>
<li>В итоге получится такая кривая:</li>
</ul>
<center>
<figure class="figure">
<img src="pics/auc4.jpg" class="figure-img">
</figure>
</center>
<p>Каждая точка на этой кривой соответствует некоторому порогу вероятности отсечения объектов. Например, выделенная точка c координатами <span class="math inline">\((\frac{1}{6}), \frac{3}{4})\)</span> соответствует порогу вероятности <span class="math inline">\(0.6\)</span>. Это означает, что если при переходе к предсказаниями модели мы будем использовать порог <span class="math inline">\(0.6\)</span>, то доля True Positive, или True Positive Rate (TRP) окажется равной <span class="math inline">\(\frac{3}{4}\)</span>, а доля False Positive, или False Positive Rate (FPR) окажется равной <span class="math inline">\(\frac{1}{6}\)</span>.</p>
<p>Построенная кривая называется <strong>ROC-кривой (receiver operating characteristic).</strong></p>
<ol start="4" type="1">
<li>Определяем площадь фигуры под ROC-кривой — это и будет <strong>значением метрики ROC-AUC</strong> (AUC — area under a curve).</li>
</ol>
<center>
<figure class="figure">
<img src="pics/auc5.jpg" class="figure-img">
</figure>
</center>
<p>В случае идеального упорядочивания наблюдений по предсказанной вероятности площадь под кривой будет равна единице. Чем ближе значение ROC-AUC к единице, тем модель работает лучше. Значение <span class="math inline">\(0.5\)</span> указывает на то, что модель совсем не ухватывает закономерность и точность её предсказаний на уровне случайного угадывания — ROC-кривая в этом случае будет проходить близко к диагонали <span class="math inline">\([(0,0),(1,1)]\)</span>.</p>
</section>
</section>
</section>
<section id="пуассоновская-регрессия" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="пуассоновская-регрессия"><span class="header-section-number">10.4</span> Пуассоновская регрессия</h2>
<section id="счетные-данные" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="счетные-данные"><span class="header-section-number">10.4.1</span> Счетные данные</h3>
<p>В жизни и практике мы часто сталкиваемся с так называемыми счётными величинами. Например, число комнат в квартире, количество детей в семье, число книг на полке, число людей, прошедших через турникет и т.д. Глобально — любые количества. Также как счётные величины можно рассмотреть шкалу Лайкерта или оценки по десятибалльной шкале — по сути, это количество набранных баллов<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>Какими свойствами обладают такие величины?</p>
<ul>
<li>Они могут принимать только неотрицательные целочисленные значения (<span class="math inline">\(x_i \in \mathbb{N}_{0}\)</span>)</li>
<li>Разброс значений зависит от среднего значения (<span class="math inline">\(\text{var}(X) \propto \mathbb{E}(X)\)</span>)</li>
</ul>
<p>Общие линейные модели, строго говоря, применимые только к непрерывным величинам1. Иногда свойства данных позволяют использовать такие методы для моделирования счётных величин, однако так бывает далеко не всегда.</p>
<p>Здесь мы обсудим самый простой подход к моделированию счетных величин, а также одну его модификацию, которая может быть полезна.</p>
<p>Модели для счётных данных базируются на распределении Пуассона.</p>
</section>
<section id="распределение-пуассона" class="level3" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="распределение-пуассона"><span class="header-section-number">10.4.2</span> Распределение Пуассона</h3>
<p>Определяется следующим образом:</p>
<p><span class="math display">\[
Y \thicksim \text{Poisson} (\mu)
\]</span></p>
<p><span class="math display">\[
f(y) = \frac{\mu^y e - \mu}{y!}
\]</span></p>
<p>Единственный его параметр — это <span class="math inline">\(\mu\)</span>. Он задает и математические ожидание, и дисперсию:</p>
<p><span class="math display">\[
\begin{split}
\mathbb{E}(Y) = \mu \\
\text{var}(Y) = \mu
\end{split}
\]</span></p>
<p>В зависимости от того, какие значения принимает этот параметр, распределение принимает достаточно сильно различающиеся формы. При малых значениях (<span class="math inline">\(\mu = 1\)</span>, <span class="math inline">\(\mu = 2\)</span>) в распределении присутствует сильная левосторонняя асимметрия, при больших (<span class="math inline">\(\mu = 10\)</span>) — распределение становится симметричным и очень похожим на нормальное распределение.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="l10_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Необходимо отметить, что пуассоновское распределение предполагает, что дисперсия связана с математическим ожидание через <em>функцию идентичности</em>, то есть с увеличением математического ожидания дисперсия возрастает ровно так же, как и само математичекое ожидание. Это факт нам будет важен далее.</p>
</section>
<section id="почему-общие-линейные-модели-плохо-работают" class="level3" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="почему-общие-линейные-модели-плохо-работают"><span class="header-section-number">10.4.3</span> Почему общие линейные модели плохо работают?</h3>
<p>Два свойства счетны случайных величин, оговоренные в самом начале раздела, объясняют, почему общие линейные модели плохо работают (или, по крайне мере, могут плохо работать) на счетных данных.</p>
<p>Первое свойство нам говорит о том, что количество не может быть отрицательным, и это логично. Обчная линейнная регрессия не имеет подобных ограничений, поэтому в предсказаниях будут появляться отрицательные значения. Что с ними делать не очень понятно.</p>
<p>Второе свойство говорит нам о том, что <em>изначально не будет выполнено допущение гомоскедастичности остатков</em>, так как чем выше математичекое ожидание, тем выше дисперсия. В итоге мы будем наблюдать воронкообразный паттер в распределении остатков.</p>
<p>Таким образом, оценки коэффициентов модели будут неточны, ошибки завышены, а следовательно, и результатам тестирования статистической значимости параметров модели доверять нельзя.</p>
<p>Что делать? Можно пойти простым путём — <em>логарифмировать целевую переменную</em> и построить модель для получившейся величины.</p>
<p>Но более корректным вариантом будет построить модель, основанную на распределении, подходящем для счётных данных. В частности, пуассоновском распределении.</p>
</section>
<section id="математическая-модель-1" class="level3" data-number="10.4.4">
<h3 data-number="10.4.4" class="anchored" data-anchor-id="математическая-модель-1"><span class="header-section-number">10.4.4</span> Математическая модель</h3>
<p>Так как мы находимся во фреймворке обобщенных линейных моделей, значит ключевой момент с формулировании модели — <strong>функция связи</strong>. У пуассоновской регрессии она очень проста — это всего лишь логарифм:</p>
<p><span class="math display">\[
\eta_i = \ln (y_i)
\]</span></p>
<p>Применяя её к нашей переменной, мы получаем величину <span class="math inline">\(\eta_i\)</span>, которую можем моделировать линейной моделью. Соответственно, идентифицируемая модель будет такой же, как и прежде:</p>
<p><span class="math display">\[
\eta_i = \hat \beta_0 + \hat \beta_1 x_{i1} + \hat \beta_2 x_{i2} + \dots + \hat \beta_p x_{ip}
\]</span></p>
</section>
<section id="результаты-моделирования" class="level3" data-number="10.4.5">
<h3 data-number="10.4.5" class="anchored" data-anchor-id="результаты-моделирования"><span class="header-section-number">10.4.5</span> Результаты моделирования</h3>
<p>Абсолютно аналогично биномиальной регрессии для пуассоновской будет:</p>
<ul>
<li>отсутствовать <span class="math inline">\(F\)</span>-статистика</li>
<li>отсутствовать <span class="math inline">\(R^2\)</span></li>
<li>необходим анализ девиансы</li>
<li>значимость коэффициентов тестироваться z-тестами Вальда</li>
<li>необходима проверка на сверхдисперсию</li>
</ul>
<p>Так как в биномиальной регрессии мы обсудили это достаточно подробно, не будем на этом останавливаться.</p>
</section>
<section id="квазипуассоновские-модели" class="level3" data-number="10.4.6">
<h3 data-number="10.4.6" class="anchored" data-anchor-id="квазипуассоновские-модели"><span class="header-section-number">10.4.6</span> Квазипуассоновские модели</h3>
<p>Однако по сравнению с биномиальной регрессией у пуассоновской есть одна особенность — а именно, возможность работать со сверхдисперсией.</p>
<p>Так как пуассоновская модель исходит из предположения равенства дисперсии и математического ожидания, если мы не обнаруживаем сверхдисперсии, то все хорошо. Если же это допущение не выполнено, то</p>
<ul>
<li>оценки стандартных ошибок коэффициентов будут занижены</li>
<li>тесты Вальда для коэффициентов модели дадут неправильные результаты — из-за того, что оценки стандартных ошибок занижены— уровень значимости будет занижен</li>
<li>тесты, основанные на сравнении правдоподобий, дадут смещенные результаты, так как соотношение девианс уже не будет подчиняться распределению <span class="math inline">\(\chi^2\)</span></li>
</ul>
<p>Происходят все эти проблемы из-за того, что для распределения Пуассона справедливы следующие соотношения:</p>
<p><span class="math display">\[
\begin{split}
&amp; \text{var}(y_i) = \mu_i \\
&amp; \text{var}(\mathbb{E}(y_i)) = \frac{\mu}{n} \\
&amp; \text{se}_{\mathbb{E}(y_i)} = \sqrt{\text{var}\big( \mathbb{E}(y_i) \big)}
\end{split}
\]</span></p>
<p>Если обнаружена сверхдисперсия, то данные не подчиняются распределению Пуассона, и дисперсия в <span class="math inline">\(\phi\)</span> раз больше среднего (<span class="math inline">\(\phi &gt; 1\)</span>). Тогда,</p>
<p><span class="math display">\[
\begin{split}
&amp; \text{var}(y_i) = \phi \mu_i \\
&amp; \text{var}(\mathbb{E}(y_i)) = \frac{\phi \mu}{n} \\
&amp; \text{se}_{\mathbb{E}(y_i)} = \sqrt{\phi \text{var}\big( \mathbb{E}(y_i) \big)}
\end{split}
\]</span></p>
<p>Каковы могут быть причины сверхдисперсии? Перечислим основные:</p>
<ul>
<li>в данных есть выбросы</li>
<li>в модель не включен важный предиктор или взаимодействие предикторов</li>
<li>нарушена независимость выборок (есть внутригрупповые корреляции)</li>
<li>выбрана неподходящая функция связи</li>
<li>выбрана неподходящая функция распределения для целевой переменной</li>
</ul>
<p>А что же делать? Пусти есть разные. Мы рассмотрим наболее простой — построение <strong>квазипуассоновских моделей</strong>.</p>
<p>Не стоит думать, что существует «квази-пуассоновское» распределение —- эти модели также основываются на распределение Пуассона, но учитывают тот самый коэффициет <span class="math inline">\(\phi\)</span>, описывающий сверхдисперсию. Это, по сути, поправка на сверхдисперсию.</p>
<p>Сама модель не меняется по сравнению с пуассоновской регрессией, коэффициент <span class="math inline">\(\phi\)</span> оценивается по данным. Кроме того, оценки параметром пуассоновских и квазипуассоновских моделей совпадают, однако в квазипуассоновских моделях:</p>
<ul>
<li>стандартные ошибки коэффициентов домножатся на <span class="math inline">\(\sqrt{\phi}\)</span></li>
<li>доверительные интервалы коэффициентов домножаются на <span class="math inline">\(\sqrt{\phi}\)</span></li>
<li>логарифмы правдоподобий, используемые для тестирования статистической значимости моделей, уменьшаются в <span class="math inline">\(\phi\)</span> раз</li>
</ul>
<p>А так как тестирование статистической значимости работает со стандартными ошибками, то и статистическая значимость также изменяется.</p>
<p>Также есть отличия и в статистических тестах:</p>
<ul>
<li>для анализа девиансы используются F-тесты</li>
<li>для тестирования эначимости коэффициентов используются t-тесты</li>
</ul>
<p>Это ключевые отличия квазипуассоновских моделей от пуассоновских — в остальном они полностью вписываются во фреймворк GLM.</p>


</section>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>А <a href="https://mathworld.wolfram.com/BernoulliDistribution.html">распределению Бернулли</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Хотя тут можно поспорить.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Скопировано!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Скопировано!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./l9.html" class="pagination-link" aria-label="L9 // Дисперсионный анализ. Ковариационный анализ">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">L9 // Дисперсионный анализ. Ковариационный анализ</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./l12.html" class="pagination-link" aria-label="L12 // Линейные модели со смешанными эффектами">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">L12 // Линейные модели со смешанными эффектами</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>