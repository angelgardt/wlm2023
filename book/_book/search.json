[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WLM 2023",
    "section": "",
    "text": "Вступление",
    "crumbs": [
      "Вступление"
    ]
  },
  {
    "objectID": "index.html#book_structure",
    "href": "index.html#book_structure",
    "title": "WLM 2023",
    "section": "Структура книги",
    "text": "Структура книги",
    "crumbs": [
      "Вступление"
    ]
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "WLM 2023",
    "section": "Благодарности",
    "text": "Благодарности\n\n\nSession info\n\nsessionInfo()\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.2 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=ru_RU.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=ru_RU.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=ru_RU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=ru_RU.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.1.2    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.1.2       htmltools_0.5.6   rstudioapi_0.15.0 rmarkdown_2.25   \n [9] knitr_1.44        jsonlite_1.8.7    xfun_0.40         digest_0.6.33    \n[13] rlang_1.1.1       evaluate_0.22",
    "crumbs": [
      "Вступление"
    ]
  },
  {
    "objectID": "signs.html#book_labels",
    "href": "signs.html#book_labels",
    "title": "Обозначения",
    "section": "Лейблы",
    "text": "Лейблы\nВ каждой главе на разделах и подразделах стоят лейблы, которые маркируют уровень сложности материала:\n\n\n\nДля начинающих и тех, кто пока не чувствует себя уверенно в самых основах.\n\n\n\nДля тех, кто разобрался с основами и хочет пойти дальше.\n\n\n\nДля тех, кто уверенно себя чувствует в материале и хочет разбираться в деталях.\n\n\n\nДля тех, кто преисполнился и хочет жести.",
    "crumbs": [
      "Обозначения"
    ]
  },
  {
    "objectID": "signs.html#book_callouts",
    "href": "signs.html#book_callouts",
    "title": "Обозначения",
    "section": "Блоки",
    "text": "Блоки\n\n\n\n\n\n\nЗаметка\n\n\n\nЗдесь написано что-то интересное. Почитайте на досуге.\n\n\n\n\n\n\n\n\nВажно\n\n\n\nТут написано что-то важное. Обратите на этот блок внимание.\n\n\n\n\n\n\n\n\nCaution with Title\n\n\n\nТут описано что-то, что потенциально может сломать ваш код. Чтобы он не сломался, стоит это изучить.\n\n\n\n\n\n\n\n\nВажно!\n\n\n\nТут написано что-то очень важное! Обратите на этот блок пристальное внимание!\n\n\n\n\n\n\n\n\nСовет\n\n\n\nЗдесь советы из собственного опыта. Возможно, они вам пригодятся.",
    "crumbs": [
      "Обозначения"
    ]
  },
  {
    "objectID": "signs.html#book_tasks",
    "href": "signs.html#book_tasks",
    "title": "Обозначения",
    "section": "Задания",
    "text": "Задания\n\nЭто вопрос, который позволит вспомнить предыдущий материал или немного глубже копнуть в текущую тему. Как правило, имеющихся знаний будет достаточно для ответа на него, но иногда придётся и погуглить.\n\n\nЭто задание, где нужно что-то посчитать. Можно с помощью кода, а можно и руками.\n\n\nЭто задание, в котором надо написать код или посчитать что-то с помощью кода.",
    "crumbs": [
      "Обозначения"
    ]
  },
  {
    "objectID": "signs.html#book_symbols",
    "href": "signs.html#book_symbols",
    "title": "Обозначения",
    "section": "Символы",
    "text": "Символы\n\n\n\n\n\n\n\n\n\n\\(\\mathbb{N}\\) — множество натуральных чисел\n\\(\\mathbb{N}_{0}\\) — множество натуральных чисел, включая ноль\n\\(\\mathbb{Z}\\) — множество целых чисел\n\\(\\mathbb{Q}\\) — множество рациональных чисел\n\\(\\mathbb{R}\\) — множество вещественных (действительных) чисел\n\\(\\mathbb{C}\\) — множество комплексных чисел\n\\(\\alpha, \\beta, \\gamma, \\dots\\) — числа, коэффициенты\n\\(X, Y, Z\\) — случайные величины\n\\(x, y, z\\) — числа, значения случайной величины\n\\(\\mathbf{a}, \\mathbf{b}, \\mathbf{c}, \\dots, \\mathbf{x}, \\mathbf{y}, \\mathbf{z}\\) — векторы\n\\(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\dots, \\mathbf{X}, \\mathbf{Y}, \\mathbf{Z}\\) — матрицы\n\\(\\mathbb{P}\\) — вероятность\n\\(\\sum\\) — сумма\n\\(\\prod\\) — произведение\n\\(\\overset{\\text{def}}{=}\\) — равно по определению",
    "crumbs": [
      "Обозначения"
    ]
  },
  {
    "objectID": "r-basics.html#rbasics-rinstall",
    "href": "r-basics.html#rbasics-rinstall",
    "title": "1  Основы R",
    "section": "1.1 Установка",
    "text": "1.1 Установка\n\n\n\nЧтобы стать счастливым пользователем R, надо установить на свой комп две программы:\n\nсобственно R\n\nна Win\nна Mac\nна Linux\n\nIDE RStudio1\n\nПричем во избежание возможных проблем, надо поставить программы именно в этом порядке — сначала R, а потом RStudio, иначе IDE может на найти R и будет ругаться.\n\n1.1.1 Зачем ставить две проги?\nВопрос не безосновательный. В целом, можно обойти и только R — можно работать на нём хоть из командной строки — однако это всё же не совсем удобно. Интерфейс самого R довольно скудный и не слишком приветливый.\n\n\n\n\nИнтерфейс R\n\n\n\nRStudio же, являясь, как уже вскользь отмечалось выше, интегрированной средой разработки (IDE), расширяет возможности R, предоставляет более юзабельный интерфейс для взаимодействия с языком и в целом делает работу с R радостной и приятной. С её интерфейсом и возможностями мы познакомимся далее.\nRStudio это не единственная среда для работы с R, но определенно самая удобная и популярная, поэтому мы будем пользоваться именно ею. RStudio является IDE, разработанной специально для работы в R, однако это вовсе не значит, что в ней нельзя использовать другие языки программирования. Например, книжка, которуя вы сейчас читаете, написана с использованием R, Python, HTML, SASS, JavaScript, YAML и других языков — при этом вся работа велась в RStudio. Вот такая мощная вещь.\n\n\n1.1.2 Что такое IDE?\nИнтегрированная среда разработки (IDE, integrated development environment) — это специальная программа, которое предоставляет широкий спектр возможностей для разработки программного обеспечения. Возможно, вы слышали такие слова, как PyCharm или Visual Studio Code — это всё варианты IDE.\nОбычно IDE содержит несколько ключевых компонентов:\n\nтекстовый редактор для написания скриптов\nтранслятор языка\nотладчик (debugger)\nсредства автоматизации сборки (build automation tools)\n\nОбычно IDE позволяют работать с несколькими языками программирования (ЯП), но бывают и специализированные.\n\nИ хотя всё ещё присутствует холивар относительного того, является ли R языком программирования (ССЫЛКА), RStudio однозначно можно назвать полноценной IDE, так как разработка в ней вполне может вестить. Пример продукта разработки прямо перед вами — книжка и другие материалы курса.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основы R</span>"
    ]
  },
  {
    "objectID": "r-basics.html#rbasics-installation-problems",
    "href": "r-basics.html#rbasics-installation-problems",
    "title": "1  Основы R",
    "section": "1.2 Подводные камни установки",
    "text": "1.2 Подводные камни установки\n\n\n\nВ целом, установщики операционных систем обычно хорошо справляются со своей задачей, и в 90% случаев всё стаёт без багов. Однако ниже я оставлю некоторые комментарии о проблемах, с которыми сталкивался сам или о которых говорили знакомые и коллеги.\n\n1.2.1 Windows\nСамая частая проблема — имя пользователя на кириллице. Компьютер вообще достаточно плохо переваривает кириллические символы. Особенную же проблему составляют такие символы в путях к файлам. Поскольку на Винде папка пользователя называется именем пользователя, то в случае киррилического имени, естественно, её имя будет на кириллице. Это можно пережить, перезадав некоторое дефолтные пути в настройках, однако если есть возможность переименовать пользователя и папку, я бы рекомендовал это сделать. Ну, так, чтобы не было неожиданных внезапностей.\n\n\n1.2.2 MacOS\nТут в 99.9% случаев всё ровно. Бывает уже в процессе работы некоторые пакеты жалуются на недоустановленное что-то или на какие-либо несовместимости, но это случается невероятно редко и обычно достаточно легко лечится.\n\n\n1.2.3 Linux\nЕсли вы пользователь Linux, значит R с RStudio вы ставили через Terminal. Скорее всего, всё прошло хорошо, и всё работает. Проблемки могут случиться чуть дальше, когда мы будем ставить дополнительные аровские пакеты, в которых будет идти основная наша работа — R может не найти некоторые системные пакеты. Такая проблема у меня возникла на Ubuntu 22.04 — помогла команда ниже:\nsudo apt install r-base-dev\nsudo apt-get install -y libxml2-dev libcurl4-openssl-dev libssl-dev libfontconfig1-dev libharfbuzz-dev libfribidi-dev linfreetype6-dev libpng-dev libtiff5-dev libjpeg-dev\nСначала мы будем знакомиться с базовым R и работать только в нём, но имейте в виду, что тут есть некая команда, которая может пригодиться.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основы R</span>"
    ]
  },
  {
    "objectID": "r-basics.html#rbasics-r-about",
    "href": "r-basics.html#rbasics-r-about",
    "title": "1  Основы R",
    "section": "1.3 Про R",
    "text": "1.3 Про R\n\n\n\n\n1.3.1 Откуда оно взялось\nR — популярный язык программирования среди исследователей в социальных и гуманитарных науках. Если совсем коротко, то начиналось всё с языка S, который был языком программирования для статистического анализа. Потом его доработали и получился R.\nХотя сегодня всё ещё можно услышать, что «R — это язык программирования для статистической обработки данных», это ложь. Да, когда-то давно дела обстояли именно так, но сейчас R — это полноценный язык программирования, который позволяет решать широкий спект задач от статистического анализа и data wrangling до машинного обучения, моделирования и создания сайтов и приложений.\nКонечно, с точки зрения сурового программирования к R есть некоторые вопросики(ССЫЛКА).\n\n\n1.3.2 Почему R?\nВ сравнении с «классическим» ПО для анализа данных (типа SPSS) R круче, потому что это:\n\nсвободное ПО (часть GNU Project)\n\nбесплатно, без смс и регистрации\n\nдинамично развивающаяся среда\n\nобновляются функции, развивается IDE\n\nгромадные возможности расширения функционала\n\nболее 20 000 пакетов\n\nмногие написаны специально для какой-либо профессиональной области — например, есть пакеты для биологов, психологов, лингвистов…\n\n\nоткрытый исходный код\n\nможно посмотреть «мясо» функций и понять, как они работают изнутри — нет проблемы «черного ящика»\n\nвозможность написать свои пакеты\n\nи сделать аналитических мир лучше\n\nбольшое сообщество по всему миру, много ресурсов для задавания вопросов — даже просто загуглив какой-либо вопрос или ошибку, вы, скорее всего, попадете на один из перечисленных ниже ресурсов\n\nStack Overflow — крайне полезный ресурс с ответами на вопросы, и не только по R. Есть версия на русском.\nStack Exchange — подобен ресурсу выше, но спектр вопросов еще шире — тут можно найти и что-то про математику, и про методы анализа данных, и ещё мого чего\nPosit Community — форум с вопросами и ответами про R\nR-bloggers — про новинки в R и рядом с ним\nХабр про R\n…\n\nLinear Warriors vs Quadratic Wizards\n\nв SPSS (и другие GUI2 пакеты) ниже порог вхождения, но развитие навыков — линейное\n\nмы просто находим новые кнопки, на которые можно понажимать и что-то получить\n\nв R порог вхождения выше, но впоследствии случается резкий буст, и вы становитесь богами дата-аналитики\n\nчем больше методов анализа мы изучаем, тем глубже мы погружаемся в сам язык и его пакеты, а необходимость сделать так, чтобы все работало, стимулирует нам глубже разобраться в сами методах, чтобы код не падал там, где не надо\n\n\n\n\n\n\n\n\n\nИсточник\n\n\n\n\nвозможность рисовать красивые картинки\n\nа также создавать интерктивные отчеты, книжки, сайты, блоги, дашборды…\n\nрепродуцируемость результатов\n\nибо с кризисом воспроизводимости приходится всячески бороться\n\n\n\n\n1.3.3 R vs Python\n\n— Я вот не могу выбрать: делать на R или на Python? — Да какая разница! Главное — делай!\n\n\n\n\n\nИсточник",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основы R</span>"
    ]
  },
  {
    "objectID": "r-basics.html#rbasics-rstudio-interface",
    "href": "r-basics.html#rbasics-rstudio-interface",
    "title": "1  Основы R",
    "section": "1.4 Интерфейс RStudio",
    "text": "1.4 Интерфейс RStudio\n\n\n\n\n\n\n\nИнтерфейс RStudio",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основы R</span>"
    ]
  },
  {
    "objectID": "r-basics.html#rbasics-updates",
    "href": "r-basics.html#rbasics-updates",
    "title": "1  Основы R",
    "section": "1.5 Обновления",
    "text": "1.5 Обновления\n\n1.5.1 Обновление RStudio\n\n\n1.5.2 Обновление R\n\n\n1.5.3 Обновление пакетов\nОшибки has non-zero exit status\nПроблема обратной совместимости: функции иногда переезжают из пакета в пакет без предупреждения",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основы R</span>"
    ]
  },
  {
    "objectID": "r-basics.html#rbasics-r-calculator",
    "href": "r-basics.html#rbasics-r-calculator",
    "title": "1  Основы R",
    "section": "1.6 R как калькулятор",
    "text": "1.6 R как калькулятор",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основы R</span>"
    ]
  },
  {
    "objectID": "r-basics.html#rbasics-vars-and-objects",
    "href": "r-basics.html#rbasics-vars-and-objects",
    "title": "1  Основы R",
    "section": "1.7 Assignment. Переменные и объекты",
    "text": "1.7 Assignment. Переменные и объекты",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Основы R</span>"
    ]
  },
  {
    "objectID": "r-dtypes.html#rdtypes-numbers",
    "href": "r-dtypes.html#rdtypes-numbers",
    "title": "2  Типы данных",
    "section": "2.1 Числовые данные",
    "text": "2.1 Числовые данные\n\n\n\n\n2.1.1 Целые числа\n\n\n\n\n\n2.1.2 Числа с плавающей точкой\n\n\n\n\n\n2.1.3 Комплексные числа",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Типы данных</span>"
    ]
  },
  {
    "objectID": "r-dtypes.html#rdtypes-logic",
    "href": "r-dtypes.html#rdtypes-logic",
    "title": "2  Типы данных",
    "section": "2.2 Логические данные",
    "text": "2.2 Логические данные",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Типы данных</span>"
    ]
  },
  {
    "objectID": "r-dtypes.html#rdtypes-strings",
    "href": "r-dtypes.html#rdtypes-strings",
    "title": "2  Типы данных",
    "section": "2.3 Строковые данные",
    "text": "2.3 Строковые данные",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Типы данных</span>"
    ]
  },
  {
    "objectID": "r-dtypes.html#rdtypes-factors",
    "href": "r-dtypes.html#rdtypes-factors",
    "title": "2  Типы данных",
    "section": "2.4 Факторы",
    "text": "2.4 Факторы",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Типы данных</span>"
    ]
  },
  {
    "objectID": "r-dtypes.html#rdtypes-na",
    "href": "r-dtypes.html#rdtypes-na",
    "title": "2  Типы данных",
    "section": "2.5 NA, NaN, NULL",
    "text": "2.5 NA, NaN, NULL",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Типы данных</span>"
    ]
  },
  {
    "objectID": "r-dstructs.html#rdstructs-vectors",
    "href": "r-dstructs.html#rdstructs-vectors",
    "title": "3  Структуры данных",
    "section": "3.1 Векторы",
    "text": "3.1 Векторы\n\n\n\n\n3.1.1 Создание векторов\n\n\n3.1.2 Генерация числовых последовательностей\n\n\n3.1.3 Индексация векторов\n\n\n3.1.4 Операции над векторами",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Структуры данных</span>"
    ]
  },
  {
    "objectID": "r-dstructs.html#rdstructs-matrices",
    "href": "r-dstructs.html#rdstructs-matrices",
    "title": "3  Структуры данных",
    "section": "3.2 Матрицы",
    "text": "3.2 Матрицы\n\n\n\n\n3.2.1 Создание матриц\n\n\n3.2.2 Индексация матриц\n\n\n3.2.3 Операции над матрицами",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Структуры данных</span>"
    ]
  },
  {
    "objectID": "r-dstructs.html#rdstructs-arrays",
    "href": "r-dstructs.html#rdstructs-arrays",
    "title": "3  Структуры данных",
    "section": "3.3 Массивы",
    "text": "3.3 Массивы",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Структуры данных</span>"
    ]
  },
  {
    "objectID": "r-dstructs.html#rdstructs-lists",
    "href": "r-dstructs.html#rdstructs-lists",
    "title": "3  Структуры данных",
    "section": "3.4 Списки",
    "text": "3.4 Списки\n\n\n\n\n3.4.1 Создание списков\n\n\n3.4.2 Индексация списков\n\n\n3.4.3 Операции над списками\nподумать, куда это деть, мб вынести в функции или куда-то еще, потому что тут просится map()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Структуры данных</span>"
    ]
  },
  {
    "objectID": "r-dstructs.html#rdstructs-dataframes",
    "href": "r-dstructs.html#rdstructs-dataframes",
    "title": "3  Структуры данных",
    "section": "3.5 Датафреймы",
    "text": "3.5 Датафреймы\n\n\n\n\n3.5.1 Датафрейм как производное матрицы и списка\n\n\n3.5.2 Создание датафреймов\n\n\n3.5.3 Индексация датафреймов",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Структуры данных</span>"
    ]
  },
  {
    "objectID": "r-funcs.html#rfuncs-concept",
    "href": "r-funcs.html#rfuncs-concept",
    "title": "4  Функции",
    "section": "4.1 Концепт функции",
    "text": "4.1 Концепт функции\n\n4.1.1 Окружение и область видимости переменных",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Функции</span>"
    ]
  },
  {
    "objectID": "r-funcs.html#rfuncs-built",
    "href": "r-funcs.html#rfuncs-built",
    "title": "4  Функции",
    "section": "4.2 Встроенные функции",
    "text": "4.2 Встроенные функции",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Функции</span>"
    ]
  },
  {
    "objectID": "r-funcs.html#rfuncs-custom",
    "href": "r-funcs.html#rfuncs-custom",
    "title": "4  Функции",
    "section": "4.3 Создание функций",
    "text": "4.3 Создание функций\nКогда надо создавать функцию?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Функции</span>"
    ]
  },
  {
    "objectID": "r-controlflow.html#controlflow-forwhile",
    "href": "r-controlflow.html#controlflow-forwhile",
    "title": "5  Управляющие конструкции",
    "section": "5.1 Циклы",
    "text": "5.1 Циклы\n\n5.1.1 for\n\n\n5.1.2 while",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "r-controlflow.html#controlflow-if",
    "href": "r-controlflow.html#controlflow-if",
    "title": "5  Управляющие конструкции",
    "section": "5.2 Условный оператор",
    "text": "5.2 Условный оператор",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Управляющие конструкции</span>"
    ]
  },
  {
    "objectID": "r-strings.html#rstrings-packages",
    "href": "r-strings.html#rstrings-packages",
    "title": "7  Строки",
    "section": "7.1 Установка дополнительных пакетов",
    "text": "7.1 Установка дополнительных пакетов\n\n7.1.1 Установка с CRAN\n\n\n7.1.2 Установка с недефолтных зеркал\n\n\n7.1.3 Установка с GitHub",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Строки</span>"
    ]
  },
  {
    "objectID": "r-strings.html#rstrings-manipulations",
    "href": "r-strings.html#rstrings-manipulations",
    "title": "7  Строки",
    "section": "7.2 Манипуляции со строками",
    "text": "7.2 Манипуляции со строками\n\n7.2.1 Создание строк\n\n\n7.2.2 Конкатенация строк\n\n\n7.2.3 Разделение строк\n\n\n7.2.4 Сортировка строк\n\n\n7.2.5 Изменение регистра\n\n\n7.2.6 Поиск подстроки\n\n\n7.2.7 Извлечение подстроки\n\n\n7.2.8 Замена подстроки",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Строки</span>"
    ]
  },
  {
    "objectID": "r-strings.html#rstrings-regex",
    "href": "r-strings.html#rstrings-regex",
    "title": "7  Строки",
    "section": "7.3 Регулярные выражения",
    "text": "7.3 Регулярные выражения\nhttps://rpubs.com/iPhuoc/stringr_manipulation\n\n7.3.1 Начало и конце строки\n\n\n7.3.2 Классы символов\n\n\n7.3.3 Повторения",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Строки</span>"
    ]
  },
  {
    "objectID": "r-dates.html#rdates-storing",
    "href": "r-dates.html#rdates-storing",
    "title": "8  Дата и время",
    "section": "8.1 Способы записи информации о дате и времени",
    "text": "8.1 Способы записи информации о дате и времени\ntimestamp\n\n8.1.1 Форматы даты и времени\n\n\n8.1.2 Часовые пояса",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Дата и время</span>"
    ]
  },
  {
    "objectID": "r-dates.html#rdates-fileinfo",
    "href": "r-dates.html#rdates-fileinfo",
    "title": "8  Дата и время",
    "section": "8.2 Информация о файле",
    "text": "8.2 Информация о файле",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Дата и время</span>"
    ]
  },
  {
    "objectID": "r-dates.html#rdates-manipulations",
    "href": "r-dates.html#rdates-manipulations",
    "title": "8  Дата и время",
    "section": "8.3 Манипуляции с датой и временем",
    "text": "8.3 Манипуляции с датой и временем\n\n8.3.1 Извлечение части даты\n\n\n8.3.2 Округление дат\n???\n\n\n8.3.3 Разница между датами",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Дата и время</span>"
    ]
  },
  {
    "objectID": "math-logic.html#math-logic-utterance",
    "href": "math-logic.html#math-logic-utterance",
    "title": "12  Элементы алгебры логики",
    "section": "12.1 Высказывания",
    "text": "12.1 Высказывания\n\n12.1.1 Атомарные высказывания",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Элементы алгебры логики</span>"
    ]
  },
  {
    "objectID": "math-logic.html#math-logic-orepation",
    "href": "math-logic.html#math-logic-orepation",
    "title": "12  Элементы алгебры логики",
    "section": "12.2 Сложные высказывания и логические операции",
    "text": "12.2 Сложные высказывания и логические операции\n\n12.2.1 Конъюнкция\n\n\n12.2.2 Дизъюнкция\n\n\n12.2.3 XOR",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Элементы алгебры логики</span>"
    ]
  },
  {
    "objectID": "math-logic.html#math-logic-conditions",
    "href": "math-logic.html#math-logic-conditions",
    "title": "12  Элементы алгебры логики",
    "section": "12.3 Условные высказывания",
    "text": "12.3 Условные высказывания\n\n12.3.1 Импликация\n\n\n12.3.2 Репликация\n\n\n12.3.3 Эквиваленция",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Элементы алгебры логики</span>"
    ]
  },
  {
    "objectID": "math-settheory.html#math-settheory-set",
    "href": "math-settheory.html#math-settheory-set",
    "title": "13  Элементы теории множеств",
    "section": "13.1 Множества и подмножества",
    "text": "13.1 Множества и подмножества\n\n13.1.1 Принадлежность элемента множеству\n\n\n13.1.2 Включение",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Элементы теории множеств</span>"
    ]
  },
  {
    "objectID": "math-settheory.html#math-settheory-operations",
    "href": "math-settheory.html#math-settheory-operations",
    "title": "13  Элементы теории множеств",
    "section": "13.2 Операции над множествами",
    "text": "13.2 Операции над множествами\n\n13.2.1 Пересечение\n\n\n13.2.2 Объединение\n\n\n13.2.3 Дополнение\n\n\n13.2.4 Разность\n\n\n13.2.5 Декартово произведение\n\n13.2.5.1 Координатная плоскость\n\n\n\n13.2.6 Power set\nкак это называется по-русски????\n\n\n13.2.7 Разбиение множества",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Элементы теории множеств</span>"
    ]
  },
  {
    "objectID": "math-settheory.html#math-settheory-mapping",
    "href": "math-settheory.html#math-settheory-mapping",
    "title": "13  Элементы теории множеств",
    "section": "13.3 Отображения",
    "text": "13.3 Отображения\n\n13.3.1 Инъекция\n\n\n13.3.2 Сюръекция\n\n\n13.3.3 Биекция\n\n\n13.3.4 Отображения и функции",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Элементы теории множеств</span>"
    ]
  },
  {
    "objectID": "math-settheory.html#math-settheory-mapping-spaces",
    "href": "math-settheory.html#math-settheory-mapping-spaces",
    "title": "13  Элементы теории множеств",
    "section": "13.4 Пространства",
    "text": "13.4 Пространства",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Элементы теории множеств</span>"
    ]
  },
  {
    "objectID": "math-combinatorics.html#принцип-сложения.-принцип-умножения",
    "href": "math-combinatorics.html#принцип-сложения.-принцип-умножения",
    "title": "14  Элементы комбинаторики",
    "section": "14.1 Принцип сложения. Принцип умножения",
    "text": "14.1 Принцип сложения. Принцип умножения",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Элементы комбинаторики</span>"
    ]
  },
  {
    "objectID": "math-combinatorics.html#перестановки",
    "href": "math-combinatorics.html#перестановки",
    "title": "14  Элементы комбинаторики",
    "section": "14.2 Перестановки",
    "text": "14.2 Перестановки\n\n\n\n\n14.2.1 Факториал",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Элементы комбинаторики</span>"
    ]
  },
  {
    "objectID": "math-combinatorics.html#размещения",
    "href": "math-combinatorics.html#размещения",
    "title": "14  Элементы комбинаторики",
    "section": "14.3 Размещения",
    "text": "14.3 Размещения\n\n14.3.1 Без повторений\n\n\n\n\n\n14.3.2 С повторениями",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Элементы комбинаторики</span>"
    ]
  },
  {
    "objectID": "math-combinatorics.html#сочетания",
    "href": "math-combinatorics.html#сочетания",
    "title": "14  Элементы комбинаторики",
    "section": "14.4 Сочетания",
    "text": "14.4 Сочетания\n\n14.4.1 Без повторений\n\n\n\n\n\n14.4.2 С повторениями",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Элементы комбинаторики</span>"
    ]
  },
  {
    "objectID": "math-analysis.html#math-analysis-sequence",
    "href": "math-analysis.html#math-analysis-sequence",
    "title": "15  Элементы математического анализа",
    "section": "15.1 Последовательность",
    "text": "15.1 Последовательность\n\n15.1.1 Ряд\n\n\n15.1.2 Частичные суммы ряда",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Элементы математического анализа</span>"
    ]
  },
  {
    "objectID": "math-analysis.html#math-analysis-convergence",
    "href": "math-analysis.html#math-analysis-convergence",
    "title": "15  Элементы математического анализа",
    "section": "15.2 Сходимость последовательности",
    "text": "15.2 Сходимость последовательности",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Элементы математического анализа</span>"
    ]
  },
  {
    "objectID": "math-analysis.html#math-analysis-limit",
    "href": "math-analysis.html#math-analysis-limit",
    "title": "15  Элементы математического анализа",
    "section": "15.3 Предел последовательности",
    "text": "15.3 Предел последовательности",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Элементы математического анализа</span>"
    ]
  },
  {
    "objectID": "math-analysis.html#math-analysis-functions",
    "href": "math-analysis.html#math-analysis-functions",
    "title": "15  Элементы математического анализа",
    "section": "15.4 Функции",
    "text": "15.4 Функции",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Элементы математического анализа</span>"
    ]
  },
  {
    "objectID": "math-analysis.html#math-analysis-deriv",
    "href": "math-analysis.html#math-analysis-deriv",
    "title": "15  Элементы математического анализа",
    "section": "15.5 Дифференцируемость функции. Производная",
    "text": "15.5 Дифференцируемость функции. Производная",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Элементы математического анализа</span>"
    ]
  },
  {
    "objectID": "math-analysis.html#math-analysis-multivariate",
    "href": "math-analysis.html#math-analysis-multivariate",
    "title": "15  Элементы математического анализа",
    "section": "15.6 Функции нескольких переменных",
    "text": "15.6 Функции нескольких переменных",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Элементы математического анализа</span>"
    ]
  },
  {
    "objectID": "math-analysis.html#math-analysis-partialderiv",
    "href": "math-analysis.html#math-analysis-partialderiv",
    "title": "15  Элементы математического анализа",
    "section": "15.7 Частные производные :: middle",
    "text": "15.7 Частные производные :: middle",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Элементы математического анализа</span>"
    ]
  },
  {
    "objectID": "math-analysis.html#math-analysis-integral",
    "href": "math-analysis.html#math-analysis-integral",
    "title": "15  Элементы математического анализа",
    "section": "15.8 Интеграл",
    "text": "15.8 Интеграл\n\n15.8.1 Определенный интеграл\n\n\n15.8.2 Неопределенный интеграл\n\n\n15.8.3 Площадь под графиком функции",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Элементы математического анализа</span>"
    ]
  },
  {
    "objectID": "math-linal.html#матрица-и-вектор",
    "href": "math-linal.html#матрица-и-вектор",
    "title": "16  Элементы линейной алгебры",
    "section": "16.1 Матрица и вектор",
    "text": "16.1 Матрица и вектор",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Элементы линейной алгебры</span>"
    ]
  },
  {
    "objectID": "math-linal.html#матричная-запись-систем-уравнений",
    "href": "math-linal.html#матричная-запись-систем-уравнений",
    "title": "16  Элементы линейной алгебры",
    "section": "16.2 Матричная запись систем уравнений",
    "text": "16.2 Матричная запись систем уравнений",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Элементы линейной алгебры</span>"
    ]
  },
  {
    "objectID": "math-linal.html#действия-над-матрицами",
    "href": "math-linal.html#действия-над-матрицами",
    "title": "16  Элементы линейной алгебры",
    "section": "16.3 Действия над матрицами",
    "text": "16.3 Действия над матрицами\n\n16.3.1 Сложение матриц\n\n\n16.3.2 Умножение матрицы на число",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Элементы линейной алгебры</span>"
    ]
  },
  {
    "objectID": "math-linal.html#скалярное-произведение-векторов",
    "href": "math-linal.html#скалярное-произведение-векторов",
    "title": "16  Элементы линейной алгебры",
    "section": "16.4 Скалярное произведение векторов",
    "text": "16.4 Скалярное произведение векторов",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Элементы линейной алгебры</span>"
    ]
  },
  {
    "objectID": "math-linal.html#произведение-матриц-матричное-умножение",
    "href": "math-linal.html#произведение-матриц-матричное-умножение",
    "title": "16  Элементы линейной алгебры",
    "section": "16.5 Произведение матриц (матричное умножение)",
    "text": "16.5 Произведение матриц (матричное умножение)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Элементы линейной алгебры</span>"
    ]
  },
  {
    "objectID": "math-linal.html#транспонирование-матрицы",
    "href": "math-linal.html#транспонирование-матрицы",
    "title": "16  Элементы линейной алгебры",
    "section": "16.6 Транспонирование матрицы",
    "text": "16.6 Транспонирование матрицы",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Элементы линейной алгебры</span>"
    ]
  },
  {
    "objectID": "math-linal.html#детерминант-и-обратная-матрица",
    "href": "math-linal.html#детерминант-и-обратная-матрица",
    "title": "16  Элементы линейной алгебры",
    "section": "16.7 Детерминант и обратная матрица",
    "text": "16.7 Детерминант и обратная матрица",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Элементы линейной алгебры</span>"
    ]
  },
  {
    "objectID": "math-linal.html#след-матрицы",
    "href": "math-linal.html#след-матрицы",
    "title": "16  Элементы линейной алгебры",
    "section": "16.8 След матрицы",
    "text": "16.8 След матрицы",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Элементы линейной алгебры</span>"
    ]
  },
  {
    "objectID": "math-linal.html#операторы",
    "href": "math-linal.html#операторы",
    "title": "16  Элементы линейной алгебры",
    "section": "16.9 Операторы",
    "text": "16.9 Операторы",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Элементы линейной алгебры</span>"
    ]
  },
  {
    "objectID": "stats-intro.html#stats-goals",
    "href": "stats-intro.html#stats-goals",
    "title": "19  Введение в статистику",
    "section": "19.1 Задачи статистики",
    "text": "19.1 Задачи статистики\nМы в какой-то малоприятной ситуации… Мы пытаемся измерить то, что в определенном смысле невозможно измерить, при этом достаточно точно, чтобы потом это можно было сравнивать или строить какие-то модели. Задача выглядит заведомо провальной…\nОднако именно в этот момент на помощь нам приходит статистика. Не в гордом одиночестве, конечно. Она проводит с собой теорию измерений, психометрику, теорию обнаружения сигнала и др. Всё это работает в нашей психологической науке в комлексе. Мы же в данном курсе сосредотачиваемся на статистической части этого салата.\nСтатистика даёт на теоретический и математический инструментарий, чтобы мы могли делать какие-либо выводы по нашим собранным данным. К сожалению, как бы нам не хотелось, мы не можем делать выводы по сырым данным, потому что измерения по выборке не отражают вот прям ровно то, что есть в генеральной совокупности. Нам их надо определенным образом обсчитать, чтобы наши выводы были корректными. Этим и занимается статистика.\nВозможно, это звучит достаточно абстрактно, но я хочу, чтобы на данном моменте вы поймали некоторое интуитивное понимание того, зачем нужна статистика. Далее это обрастёт содержанием и уложится, я надеюсь, в достаточно стройную систему.\n\n\n\n\n\n\nВажно\n\n\n\nСтатистика помогает нам делать выводы о нашей генеральной совокупности по выборке.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Введение в статистику</span>"
    ]
  },
  {
    "objectID": "stats-intro.html#stats-research",
    "href": "stats-intro.html#stats-research",
    "title": "19  Введение в статистику",
    "section": "19.2 Исследование с точки зрения статистики",
    "text": "19.2 Исследование с точки зрения статистики\n\n19.2.1 Генеральная совокупность и выборка\n\n\n19.2.2 Параметр, индикатор, статистика\n\n\n19.2.3 Исследовательский вопрос и гипотезы",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Введение в статистику</span>"
    ]
  },
  {
    "objectID": "stats-intro.html#stats-data",
    "href": "stats-intro.html#stats-data",
    "title": "19  Введение в статистику",
    "section": "19.3 Виды статистических данных",
    "text": "19.3 Виды статистических данных",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Введение в статистику</span>"
    ]
  },
  {
    "objectID": "stats-intro.html#stats-kinds",
    "href": "stats-intro.html#stats-kinds",
    "title": "19  Введение в статистику",
    "section": "19.4 Виды статистики",
    "text": "19.4 Виды статистики\nCтатистика же как набор методов и инструментов делится на два вида:\n\nОписательная статистика (descriptive statistics1) занимается обработкой статистических данных, их наглядным представлением, и собственно описанием через некоторые характеристики.\n\nЭти характеристики, количественно описывающие особенности имеющихся данных, называются описательными статистиками (descriptive statistics2).\nЗадача описательной статистики — ёмко описать имеющиеся данные и составить на основе этих описаний общее представление о них, а также обнаружить особенности, которые могут повлиять на дальнейший анализ.\n\nСтатистика вывода (inferential statistics) занимается поиском ответов на содержательные вопросы, которые мы задаем данным в ходе их анализа в рамках научных и практических исследований.\n\nСостоит из двух компонентов — тестирования статистических гипотез и статистических методов.\n\n\n\nОк, статистика вывода отвечает на вопросы, а описательная статистика описывает данные. А можно ли отвечать на вопросы с помощью описательной статистики?",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Введение в статистику</span>"
    ]
  },
  {
    "objectID": "stats-rand-exp.html#stats-rand-exp-research",
    "href": "stats-rand-exp.html#stats-rand-exp-research",
    "title": "20  Случайный эксперимент",
    "section": "20.1 Ход исследования",
    "text": "20.1 Ход исследования",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Случайный эксперимент</span>"
    ]
  },
  {
    "objectID": "stats-rand-exp.html#stats-rand-exp-event",
    "href": "stats-rand-exp.html#stats-rand-exp-event",
    "title": "20  Случайный эксперимент",
    "section": "20.2 Событие",
    "text": "20.2 Событие",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Случайный эксперимент</span>"
    ]
  },
  {
    "objectID": "stats-rand-exp.html#stats-rand-exp-space",
    "href": "stats-rand-exp.html#stats-rand-exp-space",
    "title": "20  Случайный эксперимент",
    "section": "20.3 Пространство элементарных событий",
    "text": "20.3 Пространство элементарных событий",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Случайный эксперимент</span>"
    ]
  },
  {
    "objectID": "stats-rand-exp.html#stats-rand-exp-prob-def",
    "href": "stats-rand-exp.html#stats-rand-exp-prob-def",
    "title": "20  Случайный эксперимент",
    "section": "20.4 Определение вероятности",
    "text": "20.4 Определение вероятности\n\n20.4.1 Аксиоматическое определение вероятности\n\n\n20.4.2 Классические определение вероятности\n\n\n20.4.3 Статистическое определение вероятности\n\n\n20.4.4 Геометрическое определение вероятности",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Случайный эксперимент</span>"
    ]
  },
  {
    "objectID": "stats-rand-exp.html#stats-rand-exp-prob-complex",
    "href": "stats-rand-exp.html#stats-rand-exp-prob-complex",
    "title": "20  Случайный эксперимент",
    "section": "20.5 Вероятности сложных событий",
    "text": "20.5 Вероятности сложных событий",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Случайный эксперимент</span>"
    ]
  },
  {
    "objectID": "stats-rand-exp.html#stats-rand-exp-prob-cond",
    "href": "stats-rand-exp.html#stats-rand-exp-prob-cond",
    "title": "20  Случайный эксперимент",
    "section": "20.6 Условная вероятность и независимость",
    "text": "20.6 Условная вероятность и независимость\n\n20.6.1 Парадокс Монти-Холла\n\n\n20.6.2 Теорма Байеса",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Случайный эксперимент</span>"
    ]
  },
  {
    "objectID": "stats-rand-values.html#stats-rand-values-discrete",
    "href": "stats-rand-values.html#stats-rand-values-discrete",
    "title": "21  Случайные величины",
    "section": "21.1 Дискретные случайные величины",
    "text": "21.1 Дискретные случайные величины\n\n21.1.1 Распределение дискретной случайной величины",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Случайные величины</span>"
    ]
  },
  {
    "objectID": "stats-rand-values.html#stats-rand-values-continuous",
    "href": "stats-rand-values.html#stats-rand-values-continuous",
    "title": "21  Случайные величины",
    "section": "21.2 Непрерывные случайные величины",
    "text": "21.2 Непрерывные случайные величины\n\n21.2.1 Распределение непрерывной случайной величины",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Случайные величины</span>"
    ]
  },
  {
    "objectID": "stats-rand-values.html#stats-rand-values-moments",
    "href": "stats-rand-values.html#stats-rand-values-moments",
    "title": "21  Случайные величины",
    "section": "21.3 Характеристики распределения случайный величин",
    "text": "21.3 Характеристики распределения случайный величин",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Случайные величины</span>"
    ]
  },
  {
    "objectID": "stats-rand-values.html#stats-rand-values-distributions",
    "href": "stats-rand-values.html#stats-rand-values-distributions",
    "title": "21  Случайные величины",
    "section": "21.4 Распределения случайных величин",
    "text": "21.4 Распределения случайных величин\n\n21.4.1 Распределение Бернулли\n\n\n21.4.2 Биноминальное распределение\n\n\n21.4.3 Распределение Пуассона\n\n\n21.4.4 Экспоненциальное распределение\n\n\n21.4.5 Равномерное распределение\n\n\n21.4.6 Нормальное распределение\n\\[\nX \\sim \\mathcal{N}(\\mu, \\sigma^2)\n\\]\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}\\big(\\frac{x - \\mu}{\\sigma}\\big)^2}\n\\]\nБолее того, известны вероятности, с которыми значения случайной величины, распределенной по нормальному закону, попадают в определенные интервалы стандартных отклонений (Рисунок 21.1):\n\\[\n\\mathbb{P}\\big( X \\in (\\mu\\!-\\!\\sigma, \\mu\\!+\\!\\sigma) \\big) = 0.682\n\\]\n\\[\n\\mathbb{P}\\big( X \\in (\\mu\\!-\\!2\\sigma, \\mu\\!+\\!2\\sigma) \\big) = 0.954\n\\]\n\\[\n\\mathbb{P}\\big( X \\in (\\mu\\!-\\!3\\sigma, \\mu\\!+\\!3\\sigma) \\big) = 0.996\n\\]\n\n\n\n\n\n\n\n\nРисунок 21.1: Нормальное распределение\n\n\n\n\n\nЗадавая различные параметры распределения (математическое ожидание и дисперсию), мы можем получать разные нормальные распределения (Рисунок 21.2).\n\n\n\n\n\n\n\n\nРисунок 21.2: Нормальное распределение c разными параметрами\n\n\n\n\n\n\n21.4.6.1 Стандартное нормальное распределение\n\n\n\n\n\n\n\n\nРисунок 21.3: Стандартное нормальное распределение\n\n\n\n\n\n\n\n\n21.4.7 Геометрическое распределение\n\n\n21.4.8 Отрицательное биномиальное распределение",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Случайные величины</span>"
    ]
  },
  {
    "objectID": "stats-estim.html#точечные-оценки",
    "href": "stats-estim.html#точечные-оценки",
    "title": "22  Оценивание статистических параметров",
    "section": "22.1 Точечные оценки",
    "text": "22.1 Точечные оценки\n\n\n\nПараметр обычно обозначается греческой буквой. Пусть у нас есть некоторый параметр генеральной совокупности \\(\\theta\\). Его аналогом на выборочной совокупности является его точечная оценка \\(\\hat \\theta\\). Точечная она, потому что представляет собой некоторое одно число. Таким образом, это наиболее компактный способ составить представление о значении параметра. По своей сути она, на самом деле, она является функцией от результатов наблюдений:\n\\[\n\\hat \\theta = \\hat \\theta (x), \\; x = (x_1, x_2, \\dots, x_n)\n\\]\nЧто это значит? То, что на разных выборках эта оценка может различаться. Возьмем для примера такой параметр как среднее значение. Предположим, что мы исследуем интеллект, и наша генеральная совокупность представлена таким набором наблюдений:\n\nnrow(iq)\n\n[1] 10000\n\nhead(iq)\n\n# A tibble: 6 × 2\n     id    IQ\n  &lt;int&gt; &lt;dbl&gt;\n1     1   103\n2     2   109\n3     3   100\n4     4    70\n5     5    64\n6     6   112\n\ntail(iq)\n\n# A tibble: 6 × 2\n     id    IQ\n  &lt;int&gt; &lt;dbl&gt;\n1  9995   104\n2  9996    94\n3  9997   117\n4  9998    93\n5  9999   127\n6 10000   135\n\n\nТак как мы предположили, что это генеральная совокупность, то мы можем посчитать истинное значение параметра \\(\\mu\\). В реальной жизни мы этого сделать не можем!\n\nmean(iq$IQ)\n\n[1] 100.0439\n\n\nВполне ожидаемое значение1. Теперь попробуем наизвлекать выборок человек по 50 и посчитать оценки среднего (выборочные средние) \\(\\hat \\mu\\) на них:\n\nmeans &lt;- numeric()\nfor (i in 1:100) {\n  means[i] &lt;- mean(sample(iq$IQ, 50, replace = FALSE))\n}\nggplot(NULL, aes(means)) +\n  geom_histogram(binwidth = 1, fill = 'royalblue4', color = 'lightgray') +\n  geom_vline(xintercept = mean(iq$IQ)) +\n  labs(x = 'Выборочные средние IQ',\n       y = 'Количество')\n\n\n\n\n\n\n\n\nНаблюдаем, что иногда мы при подсчёте оценке параметра попадаем близко к истинному его значению, иногда промахиваемся. Собственно, как раз об этом неопределённость и вариация.\n\n22.1.1 Метод моментов\n\n\n\nЧтобы получить точечные оценки параметров, используются разные методы в зависимости от конкретной модели анализа. Сейчас мы познакомимся с самым простым — методом моментов.\nСамо название метода отсылает нас к обсуждению характеристик распределений случайных величин. Мы говорили о том, что распредления характеризуются их моментами. В методе моментов есть три этапа:\n\nустанавливается связь между оцениваемым параметром и моментом распределения\n\n\\[\n\\theta = \\xi(\\nu_k) \\quad \\text{или} \\quad \\theta = \\xi(\\mu_k)\n\\]\n\nнаходятся выборочные моменты\n\n\\[\n\\hat \\theta = \\xi(\\nu_k^*) \\quad \\text{или} \\quad \\hat \\theta = \\xi(\\mu_k^*)\n\\]\n\nистинный момент заменяется на выборочный — получается оценка.\n\nДля примера разберём всё тот же датасет с IQ. Мы знаем, что распределение баллов IQ подчинается нормальному закону. Поэтому в качестве параметра «среднее значение коэффициента интеллекта» генеральной совокупности можно использовать математическое ожидание:\n\\[\n\\mu = \\mathbb{E}X\n\\]\nВыборочным аналогом математического ожидания является выборочное среднее:\n\\[\n\\hat \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nИ это, собственно, всё. Если вы хотя бы раз анализировали данные, вы имплицитно пользовались этим знанием. Просто, скорее всего, не задумывались, что это так работает. :)\n\n\n22.1.2 Метод максимального правдоподобия\n\n\n\n\n\n22.1.3 Bootstrap\n\n\n\n\n\n22.1.4 Свойства точечных оценок\n\n\n\nТак как точечные оценки всё же оценки, мы можем и промахнуться мимо истинного среднего — это мы наблюдали на гистограмме. Поэтому нам надо предъявить определённые требования к точечным оценкам. Их три: несмещённость, состоятельность и эффективность.\n\n22.1.4.1 Несмещенность\n\n\n\nНесмещённость выражает следующую идею: когда мы рассчитываем выборочную оценку, мы должны как можно ближе попадать в истинное значение параметра.\n\\[\n\\forall n \\; \\mathbb{E} \\hat \\theta = \\theta, \\quad (\\mathbb{E}\\hat \\theta - \\theta) \\rightarrow 0,\n\\] где \\(n\\) — объём выборки, \\((\\mathbb{E}\\hat \\theta - \\theta)\\) — смещение.\nСлева представлено требование несмещённости при любом объёме выборки, а справа — ассимптотической несмещённости.\n\n\n22.1.4.2 Состоятельность\n\n\n\nМатематически состоятельность определяется следующим образом:\n\\[\n\\lim_{n \\rightarrow \\infty} \\mathrm{P}(|\\hat \\theta - \\theta| &lt; \\varepsilon) = 1, \\, \\varepsilon &gt; 0\n\\]\nСодержательно эта запись нам говорит следующее, что при неограниченном росте мощности выборки наша оценка стремится к истинному значению параметра. Может быть, такая формулировка не совсем точна математически, но позволяет представить, что происходит.\n\n\n22.1.4.3 Эффективность\n\n\n\nЭффективность точечной оценки определяется достаточно просто. Так как оценка параметра — это случайная величина, но у неё есть дисперсия. Чтобы оценка была эффективна, её дисперсия должна быть минимальной:\n\\[\n\\sigma^2_{\\hat \\theta} = \\min\n\\]\nПример несмещённой, состоятельной и эффективной оценки — это выборочное среднее для оценки математического ожидания нормально распределённой величины. Именно то, что мы и делали в примере применения метода моментов.",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Оценивание статистических параметров</span>"
    ]
  },
  {
    "objectID": "stats-estim.html#интервальные-оценки",
    "href": "stats-estim.html#интервальные-оценки",
    "title": "22  Оценивание статистических параметров",
    "section": "22.2 Интервальные оценки",
    "text": "22.2 Интервальные оценки\n\n\n\nКроме самого значения оценки, необходимо определить качество этой оценки, иначе говоря — её точность. Для этого используется такая величина как надёжность:\n\\[\n\\gamma = \\mathrm{P}(\\theta_\\min &lt; \\theta &lt; \\theta_\\max)\n\\]\nТакая форма оценки называется интервальной оценкой параметра, так как мы указываем интервал, в котором находится истинное значение с определённой вероятностью.\nТакая форма оценки даёт исчерпывающую информацию о параметре: мы знаем (1) интервал, в котором находится значение параметра генеральной совокупности, а также (2) надёжность, с которой выбранный интервал накрывает это значение.\nЗначение надежности \\(\\gamma\\) может быть выбрано произвольно, но обычно оно близко к единице. Однако необхожимо помнить, что чем выше надёжность, тем шире границы интервальной оценки.\n\n22.2.1 Стандартная ошибка\n\n\n\n\n22.2.1.1 Почему \\(\\text{se}(X) = \\frac{\\text{sd}(X)}{\\sqrt{n}}\\)\n\n\n\n\\[\n\\text{var}(X + Y) = \\text{var}(X) + \\text{var}(Y) + 2 \\text{cov}(X, Y)\n\\]\n\\[\n\\text{var}(aX) = a^2 \\text{var}(X)\n\\]\nТак как наблюдения извлекаются из независимых одинаково распределенных величин (independent identically distributed, iid), то\n\\[\n\\text{cov}(X_i, X_j) = 0, \\sigma_{X_i} = \\sigma_{X_j} = \\sigma\n\\]\n\\[\n\\text{var}\\Big( \\frac{1}{n} \\sum X_i \\Big) = \\frac{1}{n^2} \\sum \\text{var}(X_i) = \\frac{1}{n^2} \\sum \\sigma^2 = \\frac{n}{n^2} \\sigma^2 = \\frac{\\sigma^2}{n}\n\\]\n\\[\n\\text{se}_X = \\sqrt{ \\text{var}\\Big( \\frac{1}{n} \\sum X_i \\Big)} = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\n\\]\n\n\n\n22.2.2 Доверительный интервал\n\n\n\nВариантом интервальной оценки является доверительный интервал (confidence interval). Итак, ещё раз:\n\\[\n\\mathrm{P}(\\theta_\\min &lt; \\theta &lt; \\theta_\\max) = \\gamma, \\; \\gamma \\rightarrow 1\n\\]\n\\(theta_\\min\\) и \\(\\theta_\\max\\) — границы доверительного интервала, \\(\\gamma\\) — доверительная вероятность. На практике её значение чаще всего принимается равным \\(0.95\\).\nАлгоритм определения интервальной оценки следующий:\n\nНайсти статистику \\(\\zeta(\\theta)\\), связанную с оцениваниемым параметром, закон распределения которой известен \\(f(\\zeta)\\).\nОпределить значения \\(\\zeta_\\min\\) и \\(\\zeta_\\max\\), в пределах которых статистика находится с вероятностью \\(\\gamma\\).\nЗная связь \\(\\zeta(\\theta)\\) перейти к границам \\(\\theta_\\min\\) и \\(\\theta_\\max\\).\n\nРазберемся с этим на примере построения доверительного интервала для генерального среднего.\nПервая задача — найти статистику. Мы воспользуемся тем, что за нас поработали учёные-статистики и сказали, что вот такая вполне подойдёт:\n\\[\nt = \\frac{\\bar x - \\mu}{s}\\sqrt{n-1},\n\\]\nгде \\(t\\) — значение статистики, \\(\\bar x\\) — выборочное среднее, \\(\\mu\\) — генеральное среднее, \\(s\\) — выборочное стандартное отклонение, \\(n\\) — объём выборки.\nИзвестен ли закон её распредления? Да. Эта статистика подчинается \\(t\\)-распределению (распределению Стьюдента). Оно похоже на нормальное, но хвосты у него повыше:\n\n\n\nТеперь надо сформулировать вид интервальной оценки для генерального среднего. Путем арифметических преобразований формулы выше мы имеет следующее:\n\\[\n\\mu = \\bar x - t \\frac{s}{\\sqrt{n-1}}\n\\]\nЗначит вид интервальной оценки будет таков:\n\\[\n\\mathrm{P}\\Big( \\bar x - t_\\alpha \\frac{s}{\\sqrt{n-1}} &lt; \\mu &lt;\n\\bar x + t_\\alpha \\frac{s}{\\sqrt{n-1}}\\Big) = 1 - \\alpha = \\gamma\n\\]\nПосмотрим на картинку:\n\n\n\nВидим на ней наш доверительный интвервал и значения \\(t_\\alpha\\) и \\(-t_\\alpha\\). Сама \\(\\alpha\\) обозначает вероятность выхода за границы доверительного интервала. Осталось всё это высчитать в числах, и получить границы доверительного интервала.\nХорошо, что весь этот ужас в R скрыт под капотом:\n\nmean_cl_normal(iq$IQ)\n\n         y     ymin     ymax\n1 100.0439 99.74787 100.3399\n\nmean_cl_boot(iq$IQ)\n\n         y     ymin     ymax\n1 100.0439 99.74425 100.3463",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Оценивание статистических параметров</span>"
    ]
  },
  {
    "objectID": "stats-testing.html#stats-testing-hyotheses",
    "href": "stats-testing.html#stats-testing-hyotheses",
    "title": "23  Тестирование статистических гипотез",
    "section": "23.1 Нулевая и альтернативная гипотезы",
    "text": "23.1 Нулевая и альтернативная гипотезы\nЕще раз тезисно вспомним о гипотезах в целом:\n\nГипотеза (\\(H\\)) — это предположение, которое подлежит проверке на основе результатов наблюдений.\nГипотезы бывают:\n\nтеоретические — про конструкты\nэмпирические — про переменные\nстатистические — про параметры [генеральной совокупности] и данные\n\n\nСтатистические гипотезы бывают простыми и сложными:\n\nПростая гипотеза — это такое предположение, которое включает в себя какое-либо однозначно определеяемое утверждение. Например, истинная величина параметра соответствует некоторому строго заданному значению: \\(H : \\theta = \\theta_0\\). Другой вариант — две генеральные совокупности имеют одно и то же значение одной и той же характеристики: \\(H : \\theta_1 = \\theta_2\\).\nСложная гипотеза предполагает множественность вариантов для параметра, которые укладываются в рамки проверяемого предположения. Например, \\(H : \\theta &gt; \\theta_0\\) или \\(H : \\theta_1 \\neq \\theta_2\\).\n\nВ рамках самого хода тестирования гипотез существует проверяемая (нулевая) гипотеза (\\(H_0\\)). Её обычно стараются предельно упростить, поэтому она формулируется как простая гипотеза. В противовес ей выдвигается альтернативная гипотеза (\\(H_1\\)), которая будет иметь вид сложной гипотезы.\nДля проверки гипотезы необходимы две вещи:\n\nрезультаты наблюдений и\nкритерий.\n\nРезультаты наблюдений, полученные на выборке, сами по себе, как правило, не используются. Однако на их основе рассчитываются выборочные статистики (показатели), которые непосредственно участвуют в проверке гипотезы.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "stats-testing.html#stats-testing-approaches",
    "href": "stats-testing.html#stats-testing-approaches",
    "title": "23  Тестирование статистических гипотез",
    "section": "23.2 Подходы к тестированию статистических гипотез",
    "text": "23.2 Подходы к тестированию статистических гипотез\n\n23.2.1 Фреквентистский подход\n\n\n23.2.2 Байесовский подход",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "stats-testing.html#stats-testing-results",
    "href": "stats-testing.html#stats-testing-results",
    "title": "23  Тестирование статистических гипотез",
    "section": "23.3 Возможные результаты проверки гипотез",
    "text": "23.3 Возможные результаты проверки гипотез\nВ результате проверки статистических гипотез могут возникнуть четыре ситуации.\nМы изучаем в исследовании какую-либо закономерность, которая в реальном мире может существовать, а может и не существовать. В силу неопределённости и вариативности наших данных мы может либо обнаружить интересующую нас закономерность, либо не обнаружить.\nВ качестве нулевой гипотезы мы выдвигаем предположение о том, что закономерность отсутствует — так мы упрощаем нашу нулевую гипотезу. Пусть \\(H_0\\) обозначает, что предположение, которое мы проверяем справедливо, а \\(H_1\\) — не справедливо. На основании данных мы можем либо не отклонить наше предположение (\\(\\hat H_0\\)), либо отклонить (\\(\\hat H_1\\)).\nТогда имеем следующую ситуацию:\n\n\n\n\n\\(H_0\\)\n\\(H_1\\)\n\n\n\n\n\\(\\hat H_0\\)\n✓\nОшибка II рода\n\n\n\\(\\hat H_1\\)\nОшибка I рода\n✓\n\n\n\n\nОшибка I рода возникает, когда в генеральной совокупности искомой закономерности нет, но мы в силу случайных флуктуаций в данных её нашли.\nОшибка II рода возникает, когда в генеральной совокупности искомая закономерность есть, но мы в силу каких-либо причин её не нашли.\n\nОшибки — это нехорошо, они нас не устраивают. Надо каким-то образом их контролировать.\n\nОшибка I рода контролируется достаточно просто. Так как мы нашли закономерность, которую искали, мы можем посчитать вероятность, с которой потенциально ошиблись. А собственно контролировать ошибку мы будем с помощью уровня значимости \\(\\alpha\\), который выбирается до начала процедуры тестирования гипотезы. Он и задает вероятность, с который мы позволяем себе ошибиться — отклонить нулевую гипотезу, при условии, что она верна.\nОшибку II рода контролировать сложнее, так как мы не нашли закономерность, которую искали. Нам нужна какая-то метрика, которая позволит сказать, что мы сделали всё возможное для того, чтобы обнаружить искомую закономерность. Вероятность ошибки II рода обозначается \\(\\beta\\) — тогда вероятность того, что мы не совершили ошибку II рода будет \\(1 - \\beta\\). Эта величина называется статистической мощностью, и она связана с размером эффекта и объемом выборки. Статистическую мощность можно рассчитать как до проведения статистического анализа для расчета требуемого объема выборки — так и после — для определения достигнутой статистической мощности.\n\nСоберем все обозначения в единую табличку1:\n\n\n\n\n\n\n\n\n\n\\(H_0\\)\n\\(H_1\\)\n\n\n\n\n\\(\\hat H_0\\)\n\\(\\mathrm P (\\hat H_0 | H_0)\\)\n\\(\\mathrm P (\\hat H_0 | H_1) = \\beta\\)\n\n\n\\(\\hat H_1\\)\n\\(\\mathrm P (\\hat H_1 | H_0) = \\alpha\\)\n\\(\\mathrm P (\\hat H_1 | H_1) = 1 - \\beta\\)\n\n\n\nУровень значимости \\(\\alpha\\) выбирается близким к нулю — всем знакомо конвенциональное значение \\(0.05\\). Вообще \\(\\alpha\\) можно выбрать сколь угодно малым, однако при выборе уровня значимости руководствуются принципом разумной достаточности, так как если устремить \\(\\alpha\\) к нулю, то устремиться к нулю и вероятность отклонения нулевой гипотезы.\n\n\n\nМатематические руны\n\n\\[\n\\mathrm P (\\hat H_1) = \\mathrm P (\\hat H_1 | H_0) \\cdot \\mathrm P (H_0) = \\alpha \\cdot \\mathrm P(H_0)\n\\]\n\n\nДостаточной статистической мощностью считается \\(0.8\\). Аналогично, устремляя мощность к единице (\\((1 - \\beta) \\rightarrow 1 \\Rightarrow \\beta \\rightarrow 0\\)), мы устремляем вероятность не отклонения нулевой гипотезы к нулю:\n\n\n\nЕщё математические руны\n\n\\[\n\\mathrm P (\\hat H_0) = \\mathrm P (\\hat H_0 | H_1) \\cdot \\mathrm P (H_1) = \\beta \\cdot \\mathrm P (H_1)\n\\]\n\n\nНеобходимо также помнить, что ошибки первого и второго рода связаны между собой так, что\n\\[\n\\alpha \\rightarrow 0 \\Rightarrow \\beta \\rightarrow 1\n\\]\n\n\n\nОпять математические руны\n\n\\[\n\\beta \\cdot \\mathrm P (H_1) = \\mathrm P (\\hat H_0) = \\mathrm P (\\hat H_0 | H_0) \\cdot \\mathrm P (H_0) \\Rightarrow \\beta = \\frac{1}{\\mathrm P (H_1)} \\cdot \\mathrm P (H_0) \\cdot \\mathrm P(\\hat H_0 | H_0) \\\\\n\\beta = \\frac{1}{\\mathrm P (H_1)} \\cdot \\big (1 - \\mathrm P (H_1 | H_0)\\big) = \\frac{1}{\\mathrm P (H_1)} \\cdot \\mathrm P (H_0) \\cdot (1 - \\alpha)\n\\]\n\n\n\n23.3.1 Асимметрия статистического вывода\nВыше мы сказали, что для проверки гипотезы нужны две вещи:\n\nрезультаты наблюдений и\nкритерий.\n\nС результатами наблюдений более-менее очевидно.\nКритерий — это правило, согласно которому гипотезу либо принимают, либо отклоняют. Однако перед тем как проверять гипотезу, её так-то нужно сформулировать, и сделать это правильно, поскольку от формулировки гипотезы зависит интерпретация результатов проверки и дальнейшее использование полученной информации.\nИспользуемая статистика сама по себе является [непрерывной] случайной величиной, а значит может быть построено её распределение. Критерий будет разделять это распределение на непересекающиеся области. В результате чего возникает критическая область — область отклонения гипотезы. Дополнением к ней является область неотклонения гипотезы.\nКритическая область может быть односторонней (при \\(H_1:\\theta &gt; \\theta_0\\) или \\(H_1: \\theta &lt; \\theta_0\\)) и двусторонней (при \\(H_1:\\theta \\neq \\theta_0\\)). «Размер» критической области определяется уровнем значимости.\nСтатистический вывод — заключение о том, получили ли мы подтверждение альтернативной гипотезы — по структуре представляет собой импликацию. Если вам не знаком этот термин из логики, то вот:\n\nЕсли значение нашей статистики, которое мы рассчитали на выборке, попало в критическую область, то мы говорим о том, что нулевая гипотеза отклоняется.\nЕсли значение нашей статистики, которое мы рассчитали на выборке, не попало в критическую область, то мы не получаем оснований для того, чтобы отклонить нулевую гипотезу. Однако мы также не получаем оснований, чтобы её «принять». Мы остаёмся в некотором неведении: мы не нашли различий, а есть они там или нет — хто ж их знает… Итого, мы не можем сделать никакого вывода.\n\nВ этом и заключается асимметрия статистического вывода. Как раз для того, чтобы с ней как-то жить, мы работаем со статистической мощностью.\n\nПосмотреть, как все эти штуки друг с другом соотносятся можно тут.\n\n\n\n23.3.2 Связь ошибки первого и второга рода",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "stats-testing.html#stats-testing-algorithm",
    "href": "stats-testing.html#stats-testing-algorithm",
    "title": "23  Тестирование статистических гипотез",
    "section": "23.4 Агоритм тестирования статистических гипотез",
    "text": "23.4 Агоритм тестирования статистических гипотез\nДля тестирования гипотез есть два сценария: первый и тот, которым мы будем пользоваться. Первый вариант чуть более классический, второй — более гибкий.\nСценарий номер раз\n\nФормулировка гипотезы\nВыбор статистического критерия\nВыбор уровня значимости \\(\\alpha\\)\nПостроение закона распредления статистики критерия при условии, что нулевая гипотеза верна\nОпределение границ критической области\nРасчёт выборочной статистики\nОпределение, попадает ли наблюдемое значение статистики в критическую область и вынесение решения\n\nСценарий номер два\n\nФормулировка гипотезы\nВыбор статистического критерия\nВыбор уровня значимости \\(\\alpha\\)\nПостроение закона распредлеения статистики критерия при условии, что нулевая гипотеза верна\nРасчёт выборочной статистики\nРасчёт достигнутого уровня значимости p-value\nСопоставление \\(\\alpha\\) и p-value и вынесение решения\n\nПочему второй вариант более гибкий? Представим, что мы захотели понизить уровень значимости с \\(0.05\\) до \\(0.01\\) — такие уровни значимости всречаются, например, в медицине. Если мы идем по первому сценарию, то нам надо заново пересчитать критические значения и вновь проанализировать, попадает ли наблюдаемое значение в критическую область. Если мы адепты второго сценария, то нам надо только выполнить одно новое сравнение нашего p-value с новым уровнем значимости.",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "stats-testing.html#stats-testing-effect-size",
    "href": "stats-testing.html#stats-testing-effect-size",
    "title": "23  Тестирование статистических гипотез",
    "section": "23.5 Размер эффекта",
    "text": "23.5 Размер эффекта",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "stats-testing.html#stats-testing-power",
    "href": "stats-testing.html#stats-testing-power",
    "title": "23  Тестирование статистических гипотез",
    "section": "23.6 Статистическая мощность",
    "text": "23.6 Статистическая мощность\n\n\n\n\n\n\nСтатистическая мощность Post hoc\n\n\n\nНАПИСАТЬ\nесть ли смысл рассчитывать статистическую мощность пост хок?\nкажется, нет, так как эффект уже нашли",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "stats-testing.html#stats-testing-false-positive",
    "href": "stats-testing.html#stats-testing-false-positive",
    "title": "23  Тестирование статистических гипотез",
    "section": "23.7 Ложноположительный вывод",
    "text": "23.7 Ложноположительный вывод\n\n23.7.1 Проблема множественных сравнений\nИтак, мы сравниваем попарно все группы наблюдений между собой. В каждом сравнении мы фиксируем вероятность ошибки первого рода с помощью уровня значимости на уровне \\(0.05\\). А какова будет вероятность ошибки, если мы проводим несколько сравнений?\nСчитаем, что наши сравнения независимы, поэтому вероятности будут перемножаться1. Если верояность ошибиться в одном сравнении равна \\(\\alpha\\), то вероятность сделать правильный вывод — \\(1 - \\alpha\\). Тогда вероятность сделать правильный вывод в \\(m\\) сравнениях — \\((1 - \\alpha)^m\\). Отсюда мы можем вывести вероятность ошибиться хотя бы в одном сравнении:\n\\[\n\\mathbb{P}^′ = 1 - (1 - \\alpha)^m\n\\]\nПусть у нас есть три группы, которые нам надо сравнить друг с другом — получается необходимо провести три сравнения. Итого вероятность ошибиться получается:\n\\[\n\\mathbb{P}^′ = 1 - (1 - 0.05)^3 \\approx 0.143\n\\]\nЗначительно больше, чем \\(0.05\\), что нехорошо. И дальше только хуже. Поэтому нам надо либо корректировать уровень значимости, либо использовать мощные методы типа дисперсионного анализа.\n\n\n\n\n\nРост вероятности ошибки первого рода при увеличении числа попарных сравнений\n\n\n\n\n\n23.7.1.1 Корректировка уровня значимости\nКорректировать уровень значимости можно по-разному. Например, можно разделить \\(\\alpha\\) на количество попарных сравнений — такой способ называется поправкой Бонферрони (Bonferroni):\n\\[\n\\alpha’ = \\frac{\\alpha}{n},\n\\]\nгде \\(n\\) — число попарных сравнений.\nПоправка Бонферрони считается самой консервативной поправкой — она достаточно сильно уменьшает уровень значимости, и мы можем не поймать искомую закономерность, то есть совершить ошибку второго рода2. Поэтому придумали более либеральные поправки, например, поправку Холма (Холма–Бонферрони, Holm) или поправку Тьюки (Tukey’s HSD test). Можно посмотреть на их формулы, но в целом, не обяз, потому что их все равно никто не знает, а в статистических пакетах мы либо допишем аргумент в функцию, либо нужную галку поставим.\nНа практике в силу того, что в статистических пакетах мы работаем с p-value, корректируется именно его значение.\nПо достаточно незамысловатой логике Здесь: вариант для поправки Бонферрони.\n\\[\np &lt; \\frac{\\alpha}{n} \\Rightarrow np &lt; \\alpha\n\\]\nТаким образом, мы просто сравниваем уже скорретированное p-value, которое нам считает программа, с тем же самым \\(\\alpha = 0.05\\). Жизнь становится значительно проще и приятнее.\n\n\n\n23.7.2 Проблема количества статистических тестов",
    "crumbs": [
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Тестирование статистических гипотез</span>"
    ]
  },
  {
    "objectID": "andan-descriptives.html#andan-descriptives-kinds-of-stats",
    "href": "andan-descriptives.html#andan-descriptives-kinds-of-stats",
    "title": "24  Описательные статистики",
    "section": "24.1 Виды статистики",
    "text": "24.1 Виды статистики\n\n\n\nНапомним себе, что статистика [как набор методов и инструментов] делится на два вида — описательная статистика и статистика вывода.\n\nОписательная статистика (descriptive statistics1) занимается обработкой статистических данных, их наглядным представлением, и собственно описанием через некоторые характеристики.\n\nЭти характеристики, количественно описывающие особенности имеющихся данных, называются описательными статистиками (descriptive statistics2).\nЗадача описательной статистики — ёмко описать имеющиеся данные и составить на основе этих описаний общее представление о них, а также обнаружить особенности, которые могут повлиять на дальнейший анализ.\n\nСтатистика вывода (inferential statistics) занимается поиском ответов на содержательные вопросы, которые мы задаем данным в ходе их анализа в рамках научных и практических исследований.\n\nСостоит из двух компонентов — тестирования статистических гипотез и статистических методов.\n\n\n\n\n\n\n\n\nЗамечание о машинном обучении\n\n\n\nВ названии книги упомянуто «машинное обучение». Иногда его причисляют к статистике, иногда рассматривают отдельно. На самом же деле, статистические методы лежат где-то между статистикой вывода и машинным обучением.\nПочему?\nДело в том, что на статистические методы можно смотреть по-разному.\n\nЕсли нашей задачей является поиск ответов на исследовательские вопросы о закономерностях, о связи каких-либо факторов или влиянии переменных друг на друга, то мы будем смотреть на статистические модели с точки зрения статистики вывода. Это позволит нам находить ответы на интересующие нас вопросы — причем не важно, говорим мы о научных исследованиях или об исследованиях в индустрии.\nЕсли перед нами стоит задача хорошо предсказывать одни переменные на основании значений других — например, выдавать рекомендации на Яндекс.Музыке или в Яндекс.Лавке — то мы будем смотреть на те же статистические модели с точки зрения машинного обучения.\n\nТо есть, модели в анализе данных и машинном обучении одни и те же, но то, какую модель мы назовем хорошей и как мы эту «хорошесть» определим, будет отличаться в зависимости от задачи — исследовательская или предиктивная — которая перед нами стоит.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Описательные статистики</span>"
    ]
  },
  {
    "objectID": "andan-descriptives.html#andan-descriptives-central-tendency",
    "href": "andan-descriptives.html#andan-descriptives-central-tendency",
    "title": "24  Описательные статистики",
    "section": "24.2 Меры центральной тенденции",
    "text": "24.2 Меры центральной тенденции\n\n\n\nИтак, мы хотим описать наши данные. Точнее, распределения переменных, которые у нас в данных есть. Хотим сделать это просто и ёмко. Насколько просто и ёмко? Ну, допустим максимально — одним числом. Для этого неплохо подойдет значение переменной, которое лежит в центре распределения.\nКак мы будем искать, что там в центре распределения? Зависит от шкалы (Stevens 1946), в которой измерена конкретная переменная (Таблица 24.1).\n\n\n\nТаблица 24.1: Шкалы и меры центральной тенденции\n\n\n\n\n\n\n\n\n\nШкала\nМера центральной тенденции\n\n\n\n\nНоминальная\nМода\n\n\nПорядковая\nМедиана\n\n\nИнтервальная\nСреднее арифметическое\n\n\nАбсолютная\nСреднее арифметическое, геометрическое и др.\n\n\n\n\n\n\nОднако есть некоторые нюансы.\n\n24.2.1 Мода\n\n\n\nСамый простой вариант найти центральную тенденцию — это определить наиболее часто встречающееся значение переменной. Это значение называется модой (mode).\n\nОпределение 24.1 Мода [дискретной переменной] — наиболее часто встречающееся значение данной переменной.\n\nНапример, у нас есть следующий ряд наблюдений по какой-то переменной:\n\\[\n\\begin{bmatrix}\n1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1\n\\end{bmatrix}\n\\]\nЕсли мы посчитаем, сколько раз встретилась каждое значение переменной и составим таблицу частот, то получим следующее:\n\\[\n\\begin{matrix}\n\\text{Значение} & 1 & 2 & 3 & 4 & 6 \\\\\n\\text{Частота}  & 2 & 2 & 4 & 2 & 1\n\\end{matrix}\n\\]\nОчевидно, что \\(3\\) встречается чаще других значений — это и есть мода.\nПонятно, что если на нашей шкале нет чисел, а есть текстовые лейблы, это ничего не меняет. Пусть у нас есть переменная с кодами аэропортов:\n\\[\n\\begin{bmatrix}\n\\text{DME} & \\text{LED} & \\text{IST} & \\text{AER} & \\text{IST} &\\text{SVO} & \\text{LED} & \\text{VKO} & \\text{LED} & \\text{IST} & \\text{IST} & \\text{VKO} & \\text{AER} & \\text{DME}\n\\end{bmatrix}\n\\]\n\\[\n\\begin{matrix}\n\\text{Значение} & \\text{DME} & \\text{LED} & \\text{IST} & \\text{AER} & \\text{SVO} & \\text{VKO}\\\\\n\\text{Частота}  & 2 & 3 & 4 & 2 & 1 & 2\n\\end{matrix}\n\\]\nМода — \\(\\text{IST}\\) (Международный аэропорт Стамбула, İstanbul Havalimanı).\nТак мы действуем в случае с эмпирическим распределением. Если нам известна функция вероятности переменной (probability mass function, PMF), то мы можем определить моду, основываясь на ней:\n\nОпределение 24.2 Мода [дискретной переменной] — это значение переменной, при котором её функция вероятности принимает своё максимальное значение.\n\n\\[\n\\text{mode}(X) = \\arg \\max(\\text{PMF}(X)) = \\arg \\max_{x_i}(\\mathbb{P}(X = x_i)),\n\\tag{24.1}\\]\nгде \\(X\\) — дискретная случайная величина, \\(x_i\\) — значение этой случайной величины.\n\n\n\n\n\n\n\n\nРисунок 24.1: Определение моды с помощью функции вероятности\n\n\n\n\n\nОкей, мы видим, что мода отлично считается на дискретных переменных. А как же быть с непрерывными?\nНапомним себе, что вероятность того, что непрерывная случайная величина примет своё конкретное значение, равна нулю. Из этого следует, что все значения непрерывной случайное величины уникальны — каждое повторяется только один раз. Получается, что строить частотную таблицу бессмысленно…\nПо этой причине для непрерывных переменных моду не считают.\n\n24.2.1.1 Мода для непрерывной переменной\n\n\n\nДа, это так. Действительно, посчитать моду для непрерывной переменной способом, аналогичным тому, что мы увидели выше, не получится. Однако математиков это не остановило.\nЕсли мы посмотрим на график плотности вероятности (probability density function, PDF), который является аналогом PMF для дискретных переменных, мы увидим, что какие-то значения встречаются чаще, а какие-то реже. Что в общем-то логично. Напомним себе, как это выглядит, например, для любимого [стандартного] нормального распределения:\n\n\n\n\n\n\n\n\nРисунок 24.2: Частоты интервалов значений непрерывной случайной величины на функции плотности распределения\n\n\n\n\n\nТо есть, самые часто встречающиеся значения — это пик распределения. Там и должна быть мода. Визуально это выглядит достаточно справедливо.\nМатематики так и решили:\n\nОпределение 24.3 Мода [непрерывной переменной] — это значение переменной, при котором её функция плотности вероятности достигает локального3 максимума.\n\n\\[\n\\text{mode}(X) = \\arg \\max(\\text{PDF}(X)) = \\arg \\max_{x \\in S}f(x),\n\\tag{24.2}\\]\nгдe \\(X\\) — непрерывная случайная величина, \\(x\\) — значение этой случайной величины, \\(S\\) — имеющаяся выборка значений переменной.\n\n\n\n\n\n\n\n\nРисунок 24.3: Положение моды на функции плотности [стандартного] нормального распределения\n\n\n\n\n\nХотя моду для непрерывной переменной вычислить можно, обычно этого не делают, так как достаточно других мер центральной тенденции для описания распределения.\n\n\n\n\n\n\n\nTake-home: мода\n\n\n\n\nмода — это значение переменной, которое встречается в выборке чаще всего\nна практике она рассчитывается через построение частотной таблицы\nиспользуется с дискретными (номинальными и порядковыми) переменными\nдля непрерывных переменных её рассчитать можно, но обычного этого не делают\n\n\n\n\n\n\n24.2.2 Унимодальные и полимодальные распределения\n\n\n\nНормальное распределение, как и ряд других — биномиальное, отрицательное биномиальное, пуассоновское — относятся к унимодальным. Такие распределения имеют только одну моду (см. Рисунок 24.4, Рисунок 24.5, Рисунок 24.6).\n\n\n\n\n\n\n\n\nРисунок 24.4: Нормальное распределение (μ = 2$, σ = 0.5). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 24.5: Биномиальное распределение (n = 50, p = 0.3). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 24.6: Распределение Пуассона (λ = 5.5). Пунктирной линией обозначено положение моды.\n\n\n\n\n\nЭто теоретические распределения. С эмпирическими распределениями дело обстоит так же, хотя они обычно менее гладенькие и красивые (см. Рисунок 24.7 и Рисунок 24.8).\n\n\n\n\n\n\n\n\nРисунок 24.7: Эмпирическое распределение, сгенерированное из нормального распределения (μ = 8, σ = 4, n = 100). set.seed(314). Пунктирной линией обозначено положение моды.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 24.8: Эмпирическое распределение, сгенерированное из логнормального распределения (μ = 1.1, σ = 1.39, n = 30). set.seed(314). Пунктирной линией обозначено положение моды.\n\n\n\n\n\nОднако на практике возможны и другие ситуации. Например, такие (Рисунок 24.9, Рисунок 24.10):\n\n\n\n\n\n\n\n\nРисунок 24.9: Бимодальное распределение. Сгенерировано из двух нормальных распределений (μ1 = 1.5, σ1 = 0.4, n1 = 80; μ2 = 4, σ2 = 0.5, n2 = 40). set.seed(65). Пунктирными линиями обозначены положения мод.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 24.10: Полимодальное распределение. Сгенерировано из двух нормальных распределений (μ1 = 1.5, σ1 = 0.3, n1 = 80; μ2 = 3.4, σ2 = 0.5, n2 = 40) и бета-распределения (α = 2, β = 4, n = 50). set.seed(65). Пунктирными линиями обозначены положения мод.\n\n\n\n\n\nВ первом случае (Рисунок 24.9) мы видим два локальных максимума функции плотности вероятности — такое распределение называется бимодальным. Во втором случае (Рисунок 24.10) функция плотности вероятности имеет три локальных максимума — такое распределение называется полимодальным. Бимональное распределение является частным случаем полимодального распределения.\nВ прицнипе, пиков может быть и больше, однако при работе с реальными данными чаще всего мы сталкиваемся с бимодальными распределениями.\nЧто это значит и что с этим делать?\nБимодальное распределение сигнализирует нам о гетерогенности выборки. Если мы видим два выделяющихся пика, стоит подумать о том, что наша выборка неоднородна и в ней выделяются две подвыборки. Посмотрим на структуру выборки из примера выше (Рисунок 24.11):\n\n\n\n\n\n\n\n\nРисунок 24.11: Структура бимодального распределения из Рисунок 24.9. Для удобства сопоставления графиков плотностей вероятности по оси ординат отложены частоты.\n\n\n\n\n\nДействительно, наше распределение состоит из двух других распределений, у каждого из которого есть своя мода — поэтому итоговое распределение получается бимодальным. Конечно, сейчас нам это очень удобно показать, потому что мы знаем, как это распределение генерировалось. Когда же у нас есть реальные данные и мы там наблюдаем такого «верблюда», бывает достаточно сложно сказать, что «пошло не так».\nСамо по себе распределение не даст нам ответ на вопрос, почему оно бимодальное — чтобы выяснить причины такого поведения переменной нам потребуются другие данные. Обычно у вас в данных есть «соцдем» — пол, возраст, сфера и место работы, уровень обрвазования и др. Попробуйте построить распределение с разбиением исследуемой бимодальной переменной по переменным «соцдема». Это, к сожалению, не является рецептом успеха, поскольку причина гетерогенности выборки может и не содержаться в ваших данных, но такое изучение данных станет хорошим показателем того, что вы не просто «забили» на странное распределение своей переменной, а поисследователи возможные его причины.\nЕсли вам удалось найти причины гетерогенности выборки — допустим, у вас выделяются подвыборки «бакалавры» и «магистры» — стоит подумать о том, как обойтись с этой переменной в планируемом анализе, так как игнорировать её, по-видимому, нельзя, поскольку она влияет на вариатиность данных.\n\n\n\n\n\n\nСоцдем лишним не бывает\n\n\n\nНа этапе планирования исследования подумайте о том, чем могут отличаться ваши респонденты или испытуемые между собой, помимо индивидуальных различий.\n\nЕсли в эксперименте используете задачу мысленного вращения (mental rotation, (Shepard and Metzler 1971)), вполне возможно, испытуемые, работающие в сфере 3D-моделирования или дизайна интерьеров, могут сформировать подвыборку.\nВ случае HR-исследования, где фиксируется доход респондента, необходимо записать город, в котором он проживает и/или работает.\nПри изучении удовлетворенности городским пространством важными пунктами станут беременность, наличие/отсутствие детей, наличие/отсутствие автомобиля и др.\n\nИ так далее. Примеров для каждого случая можно подобрать много.\nСтоит ли, скажем, в первом случае сразу исключить из выборки 3D-моделлеров? Зависит. От количества времени и денег на проведение исследования. Однако как минимум эту информацию надо зафиксировать в данных. А решить, исключать ли этих респондентов из выборки или нет, можно и позже. Главно об этом написать в отчете/статье, когда будете описывать предобработку данных.\n\n\n\n\n\n\n\n\n\nTake-home: бимодальное распределение\n\n\n\n\nбимодальное распределение намекает на неоднородность данных — скорее всего, в выборке есть две подвыборки\nнеобходимо поискать в данных причины этой неодноросности, например, в социально-демографических переменных\nесли удалось найти переменную, объясняющую бимодальность, стоит подумать о том, как её учитывать в планируемом анализе\n\n\n\n\n\n24.2.3 Медиана\n\n\n\nДля номинальной шкалы мода — это единственно возможная мера центральной тенденции, потому что на этой шкале отсутствует порядок элементов. На других шкалах наблюдения уже можно сортировать по возрастнию или убыванию, поскольку начиная с ранговой (порядковой) шкалы на всех них определена операция сравнения на «больше-меньше».\nВозьмем тот же ряд наблюдений, что и в предыдущем разделе:\n\\[\n\\begin{bmatrix}\n1 & 3 & 4 & 6 & 3 & 2 & 3 & 3 & 2 & 4 & 1\n\\end{bmatrix}\n\\]\nОтсортируем наблюдения по возрастанию:\n\\[\n\\begin{bmatrix}\n1  & 1 & 2 & 2 & 3 & 3 & 3 & 3 & 4 & 4 & 6\n\\end{bmatrix}\n\\]\nНаша задача — определить центральную тенденцию. Давайте посмотрим, что оказалось в середине отсортированного ряда:\n\\[\n\\begin{bmatrix}\n1 & 1 & 2 & 2 & 3 & \\mathbf{3} & 3 & 3 & 4 & 4 & 6\n\\end{bmatrix}\n\\]\nЭто медиана. В данном случае она равна \\(3\\).\n\nОпределение 24.4 Медиана (median) — это значение, которое располагается на середине отсортированного ряда значений переменной.\n\nМедиана делит все наблюдения переменной ровно пополам и половина наблюдений оказывается по одну сторону от медианы, а половина — по другую.\nЕсли число наблюдений нечётное, то всё ясно — в середине отсортированного ряда будет какое-то значение. А если число наблюдений чётное? Тогда мы попадаем между значениями.\nВозьмем для примера такой вектор наблюдений:\n\\[\n\\begin{bmatrix}\n14 & 10 & 9 & 16 & 30 & 3 & 25 & 8 & 18 & 7\n\\end{bmatrix}\n\\]\nОтсортируем:\n\\[\n\\begin{bmatrix}\n3 & 7 & 8 & 9 & 10 & 14 & 16 & 18 & 25 & 30\n\\end{bmatrix}\n\\]\nНайдем середину:\n\\[\n\\begin{bmatrix}\n3 & 7 & 8 & 9 & 10 & | & 14 & 16 & 18 & 25 & 30\n\\end{bmatrix}\n\\]\nВ таком случае в качестве медианы берется среднее между двумя срединными значениями:\n\\[\n\\text{median} = \\frac{10 + 14}{2} = 12\n\\]\nИтого, формализовать вычисление медианы можно следующим образом:\n\\[\n\\text{median}(X) = X(a) =\n\\cases{\nX\\left(\\frac{n+1}{2}\\right), & if  2 | n \\\\\n\\dfrac{X(\\frac{n}{2}) + X(\\frac{n}{2} + 1)}{2}, & otherwise\n}\n\\tag{24.3}\\]\nгде \\(X\\) — ряд наблюдений случайной величины, \\(n\\) — число наблюдений, \\(X(a)\\) — наблюдение с индексом \\(a\\) в отсортированном векторе \\(X\\).\nЕсли мы будем смотреть на медиану с позиции описания распределения, то она будет той самой линией, которая разделит площадь под графиком функции плотности вероятности пополам:\n\n\n\n\n\n\n\n\nРисунок 24.12: Медиана нормального распределения\n\n\n\n\n\nПри этом форма распределения не имеет значения — площадь под графиком всегда будет делиться пополам:\n\n\n\n\n\n\n\n\nРисунок 24.13: Медиана распределения с отрицательной асимметрией.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 24.14: Медиана распределения с положительной асимметрией.\n\n\n\n\n\n\n\n\n\n\n\n\n\nРисунок 24.15: Медиана бимодального распределения.\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home: медиана\n\n\n\n\nмедиану можно расчитать только на шкалах, где задан порядок (ранговая, интервальная, абсолютная)\nмедиана делит выборку наблюдений на две равные части\nлиния медианы раздели площадь под графиком функции плотности вероятности пополам\n\n\n\n\n\n24.2.4 Среднее\n\n\n\nЕсли наша переменная измерена в самых мощных шкалах — интервальной или абсолютной — то нам доступна ещё одна мера центральной тенденции.\n\n24.2.4.1 Арифметическое среднее\n\n\n\nС этим существом все знакомы еще со школы. Арифметическое среднее (arithmetic mean, mean, average) считается так:\n\\[\nM_{X} = \\bar x = \\dfrac{\\sum_{i=1}^{n}x_i}{n},\n\\]\nгде \\(\\bar X\\) — среднее арифметическое, \\(x_i\\) — наблюдение в векторе \\(X\\), \\(n\\) — количество наблюдений.\nНу, то есть всё сложить и поделить на количество того, чего сложили. Изи.\n\n24.2.4.1.1 Свойства среднего арифметического\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то среднее увеличится на это же константу.\n\n\\[\nM_{X+c} = M_X + c\n\\]\nВот почему:\n\\[\nM_{X+c} = \\frac{\\sum_{i=1}^n (x_i + c)}{n} = \\frac{\\sum_{i=1}^n x_i + nc}{n} = \\frac{\\sum_{i=1}^n x_i}{n} + c = M_X + c\n\\]\nИначе говоря, распределение просто сдвинется. Например, если к каждому значению синего распределения прибавить \\(2\\), получится красное:\n\n\n\n\n\n\n\n\n\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то среднее увеличится во столько же раз.\n\n\\[\nM_{X \\times c} = M_X \\times c\n\\]\nВот почему:\n\\[\nM_{X \\times c} = \\frac{\\sum_{i=1}^n (x_i \\times c)}{n} = \\frac{c \\times \\sum_{i=1}^n x_i}{n} = \\frac{\\sum_{i=1}^n x_i}{n} \\times c = M_X \\times c\n\\]\nНапример, здесь каждое значение синего распределения умножили на \\(3\\) и получили красное:\n\n\n\n\n\n\n\n\n\nТут, правда, явно что-то ещё произошло, но мы пока этого не знаем. Однако, отметит этот факт.\n\nСумма отклонений от среднего значения равна нулю.\n\n\\[\n\\sum_{i=1}^n(x_i - M_X) = 0\n\\]\nЭлегантное доказательство:\n\\[\n\\sum_{i=1}^n(x_i - M_X) = \\sum_{i=1}^n x_i - \\sum_{i=1}^n M_X = \\sum_{i=1}^n x_i - nM_X = \\\\\n= \\sum_{i=1}^n x_i - n \\times \\frac{1}{n} \\sum_{i=1}^n x_i = \\sum_{i=1}^n x_i - \\sum_{i=1}^n x_i = 0\n\\]\nНо можно это осмыслить и более просто графически.\nОтклонение — это разность между средним и конкретным значением переменной. И, действительно, так как среднее находится в центре распределения, то часть значений лежит справа, а часть слева. Значит, будут как положительные, так и отрицательные отклонения — и их сумма в итоге будет равна нулю.\n\n\n\n\n\n\n\n\n\n\n\n\n24.2.4.2 Усеченное среднее\n\n\n\nПРО УСЕЧЕННОЕ СРЕДНЕЕ\nСреднее арифметическое не одиноко — есть и другие. Встретяться они вам примерно нигде — то есть о-о-о-очень редко и, скорее всего, в каком-то изощрённом виде. Но упомянуть их, пожалуй, стоит.\n\n\n24.2.4.3 Геометрическое среднее\n\n\n\nРедко встречается в научных работах, но заради общего представления пусть будет. Поскольку оно считается через умножение, то может быть рассчитано только на абсолютной шкале.\n\\[\nG_{X} = \\sqrt[n]{\\prod_{i=1}^n x_i} = \\Big(\\prod_{i=1}^n x_i\\Big)^{\\tfrac{1}{n}}\n\\]\n\n\n24.2.4.4 Квадратичное среднее\n\n\n\n\nА вот это уже более полезная история. Мы с ним столкнёмся далее, правда под разными масками.\n\nКвадратичное среднее (quadratic mean, root mean square, RMS) — это квадратный корень из среднего квадрата наблюдений. Ничего не понятно, поэтому по порядку.\n\nесть наблюдение \\(x_i\\)\nзначит есть и его квадрат \\(x_i^2\\)\nмы умеем считать обычно среднее арифметическое, но ведь \\(x_i^2\\) — это тоже наблюдение, просто в квадрате, так?\nзначит можем посчитать среднее арифметическое квадратов наблюдений — средний квадрат\n\n\\[\n\\frac{\\sum_{i=1}^n x_i^2}{n}\n\\]\n\nнорм, а теперь извлечём из этого дела корень — получим то, что там надо\n\n\\[\nX_{\\mathrm{RMS}} = \\sqrt{\\frac{\\sum_{i=1}^n x_i^2}{n}}\n\\]\nPer se4 мы его вряд ли ещё когда-то увидим, но пару раз оно внезапно всплывет.\n\n\n24.2.4.5 Гармоническое среднее\n\n\n\n\nСуперэкзотичный покемон.\n\n\\[\nH_X = \\frac{n \\prod_{i=1}^n x_i}{\\sum_{i=1}^n (\\tfrac{1}{x} \\prod_{j=1}^n x_j)} = \\frac{n}{\\sum_{i=1}^n \\tfrac{1}{x_i}}\n\\]\n\n\n24.2.4.6 Взвешенное среднее\n\n\n\nЧасто бывает такая ситуация, что нас нужно посчитать среднее по каким-либо имеющимся параметрам, но одни параметры для нас важнее, чем другие. Например, мы хотим вычислить суммарный балл обучающегося за курс на основе ряда работ, выполненных в течение курса, однако мы понимаем, что тест из десяти вопросов с множественном выбором явно менее показателен, чем, например, аналитическое эссе или экзаменационная оценка. Что делать? Взвесить параметры!\nЧто значит взвесить? Умножить на некоторое число. На самом деле, любое. Пусть мы посчитали, что написать эссе в три абстрактных раза тяжелее, чем написать тест, а сдать экзамен в два раза тяжелее, чем написать эссе. Тогда мы можем присвоить баллу за тест вес \\(1\\), баллу за аналитическое эссе вес \\(3\\), а экзамену — вес \\(6\\). Тогда итоговая оценка за курс будет рассчитываться следующим образом:\n\\[\n\\text{final score } = 1 \\cdot \\text{test} + 3 \\cdot \\text{essay} + 6 \\cdot \\text{exam}\n\\]\nСуперкласс. Однако! Весьма вероятно, что в учебном заведении принята единая система оценки для всех видов работ (ну, скажем, некая абстрактная десятибалльная система в сферическом вакууме). Получается, если и за тест, и за эссе, и за экзамен у студента по 10 баллов, то суммарный балл 100, что, кажется, больше, чем 10. Чтобы вернуться к изначальным границам баллов, нужно моделить суммарный балл на сумму весов параметров:\n\\[\n\\text{final score } = \\frac{1 \\cdot \\text{test} + 3 \\cdot \\text{essay} + 6 \\cdot \\text{exam}}{1 + 3 + 6}\n\\]\nКайф! Собственно, это и есть взвешенное среднее. Коэффициенты, на которые мы умножаем значение парамернов, называются весами параметров. И в общем виде формула принимает следующий вид.\n\\[\n\\bar x = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i} = \\sum_{i=1}^n w_i' x_i,\n\\]\nгде \\(x_i\\) — значения конкретных параметров, \\(w_i\\) — веса конкретных параметров, \\(w_i'\\) — нормированные веса параметров.\nВторая часть формулы показывается нам, что можно облегчить себе вычислительную жизнь, если заранее нормировать веса, то есть разделить каждый коэффициент на сумму коэффициентов:\n\\[\nw_i' = \\frac{w_i}{\\sum_{i=1}^n w_i}\n\\]\nТогда сумма коэффициентов будет равна единице. Так чаще всего и поступают, так как тогда коэффициент будет представлять долю, которую весит данный параметр в суммарной оценке. Удобно, практично, красиво.\nВзвещенное среднее часто применяется именно во всякого рода ассессментах, и не только образовательных. Например, вы HR-аналитик и оцениваете персонал. Вы аналитически вычисляете веса коэффициентов (допустим, с помощью линейной регрессии), а далее на их основе высчитаете интегральный балл, по которому будете оценивать сотрудников. Это как один из индустриальных примеров.\nТакже оно применяется, когда в наших данных есть какая-то группировка (например, когорты), при этом группы неравномерны.\n\n\n\n24.2.5 Среднее vs медиана\n\n\n\nПомимо того, что среднее и медиана информативны сами по себе, полезно смотреть на их взаимное расположение.\n\nСравнивать будем моду, медиану и среднее [арифметическое].\n\nИтак, все три статистики — мода, медиана и среднее — описывают центральную тенденцию — некоторое значение изучаемой нами переменной, вокруг которого собираются другие значения. Но если их три и все они используются, значит между ними должны быть какие-то различия. Посмотрим, какие.\nВо-первых, очевидно, что моду невозможно посчитать для непрерывной переменной.\n\n\n\nНет, не очевидно\n\nТак как вероятность того, что непрерывная случайная величина принимает своё конкретное значение, равна нулю, каждое наблюдение в нашей выборке будет уникально — встретится ровно один раз. Вспомните [посмотрите] пример из предыдущей главы, где мы набирали числа из отрезка. Получается, что мода теряет свой смысл.\n\n\nВо-вторых, медиану нельзя посчитать на номинальной шкале. Кстати, почему?\n\n\n\nПотому что\n\nна номинальной шкале нет отношения порядка между элементами. Помните, на ней нельзя сравнивать на больше-меньше. Поэтому невозможно отсортировать наблюдения, а значит, и найти медиану.\n\n\nВ-третьих, среднее тоже нельзя посчитать на номинальной шкале.\n\n\n\nМожно, но осторожно\n\nВообще, конечно, да — нельзя, потому что на номинальной шкале не определена операция сложения, входящая в вычисление среднего. Однако если на номинальной шкале есть только две категории, которые закодированы 0 и 1, то посчитать среднее можно. Но что оно будет значить?\nИсходный математический смысл среднего явно утерян. Посмотрим на это по-другому: посчитать сумму единиц это всё равно, что посчитать количество единиц. То есть, если мы сложим все нули и единицы, то получим количество единиц среди всех наших наблюдений. А разделив количество единиц на количество наблюдений, мы получим долю единиц — то есть долю наблюдений с лейблом 1.\nВот так вот.\n\n\nВ-четвертых, для дискретной переменной значение среднего арифметического будет не особо осмысленно. Ну, скажем, странно сказать, что в аудитории в среднем стоят 15.86 столов или в российских семьях в среднем 1.5 ребенка. Конечно, в ряде случаев можно это как-то более-менее водержательно интерпретировать, но это требует усилий, а мы ленивые, поэтому лучше использовать медиану.\nИтого, делаем следующие выводы:\n\nдля номинальной шкалы пригодна только мода\nдля дискретных переменных подходят мода и медиана\n\nмода иногда лучше, так как точно всегда будет целым числом\n\nдля непрерывных переменных подходят медиана и среднее\n\nТеперь нам надо разобраться, как будут себя вести меры центральной тенденции в зависимости от формы распределения.\nНа симметричном распределении мода, медиана и среднее совпадают [или, по крайней мере, находятся очень близко друг к другу]. Здесь и далее: красная линия — среднее, синяя — медиана, зелёная — мода.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nНа асимметричном распределении мода [практически] в пике. Практически, потому что функция плотности вероятности [черная линия на графике] на всегда точно аппроксимирует (в данном случае то же, что и сглаживает) эмпирическое распределение. На картинке ниже мы видим, что на гистограмме мода — самый высокий столбик, что и показывает нам зелёная линия, которой обозначена мода. Однако при сглаживании гистограммы пик немного съехал, и мода оказалась не совсем в вершине графика функции плотности вероятности.\nВообще-то это нормально, потому что мода для непрерывной величины, которую мы и визуализируем с помощью графика плотности, либо не может быть посчитана вовсе, либо — если так получилось, и у нас все же есть повторяющиеся значения — не слишком хорошая мера центральной тенденции. В целом, и на симметричном распределении мода тоже может находиться немного в стороне от пика.\nНа асимметричном распределении медиана и среднее смещены в сторону хвоста. Среднее смещено сильнее медианы. Это связано с тем, что медиана зависит только от количества наблюдений, а среднее ещё и от самих значений. На картинке ниже пример для распределения с правосторонней асимметрии (потому что хвост справа) — среднее (красная линия) правее медианы (синяя линия).\n\n\n\n\n\n\n\n\n\nА это пример для распределения с левосторонней асимметрией (так как хвост слева) — среднее (красная линия) левее медианы (синяя линия).\n\n\n\n\n\n\n\n\n\nДля того, чтобы лучше разобраться с тем, как большие и малые значения влияют на моду и медиану посмотрим такой пример. Пусть у нас есть оценки за выпускную квалификационную работу. Например, такие:\n\nmarks\n\n[1] 6 7 7 8 8\n\n\nПосчитаем медиану и среднее:\n\nmedian(marks)\n\n[1] 7\n\nmean(marks)\n\n[1] 7.2\n\n\nСреднее \\(7.2\\) округлиться до \\(7\\), то есть можно считать, что среднее и медиана совпали. Ну, ок.\nНо в комиссии сидят два требовательных доктора наук, которые поставили оценки, сильно отличающиеся от остальных:\n\nmarks\n\n[1] 6 7 7 8 8 3 4\n\n\nПосчитаем медиану и среднее теперь:\n\nmedian(marks)\n\n[1] 7\n\nmean(marks)\n\n[1] 6.142857\n\n\nМедиана осталась на месте — всё ещё \\(7\\). А вот среднее \\(6.1\\) округлится до \\(6\\). Казалось бы, это немного, но в смысле оценок — это прилично, и может сильно повлиять на GPA.\nИтого, среднее более чувствительно к нетипичным значениям (очень большим или очень малым).\nЕсть ещё один интересный вариант распределений — бимодальные. Значит ли, что у этого распределения две моды? Не всегда. Посмотрим пример ниже:\n\n\n\n\n\n\n\n\n\nМы видим, что на графике есть два пика, однако строго математически мода одна (зеленая линия) — и она в более высоком пике. Это логично, ибо там самые часто встречающиеся значения.\n\nИ все жё содержательно мы не можем пренебречь вторым пиком. Почему нам он важен? Обычно бимодальное распределение — это повод задуматься о том, что наша выборка неоднородна. Бимодальное распределение как бы сложено из двух с центрами в двух пиках. То есть в нашей выборке как будто бы две подвыборки, которые обладают разными распределениями интересующего нам признака.\nЧто с этим делать? Хорошо всегда иметь в данным какие-либо дополнительные переменные — как минимум соцдем — чтобы мы могли по данным попытаться предположить, какую группировку мы могли забыть учесть при планировании исследования.\n\nСо средним и медианой происходит примерно то же, что и в случае асимметричного распределения. Второй пик смещает к себе обе меры центральной тенденции, причем среднее вновь сильнее, чем медиану.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Описательные статистики</span>"
    ]
  },
  {
    "objectID": "andan-descriptives.html#andan-descriptives-variability",
    "href": "andan-descriptives.html#andan-descriptives-variability",
    "title": "24  Описательные статистики",
    "section": "24.3 Меры разброса",
    "text": "24.3 Меры разброса\n\n\n\nИтак, мы разобрались с мерами центральной тенденции. Однако для описания распределения их оказвается недостаточно. Почему?",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Описательные статистики</span>"
    ]
  },
  {
    "objectID": "andan-descriptives.html#why_we_need_variation",
    "href": "andan-descriptives.html#why_we_need_variation",
    "title": "24  Описательные статистики",
    "section": "24.4 Зачем нужны меры разброса",
    "text": "24.4 Зачем нужны меры разброса\nПосмотрим на несколько распределений:\n\nset.seed(123)\ntibble(id = 1:100,\n       x1 = rnorm(100, mean = 2, sd = 1),\n       x2 = rnorm(100, mean = 2, sd = 3),\n       x3 = rnorm(100, mean = 2, sd = 0.5)) -&gt; rnorm_three\n\n\n\n\n\n\n\n\n\n\nМетодом пристального взгляда можно установить, что у всех распределений одинаковые средние:\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nОднако мы видим, что значения по-разному группируются вокруг среднего. Как они группируются — плотно, как на третьем рисунке, или не особо, как на втором — можно описать с помощью мер разброса, или мер вариативности.\n\n24.4.1 Основные характеристики статистических данных\nВообще если посмотреть на это более свысока, то необходимость описания разброса определяется тем, что статистические данные обладают двумя ключевыми особенностями — неопределенностью и вариативностью.\n\nНеопределённость нам говорит о том, что мы не знаем, что именно мы получим в результате наших измерений для конкретной выборки. В том числе потому, что мы работаем на просторах случайных величин.\nВариативность означает, что наши данные будут различатся ещё и от респондента к респонденту. И между выборками тоже. Здесь и ошибка измерения, и различные смешения и ещё куча всего.\n\nБолее того, вариативность настолько важна, что она входит в расчёт любого статистического критерия. Именно вариативность — а не центральная тенденция — позволяет нам сделать вывод о том, что наши выборки различаются (или нет).\n\n\n24.4.2 Минимум, максимум, размах\n\n\n\nНачнем с самого простого. Как наиболее просто описать вариативность? Мы работаем с выборкой, а в выборке, как известно, ограниченное число наблюдений. А если оно ограниченое, значит среди них точно есть наибольшее — максимальное — и наименьшее — минимальное.\nДопустим, мы открыли ведомость по «Анатомии и физиологии ЦНС» некоторой академической группы и пронаблюли следующее:\n\nanat_marks\n\n [1]  7  4  6  9 10  5  6  9  6  6  3  6  8  8  5 10  7  5  7  3  9  4  8  3  8\n[26]  4  6  8  7  5\n\n\nМы можем посчитать минимальное и максимальне значение по этому ряду наблюдений:\n\nmin(anat_marks)\n\n[1] 3\n\nmax(anat_marks)\n\n[1] 10\n\n\nПолучается, что оценки варьируются от \\(3\\) до \\(10\\). Ну, приемлемо. Разница между максимальным и минимальным значением называется размах (range):\n\\[\n\\mathrm{range}(X) = \\max(X) - \\min(X)\n\\]\nПравда вот функция range в R вернёт не само значение размаха, а минимальное и максимальное значение. Ну, ладно.\n\nrange(anat_marks)\n\n[1]  3 10\n\n\nИ вот мы преисполнившиеся идёт описывать вариативность переменной с помощью размаха, но обнаруживаем в другой ведомости этой же группы (по «Введению в психологию») вот что:\n\nintro_psy_marks\n\n [1]  6  8  4  6  7  5  7 10  4  6  7  8  7  6  8 10  8  7  7  6  8  7  6  8  6\n[26]  3  8  6  6  4\n\n\nРазмах вроде как такой же:\n\nrange(intro_psy_marks)\n\n[1]  3 10\n\n\nЗначит ли это, что вариативность одинаковая?\nНарисуем.\n\n\n\n\n\n\n\n\n\nКажется, что вариативность различна. Распределение оценок по «Анатомии и физиологии ЦНС» более равномерное, в то время как оценки по «Введению в психологию» активнее группируются где-то в середине.\nШтош, размах хоть и дает нам некоторую информацию о вариативности, нам этого маловато. Будем искать другие меры разброса.\n\n\n24.4.3 Среднее абсолютное отклонение\n\n\n\n\n24.4.3.1 Среднее абсолютное отклонение от среднего\n\n\n\n\n\n24.4.3.2 Среднее абсолютное отклонение от медианы\n\n\n\n\n\n24.4.3.3 Медианное абсолютное отклонение\n\n\n\n\n\n\n24.4.4 Дисперсия\n\n\n\nХотя описание разброса переменных с помощью квантилей (в частности, квартилей) может дать нам много полезной информации, все же у них есть существенный недостаток: они никак не взаимодействуют с самими значениями нашей переменной.\nДействительно, мы делим нашу сортированную выборку на равные части, и смотрим, что в эти части попало. Но хотелось бы как-то учесть ещё и сами значения переменной в некотрой числовой мере разброса.\nНу, хорошо. Поступим следующим образом. Мы все ещё хотим узнать, как наши значения группируются вокруг среднего. В предыдущей главе мы уже видели, что наши наблюдения отклоняются от среднего значения — значит мы можем посчитать отклонение для каждого наблюдения:\n\\[\n\\bar x - x_i\n\\]\nОкей. Если мы сложим все отклоненияи и поделим на их количество (которое равно количеству наблюдений), то мы получим среднее отклонение, да?\n\\[\n\\frac{1}{n} \\sum_{i=1}^n \\bar x - x_i\n\\]\nДа. Однако есть одна проблема. В прошлой главе мы выяснили, что сумма отклонений от среднего равна нулю, а значит и среднее отклонение также будет равно нулю.\nХорошо. Но отрицательные значения ведь можно победить! Есть два пути:\n\nМодуль. Преимущество первого в том, что размерность величины разброса остается той же, что и у измеряемой переменной.\nКвадрат. Преимущество второго в том, что сильные отклонения будут оказывать более сильное влияние на окончательное значение статистики, в то время как для первого малые и большие отклонения равноценны.\n\nВторой путь на практике оказывается полезнее, так как мы хотим, чтобы сильно отличающиеся наблюдения вносили вклад в меру разброса.\nВозведя отклонения в квадрат, получим формулу дисперсии (вариации, variation):\n\\[\nD(X) = \\mathrm{var}(X) = \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (\\bar x - x_i)^2\n\\]\nГениально.\nНе совсем. Формула, которую мы получили, пригодна для расчета дисперсии генеральной совокупности — на выборке же она будет давать неточную оценку.\nЧтобы получить точную (несмещенную) оценку дисперсии по выборке, нам нужно исправить знаменатель дроби — вместо \\(n\\) использовать \\(n-1\\):\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x - x_i)^2\n\\]\nНо почему?\n\n24.4.4.1 Степени свободы\nВо всём виновата выборка.\nВзглянем на формулу дисперсии: в неё входит среднее арифметическое. То есть для того, чтобы рассчитать дисперсию на выборке, сначала нам необходимо на этой же выборке рассчитать среднее. Тем самым, мы как бы «фиксируем» нашу выборку этим средним значением — у значений нашего распределения становится меньше свободы для варьирования. Теперь свободно варьироваться могут \\(n-1\\) наблюдение, так как последнее всегда будет возможно высчитать, исходя из среднего значения. По этой причине нам необходимо корректировать исходную формулу расчета дисперсии.\nА что если не корректировать?\nМы стремимся к тому, чтобы наши расчеты на выборке достаточно точно [на столько, на сколько это возможно] отражали то, что происходит в генеральной совокупности. Математики-статистики выяснили, что та оценка, которая хорошо подходит для расчета дисперсии генеральной совокупности, при применении на выборке даёт смещенные оценки. То есть оценка выборочной дисперсии по формуле дисперсии для генеральной совокупности содержит в себе смещение — некоторую систематическую ошибку. Это нехорошо.\nК концепту степеней свободы мы ещё неоднократно вернемся. Сейчас хотелось бы, чтобы сформировалось какое-то минимальное более-менее освязаемое понимание того, почему они вообще нам нужны. Если на основе предыдущих абзацев раздела этого сделать не получилось, то давайте попробуеи воспользоваться следующим рассуждением.\nНа выборке происходят некоторые статистические преколы, которые несколько портят нам жизнь, и нам их неободимо учесть, чтобы адекватно оценивать то, что происходит в генеральной совокупности. В частности, нам необходимо учитывать количество степеней свободы, которое есть в нашей выборке. Для расчета выборочной дисперсии оно равно \\(n-1\\), так как мы для того, чтобы рассчитать дисперсию по выборке, нам сначала по той же самой выборке надо рассчитать ещё одну оценку — среднее арифметическое. Этот расчет заберет одну степень свободы у нашей выборки.\n\n\n24.4.4.2 Дисперсия генеральной совокупности\n\n\n\n\n\n24.4.4.3 Дисперсия выборки\n\n\n\n\n\n\n24.4.5 Стандартное отклонение\n\n\n\nИ вот мы получили невероятное! У нас есть формула расчета меры разброса, которая позволяет учесть сами значения переменной! Ну не чудо ли!\nЧудо, конечно, однако есть некоторая проблема. Мы возводили отклонения в квадрат. Представим, что мы хотим посчитатить дисперсию роста студентов психфака. Пусть мы измеряли рост в метрах. Отклонения тоже будут в метрах (потому что среднее — это тоже метры, а если из метров вычитать метры, то мы получим метры). А при возведении метров в квадрат получаются метры в квадрате. Очевидно, что если мы модели квадратные метры на некоторое число (\\(n\\)), они все еще останутся метрами в квадрате.\nО, нет! А счастье было так близко, так возможно! Получается, мы не можем интерпретировать эту меру разброса? Не сможем даже нарисовать?\nДа, но это не очень большая беда. Для того, чтобы вернуться обратно к единицам измерения нашей переменной, нам всего лишь нужно извлечь корень из дисперсии:\n\\[\n\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\bar x - x_i)^2}\n\\]\nМы получили величину, называемую стандартным отклонением (standard deviation). Чем она хороша? Тем, что её размерность совпадает с размерностью нашей переменной. Стандартное отклонение уже может быть достаточно интерпретабельно и хорошо визуализируемо.\nКстати, формула выше, которая что-то очень напоминает, — это стандартное отклонение генеральной совокупности, потому что под корнем стоит дисперсия генеральной совокупности.\nЧтобы посчитать стандартное отклонение по выборке, нам надо извлечь корень из выборочной дисперсии:\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (\\bar x - x_i)^2}\n\\]\n\n\n24.4.6 Свойства дисперсии и стандартного отклонения\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то дисперсия не изменится.\n\n\\[\nD_{x+c} = D_{x}\n\\]\nВот почему:\n\\[\nD_{x+c} = \\frac{\\sum_{i=1}^n \\big((\\bar x + c) - (x_i + c)\\big)^2}{n-1} = \\frac{\\sum_{i=1}^n \\big(\\bar x + c - x_i - c\\big)^2}{n-1} = \\frac{\\sum_{i=1}^n \\big(\\bar x - x_i\\big)^2}{n-1} = D_x\n\\]\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то дисперсия увеличится в \\(c^2\\) раз.\n\n\\[\nD_{x \\times c} = D_{x} \\times c^2\n\\]\nВот почему:\n\\[\nD_{x \\times c} = \\frac{\\sum_{i=1}^n (c\\bar x - cx_i)^2}{n-1} = \\frac{\\sum_{i=1}^n c^2(\\bar x - x_i)^2}{n-1} = \\frac{c^2 \\sum_{i=1}^n (\\bar x - x_i)^2}{n-1} = D_x \\times c^2\n\\]\n\nЕсли к каждому значению распределения прибавить некоторое число (константу), то стандартное отклонение не изменится.\n\n\\[\ns_{x+c} = s_x\n\\]\nЭто следует из свойства дисперсии:\n\\[\ns_{x+c} = \\sqrt{D_{x+c}} = \\sqrt{D_x} = s_x\n\\]\nКак мы уже видели, распределение просто сдвигается на константу. Например, если к каждому значению синего распределения прибавить \\(2\\), получится красное — разброс у обоих распределений одинаковый:\n\n\n\n\n\n\n\n\n\n\nЕсли каждое значение распределение умножить на некоторое число (константу), то стандартное отклонение увеличится во столько же раз.\n\n\\[\ns_{x \\times c} = s_x \\times c\n\\]\nЭто также следует из свойства дисперсии:\n\\[\ns_{x \\times c} = \\sqrt{D_{x \\times c}} = \\sqrt{D_x \\times c^2} = s_x \\times c\n\\]\nНапример, здесь каждое значение синего распределения умножили на \\(3\\) и получили красное — разброс также увеличился в три раза, поэтому распределение более плоское:\n\n\n\n\n\n\n\n\n\n\n\n24.4.7 Квантили",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Описательные статистики</span>"
    ]
  },
  {
    "objectID": "andan-descriptives.html#quantiles",
    "href": "andan-descriptives.html#quantiles",
    "title": "24  Описательные статистики",
    "section": "24.5 Квантили",
    "text": "24.5 Квантили\nВозьмем распределение суммарного балла по шкале «Доверие к техническим интеллектуальным системам». Выглядит оно как-то так:\n\n\n\n\n\n\n\n\n\nТеперь нам понадобится определение квантиля распределения.\nКвантиль — это значение переменной, которое не превышается с определенной вероятностью (обозначим её \\(p\\)). Иначе говоря, слева от значения квантиля лежит \\(p\\%\\) наблюдений.\nПосмотрим на картинки.\nСлева относительно квантиля-0.05 (\\(x_{0.05}\\)) лежит 5% наблюдений:\n\n\n\n\n\n\n\n\n\nСлева относительно квантиля-0.68 (\\(x_{0.68}\\)) лежит 68% наблюдений:\n\n\n\n\n\n\n\n\n\nСлева относительно квантиля-0.99 (\\(x_{0.99}\\)) лежит 99% наблюдений:\n\n\n\n\n\n\n\n\n\nИтак, мы поняли, а также приняли и осознали, что такое квантиль. Неясно только, как он нам поможет описать вариативность данных.\n\n24.5.1 Квартили\nДля этого нам пригодятся специально обученные квантили. Оказалось достаточно удобно поделить все наблюдение на четыре равные части — вот так:\n\n\n\n\n\n\n\n\n\nЗначения переменной, которые делят выборку на четыре равные части называются квартили. Получается, что\n\nслева от первого (нижнего) квартиля (\\(Q_1\\), \\(x_{0.25}\\)) лежит 25% наблюдений\nслева от второго (среднего) квартиля (\\(Q_2\\), \\(x_{0.50}\\)) лежит 50% наблюдений\n\nа значит и справа 50% — получается второй квартиль делит выборку пополам — это медиана\n\nслева от третьего (верхнего) квартиля (\\(Q_3\\), \\(x_{0.75}\\)) лежит 75% наблюдений\n\nЧетвертый квартиль не используется, потому что это максимальное значение — слева от него лежит 100% наблюдений.\nКстати, можно также отметить, что первый квартиль — это медиана нижней (меньшей) половины наблюдений, а третий — медиана верней (большей) половины наблюдений.\nВот такая вот прикольная история.\n\n\n24.5.2 Децили\nК слову, делить выборку можно не только на четверти — можно поделить, скажем, на 10 частей и получить децили. Так, слева от первого дециля (\\(x_{0.10}\\)) лежит 10% наблюдений, а слева от третьего (\\(x_{0.30}\\)) — 30%.\nДецили встречаются редко (в основном в психометрике), но знать о них полезно.\n\n\n24.5.3 Перцентили\nГораздо чаще встречаются перцентили — значения переменной, которые делят выборку на 100 равных частей. Например, так устроен ваш рейтинг. Только стоит помнить, что в рейтинге отсчет ведется от максимального среднего балла, поэтому если у вас нулевой перцентиль (\\(x_{0.00}\\)) по программе, значит выше вас в рейтинге никого нет. А если ваш перцентиль, скажем, 36-ой (\\(x_{0.36}\\)), то выше вас в рейтинге 36% ваших однокурсников, то есть вы все ещё в первой половине рейтинга, что очень неплохо!\n\n\n24.5.4 Интерквартильный размах\nИ — о, ура! — мы наконец-то добрались до того, ради чего тут собрались! Зная первый и третий квартили распределения, можно рассчитать интерквартильный (межквартильный) размах (interquartile range, IQR).\n\\[\n\\mathrm{IQR}(X) = Q_3(X) - Q_1(X)\n\\]\nИнтерквартильный размах — это разница между третьим и первым квартилем распределения. Эта величина описывает интервал значений признака, в котором лежит 50% наблюдений.\n\n\n\n\n\n\n\n\n\nВ данном случае он равен 40:\n\nIQR(taia$DT)\n\n[1] 40\n\n\nТо есть 50% наблюдений лежит в пределах 40 единиц шкалы.\n\n\n24.5.5 Визуализация квартилей. Боксплот\nОтображать квартили на гистограмме, во-первых, совершенно неудобно, а во-вторых, не то чтобы график получается информативный. Для визуализации квартилей придумали специальный тип графика — ящик с усами, или боксплот (boxplot).\n\n\n\n\n\n\n\n\n\nПрикольная ерунда. Научимся его читать.\nЗначения переменной идут по вертикальной оси (оси ординат). По горизонтальной оси (оси абсцисс) здесь ничего не идет5. Жирная линия по середине ящика — медиана (второй квартиль). Нижняя граница ящика — первый квартиль, верхняя — третий. Получается, что границы ящика показывают нам значения, в пределах которых лежит половина наблюдений.\nНижний ус — первый квартиль минус полтора межквартильных размаха. Верхний ус — третий квартиль плюс полтора мехквартильных размаха.\n\n\n\n\n\n\n\n\n\n\n\n\nЗамечание\n\nЯщик может быть асимметричным — то есть верхняя его часть (расстояние между медианой и третьим квартилем) и нижняя его часть (расстояние между медианой и первым квартилем) могут быть разными. Это нам говорит об асимметричности распределения. Усы также могут быть неравными, если один из них упирается в максимум / минимум — тоже по причине асимметричности распределения.\n\n\nНу, допустим. А что тогда точки?\n\n24.5.5.1 Выбросы\nВообще справедливо было бы задаться вопросом, а зачем нам вообще усы на этом графике? И почему мы прибавляем полтора межквартильных размаха?\nЭто один из подходов к определению нехарактерных значений — выбросов. При исследовании данных мы часто задаемся вопросом, если ли в наших данных такие значения, которые сильно отличаются от распределения той или иной переменной. Но как определить это самое «сильно»?\nВот один из подходов. Будем считать, что значения, которые укладываются в интервал \\((Q_1 - 1.5 \\times \\mathrm{IQR}, \\, Q_3 + 1.5 \\times \\mathrm{IQR})\\), нас устраивают. Все что попадает в этот интервал — это «нормальные», типичные значения нашей переменной. Те же, которые будут находиться за пределами этого интервала, мы назовем нетипичными, аномальными значениями, или выбросами. Эти значения и будут отмечены точками на графике boxplot.\nЧто с ними делать? Во-первых, содержательной анализировать. Выбросы могут возникнуть по разным причинам. Может быть испытуемый отвлекся на прилетевшего в окно голубя, и у нас в данных появилось время реакции 200 секунд. Такие выбросы мы можем исключить из данных. А возможно в нашу выборку попали какие-то люди, которые, скажем, очень сильно или очень слабо доверяют искусственному интеллекту (как в примере на рисунке). Эти наблюдения необходимо дополнительно проанализировать — возможно, это представители специфических групп нашей генеральной совокупности (например, программисты-разработчики или люди пенсионного возраста). Анализ принесет нам дополнительную информацию, которую мы могли не учесть при планировании исследования. Крч, думать надо. И собирать побольше данных, чтобы можно было найти содержательную интерпретацию происходящему.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Описательные статистики</span>"
    ]
  },
  {
    "objectID": "andan-descriptives.html#andan-descriptives_variation_comparison",
    "href": "andan-descriptives.html#andan-descriptives_variation_comparison",
    "title": "24  Описательные статистики",
    "section": "24.6 Сравнение мер разброса",
    "text": "24.6 Сравнение мер разброса\nКак и разные меры центральной тенденции, разные меры разброса по-своему хороши. Более того, они дружат с мерами центральной тенденции. Так, с медианой используется мехквартильных размах, а со средним арифметическим — стандартное отклонение.\nРазмах подходит для всего сразу. Его стоит рассчитать, чтобы составить самое первое представление в разбросе, о границах измерения изучаемого признака [на нашей выборке].\nСтоит также отметить, что все, что мы тут обсуждали, совершенно не годиться для номинативных переменных. Однако у них тоже есть вариативность. Согласитель, что выборка из Питера, Москвы, и Казани более вариативна, чем выборка из Москвы. Аналогом меры разброса для номинальной переменной можно назвать количество уникальных значений этой переменной.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Описательные статистики</span>"
    ]
  },
  {
    "objectID": "andan-descriptives.html#andan-descriptives-skewness",
    "href": "andan-descriptives.html#andan-descriptives-skewness",
    "title": "24  Описательные статистики",
    "section": "24.7 Асимметрия",
    "text": "24.7 Асимметрия",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Описательные статистики</span>"
    ]
  },
  {
    "objectID": "andan-descriptives.html#andan-descriptives-kurtosis",
    "href": "andan-descriptives.html#andan-descriptives-kurtosis",
    "title": "24  Описательные статистики",
    "section": "24.8 Эксцесс",
    "text": "24.8 Эксцесс",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Описательные статистики</span>"
    ]
  },
  {
    "objectID": "andan-descriptives.html#andan-descriptives-final",
    "href": "andan-descriptives.html#andan-descriptives-final",
    "title": "24  Описательные статистики",
    "section": "24.9 Итоги",
    "text": "24.9 Итоги\n\n\n\n\nShepard, Roger N., and Jacqueline Metzler. 1971. “Mental Rotation of Three-Dimensional Objects.” Science 171 (3972): 701–3. https://doi.org/10.1126/science.171.3972.701.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Описательные статистики</span>"
    ]
  },
  {
    "objectID": "andan-corr.html#andan-corr-cov",
    "href": "andan-corr.html#andan-corr-cov",
    "title": "27  Корреляционный анализ",
    "section": "27.1 Ковариация",
    "text": "27.1 Ковариация\nМы хотим описать имеющиеся взаимосвязи как можно проще и опираясь на то, что у нас уже есть. Мы говорили, что дисперсия, или вариация, заключает в себе информацию об изменчивости признака. Если мы хотим исследовать взаимосвязь между признаками, то логично будет посмотреть, как изменяется один из признаков при изменении другого — иначе говоря, рассчитать совместную изменчивость признаков, или ко-вариацию (covariance).\nКак мы её будем считать? Подумаем графически. Расположим две переменные на осях и сопоставим каждому имеющемуся наблюдению точку на плоскости.\nКАРТИНКА\nОтметим средние значения по обеим переменным.\nКАРТИНКА\nЗаметим, что если наши наблюдения по переменной \\(x_1\\) отклоняются в большую сторону, то они отклоняются в большую сторону и по переменной \\(x_2\\). Аналогично, если они будут отклоняться в меньшую сторону по \\(x_1\\), то в меньшую же сторону они будут отклоняться и по \\(x_2\\).\nКАРТИНКА\nПолучается, мы можем на основании согласованности отклонений уже заключить о направлении связи. Произведение отклонений по обеим величинам будет положительно, если отклонения сонаправленны. Запишем это математически.\n\\[\n(\\bar x_1 - x_{i1}) (\\bar x_2 - x_{i2}) &gt; 0 \\Leftarrow \\big( (\\bar x_1 - x_{i1}) &gt; 0 \\wedge (\\bar x_2 - x_{i2}) &gt; 0 \\big) \\vee \\big( (\\bar x_1 - x_{i1}) &lt; 0 \\wedge (\\bar x_2 - x_{i2}) &lt; 0 \\big)\n\\]\nСоответственно, если отклонения будут направлены в разные стороны, из произведение будет отрицательным. Ну, осталось только понять, как совместные отклонения организованы в среднем — это и будет ковариацией двух величин:\n\\[\n\\mathrm{cov}(X_1, X_2) = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x_1 - x_{i1}) (\\bar x_2 - x_{i2})\n\\]\n\nЧто такое ковариация величины самой с собой (\\(\\mathrm{cov}(X_1, X_1)\\))?\nПопробуйте получить ответ через вывод формулы.\n\nВажно отметить, что ковариация улавливается только линейную составляющую взаимосвязи между признаками, поэтому если \\(\\mathrm{cov}(X_1,X_2) = 0\\), то мы можем сказать, что между переменными нет линейной взаимосвязи, однако это не значит, что между этими переменными нет никакой другой зависимости.\nКАРТИНКА\nУ ковариации есть два важных недостатка:\n\nэто размерная величина, поэтому её значение зависит от единиц измерения признаков,\nона зависит от дисперсий признаков, поэтому по её значению можно определить только направление связи (прямая или обратная), однако ничего нельзая сказать о силе связи.\n\nПоэтому нам нужно как-то модицифировать эту статистику, чтобы мы могли больше вытащить из её значения.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "andan-corr.html#andan-corr-cor",
    "href": "andan-corr.html#andan-corr-cor",
    "title": "27  Корреляционный анализ",
    "section": "27.2 Корреляция",
    "text": "27.2 Корреляция\nРаз ковариация зависит от дисперсии, то можно сделать некоторые математические преобразования, чтобы привести эмпирические распределения к какому-то одному виду — сделать так, чтобы они имели одинакое математическое ожидание (среднее) и одинаковую дисперсию. С этой задачей прекрасно справляется стандартизация. Напоминаю формулу:\n\\[\nx_i^* = \\frac{x_i - \\bar x}{s}\n\\]\nПосле такого преобразования математическое ожидание нашего распределения будет равно нуля, а стандартное отклонение — единице. Это избавит нас от влияния дисперсии на значение ковариации. Ковариация двух стандартно нормально распределенных величин называется корреляцией (correlation).\n\\[\n\\mathrm{cov}(X_1^*, X_2^*) = \\frac{1}{n-1} \\sum_{i=1}^n x_{i1}^* x_{i2}^* = \\mathrm{corr}(X_1, X_2),\n\\] где \\(X_1^*\\) и \\(X_2^*\\) — стандартизированные величины \\(X_1\\) и \\(X_2\\) соответственно.\nКорреляцию можно выразить через ковариацию:\n\\[\n\\mathrm{corr}(X_1, X_2) = \\frac{1}{n-1} \\sum_{i=1}^n \\Big( \\frac{\\bar x_1 - x_{i1}}{s_1} \\Big) \\Big( \\frac{\\bar x_2 - x_{i2}}{s_2} \\Big) =\n\\frac{1}{s_1 s_2} \\Big( \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x_1 - x_{i1})(\\bar x_2 - x_{i2}) \\Big) = \\frac{\\mathrm{cov}(X_1, X_2)}{s_1 s_2}\n\\]\nЕсли внимательно всмотреться в формуле, то можно обнаружить, что корреляция это не что иное, как стандартизированное значение ковариации.\nКоэффициент корреляции имеет четкие пределы изменения: \\([-1; \\,1]\\). Крайнее левое значение говорит о том, что присутствует полная обратная линейная взаимосвязь, крайнее правое — что присутствует полная прямая линейная взаимосвязь. Как и ковариация, корреляция ловит только линейную составляющую связи, поэтому нулевое значение корреляци показывает, что между переменными отсутствует линейная взаимосвязь. Это всё еще не значит, что связи нет вовсе.\n\n27.2.1 Тестирование статистической значимости коэффициента корреляции\nОценку коэффициента корреляции мы получаем методом моментов, заменяя истинный момент \\(\\rho_{ij}\\) выборочным \\(r_{ij}\\):\n\\[\n\\hat \\rho_{ij} = \\overline{\\big( (X_{ki} - \\bar X_i) (X_{kj} - \\bar X_j) \\big)} = r_{ij}\n\\]\nЕсли в генеральной совокупности связь между признаками отсутствует, то есть \\(\\rho_{ij} = 0\\), будет ли равен нулю \\(r_{ij}\\)? Можно с уверенностью сказать, что не будет, так как выборочный коэффициент корреляции — случайная величина. А мы помним, что вероятность принятия случайной величиной своего конкретного значения равна нулю.\nТогда необходимо протестировать статистическую гипотезу:\n\\[\nH_0: \\rho_{ij} = 0 \\; \\text{(линейной связи нет)} \\\\\nH_1: \\rho_{ij} \\neq 0 \\text{(наиболее частый вариант альтернативы)}\n\\]\nДля проверки нулевой гипотезы используется следующая статистика:\n\\[\nt = \\frac{r_{ij}}{\\sqrt{\\frac{1 - r^2_{ij}}{n-2}}} \\overset{H_0}{\\thicksim} t(\\nu = n-2)\n\\]\nВывод о статистической значимости коэффициента корреляции делается согласно алгоритму тестировния статистических гипотез.\n\n\n27.2.2 Доверительный интервал для коэффициента корреляции\nС построением интервальной оценки коэффциента корреляции возникают некоторые сложности. Наша задача состоит в том, чтобы определить в каких границах будет лежать значение истинного коэффициента корреляции с заданной вероятностью:\n\\[\n\\mathrm{P} (\\rho_{ij,\\min} &lt; \\rho_{ij} &lt; \\rho_{ij,\\max}) = \\gamma\n\\]\nНам необходимо найти статистику, закон распределения корой известен, однако ранее упомянутся статистика не подходит, так как она имеет распределение Стьюдента, когда верна нулевая гипотеза об отсутствии связи. Если же мы строим интервальную оценку, нас интересует случай наличия связи.\nТакую статистику искали долго, и её удалось найти, когда ввели определённое преобразование выборочного критерия корреяции — z-преобразования Фишера:\n\\[\nz(r_{ij}) = \\frac{1}{2} \\ln \\frac{1 + r_{ij}}{1 - r_{ij}} \\thicksim \\mathrm{N}(\\bar z_{ij}, \\tfrac{1}{n-3}),\n\\] где \\(n\\) — объём выборки, а \\(\\bar z_{ij}\\) получается расчётом по указанной формуле после подставления точечной оценки коэффициента корреляции.\nТогда интервальная оценка для величины \\(z_{ij, \\mathrm{true}}\\) приобретает такой вид:\n\\[\n\\mathrm{P} \\Big( \\bar z_{ij} - t_\\gamma \\sqrt{\\tfrac{1}{n-3}} &lt; z_{ij, \\mathrm{true}} &lt; \\bar z_{ij} + t_\\gamma \\sqrt{\\tfrac{1}{n-3}}  \\Big) = \\gamma = \\Phi(t_\\gamma)\n\\]\nДалее путём обратного преобразования получаются значения границ интервала \\((\\rho_{ij,\\min}, \\; \\rho_{ij,\\max})\\).",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "andan-corr.html#andan-corr-scales",
    "href": "andan-corr.html#andan-corr-scales",
    "title": "27  Корреляционный анализ",
    "section": "27.3 Коэффициенты корреляции для разных шкал",
    "text": "27.3 Коэффициенты корреляции для разных шкал\nДла разных шкал разработаны разные коэффициенты корреляции. Оценки коэффициентов будут рассчитываться по-разному, но логика тестирования статистических гипотез остаётся одинаковой.\n\n\n\n\n\n\n\n\nПеременная \\(X\\)\nПеременная \\(Y\\)\nМера связи\n\n\n\n\nИнтервальная или отношений\nИнтервальная или отношений\nКоэффициент Пирсона\n\n\nРанговая, интервальная или отношений\nРанговая, интервальная или отношений\nКоэффициент Спирмена\n\n\nРанговая\nРанговая\nКоэффициент Кенделла\n\n\n\nВ функциях cor() и cor.test() требуемый коэффициент задаётся черед аргумент method:",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "andan-corr.html#частный-и-множественный-коэффициент-корреляции",
    "href": "andan-corr.html#частный-и-множественный-коэффициент-корреляции",
    "title": "27  Корреляционный анализ",
    "section": "27.4 Частный и множественный коэффициент корреляции",
    "text": "27.4 Частный и множественный коэффициент корреляции\nЕсли у нас два признака, то с ними всё достаточно понятно. А если признаком много? Тогда у нас могут быть сложные взаимосвязи, и возможен такой случай, что некоторый признак оказывает связан как с одним, так и с другим из интересующих нас. Таким образом, мы можем наблюдать ложную корреляцию. Чтобы избавиться от влияния сторонних признаков, используюся частные коэффициенты корреляции.\nФункция cor() может возвращать не только оценку одного коэффициента корреляции, но и корреляционную матрицу, отобрадающую связи всех признаков со всеми. Например, продолжим работать со шкалой морального возмущения и изучим взаимосвязи внутри неё:\nВ корреляционной матрице на главной диагонали стоят единицы, отражающай связь переменной в самой собой — разумеется, она будет абсолютно линейная.\n\n\nА как посчитать ковариационную матрицу?\n\nВ общем виде корреляционная матрица имеет следующий вид:\n\\[\nR =\n\\begin{pmatrix}\n1 & r_{12} & \\dots & r_{1p} \\\\\nr_{12} & 1 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & 1\n\\end{pmatrix}\n\\]\nМатрица, как можно заметить, симметрична относительно главной диагонали, так как \\(r_{ij} = r_{ji}\\).\nЕё можно визуализироать, например, так:\nНо можно и усовершенствовать визуализацию, отобразив сами значения:\nНа основе этой матрицы мы можем протестировать статистическую значимость каждого из коэффициентов (не забыв про поправки на множественные сравнения!):\nЧтобы перенести их на график, нам нужно получить матрицу из p-значений:\nНу, у нас ничего не поменялось, так как коэффициенты все оказались значимы. Эх…\nНо вот для примера на одно из встроенных датасетов:\nИтак, возвращается к частному коэффициенту корреляции. Он определяется так:\n\\[\nr_{ij, J(i,j)} = - \\frac{A_{ij}}{\\sqrt{A_{ii} A_{jj}}},\n\\]\nгде \\(A\\) — алгебраическое дополнение.\nВ общем виде это осознать сложно, поэтому давайте на примере трёх признаков.\n\\[\nR =\n\\begin{pmatrix}\n1 & r_{12} & r_{13} \\\\\nr_{21} & 1 & r_{23} \\\\\nr_{31} & r_{32} & 1\n\\end{pmatrix}\n\\]\n\\[\nr_{12,3} = \\frac{r_{12} - r_{13} \\cdot r_{23}}{\\sqrt{(1 - r^2_{23})(1-r^2{13})}}\n\\]\n\\[\nH_0: \\rho_{12,3} = 0 \\\\\nH_1: \\rho_{12,3} \\neq 0 \\\\\nt = \\frac{r_{12,3} \\sqrt{n-3}}{\\sqrt{1 - r^2_{12,3}}} \\overset{H_0}{\\thicksim} t(\\nu = n-3)\n\\]\nНо слава богу, что в R это все делается в одну строку:\nХорошо, а если нас интересует связь одного признака с несколькими сразу? Тогда нам нужен множественный коэффициент корреляции. Он также вычисляется на основе корреляционной матрицы и определяется следующим образом. Пусть нас интересует связь первого признака со всеми остальными:\n\\[\nR_1 = \\sqrt{1 - \\frac{\\det R}{A_{11}}}\n\\]\nКвадрат множественонго коэффициента корреляции называется коэффициентом детерминации1. Он показывает, во-первых, степень тесноты связи данного признака со всеми остальными, но, кроме того, ещё и долю дисперсии данного признака, определяемую вариацией все остальных признаков, включенных в данную корреляционную модель.\nМы подробнее его изучим в следуюшей теме, а также увидим, где нам его найти, чтобы не считать руками.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "andan-corr.html#другие-корреляции",
    "href": "andan-corr.html#другие-корреляции",
    "title": "27  Корреляционный анализ",
    "section": "27.5 Другие корреляции",
    "text": "27.5 Другие корреляции\nМожно коррелировать не только количественные и ранговые шкалы между собой, но и качественные тоже:\n\n\n\nПеременная \\(X\\)\nПеременная \\(Y\\)\nМера связи\n\n\n\n\nДихотомическая\nДихотомическая\n\\(\\phi\\)-коэффициент\n\n\nДихотомическая\nРанговая\nРангово-бисериальный коэффициент\n\n\nДихотомическая\nИнтервальная или отношений\nБисериальный коэффициент\n\n\n\n\n27.5.1 \\(\\phi\\)-коэффициент\nЭтот коэффициент позволяет рассчитать корреляцию между двумы дихотомическими шкалами. Он основан на расчёте статистики \\(\\chi^2\\).\nПо двум дихотомическим переменным можно построить таблицу сопряженности. Разберемся на котиках и пёсиках:\nПо данной таблице можно рассчитать критерий согласия Пирсона (\\(\\chi^2\\)):\nСам хи-квадрат тестирует гипотезу о том, что между двумя категориальными переменными нет связи. Он это делает путём сравнения теоретической и эмпирической таблицы частот.\nЭмпирическую таблицу частот мы получаем по результатам наблюдений (то, что мы делаем с помощью функции table()):\n\n\n\n\n\\(X_1\\)\n\\(X_2\\)\n\n\n\n\n\\(Y_1\\)\n\\(p_{X_1,Y_1} = a\\)\n\\(p_{X_2,Y_1} = b\\)\n\n\n\\(Y_2\\)\n\\(p_{X_1,Y_2} = c\\)\n\\(p_{X_2,Y_2} = d\\)\n\n\n\nДалее вычисляются теоретические частоты:\n\n\n\n\n\n\n\n\n\n\\(X_1^*\\)\n\\(X_2^*\\)\n\n\n\n\n\\(Y_1^*\\)\n\\(\\frac{(a+b) \\times (a+c)}{N}\\)\n\\(\\frac{(b+a) \\times (b+d)}{N}\\)\n\n\n\\(Y_2^*\\)\n\\(\\frac{(c+d) \\times (a+c)}{N}\\)\n\\(\\frac{(d+c) \\times (b + d)}{N}\\)\n\n\n\nгде \\(N = a + b + c + d\\).\nЗатем считаются расхождения частот, которые суммируются и получается статистика \\(\\chi^2\\):\n\\[\n\\chi^2 = \\sum_{i,j} \\frac{p_{X_i,Y_j} - p_{X_i^*,Y_j^*}}{p_{X_i^*,Y_j^*}}\n\\]\nСтатистика подчиняется распределению \\(\\chi^2\\), и чем больше значение этой статистики, тем сильнее связаны признаки. В нашем случае мы получили значение 0, что говорит о абсолютном отсутствии связи между видом животного и его размером.\nНо по значению \\(\\chi^2\\) сложно что-то сказать о силе связи, поэтому его нормируют следующим образом, чтобы получить значения от 0 до 1, которые можно интерпретироват аналогично коэффициенту корреляции:\n\\[\n\\phi = \\sqrt{\\frac{\\chi^2}{N}}\n\\]\nТак как в нашем случае значение \\(\\chi^2\\) было 0, то и коэффициент \\(\\phi\\) мы получили 0.\n\n\n27.5.2 Бисериальный коэффициент корреляции\nЭтот коэффициент используется для вычисления корреляции между количественной (\\(y\\)) и категориальной (\\(x\\)) шкалой и рассчитывается следующим образом:\n\\[\nr = \\frac{\\bar x_1 - \\bar x_2}{s_y} \\sqrt{\\frac{n_1 n_2}{N(N-1)}},\n\\] где \\(\\bar x_1\\) — среднее по элементам переменной \\(y\\) из группы \\(x_1\\), \\(\\bar x_2\\) — среднее по элементам \\(y\\) из группы \\(x_2\\), \\(s_y\\) — стандартное отклонение по переменной \\(y\\), \\(n_1\\) — число элементов в группе \\(x_1\\), \\(n_2\\) — число элементов в группе \\(x_2\\), \\(N\\) — общее число элементов.\nВажно отметить, что несмотря на то, что значение коэффициента может быть как положительным, так и отрицательным, это не влияет на интерпретацию. Это одно из исключений из общего правила.\nВ R его можно вычислить так:\n\n\n27.5.3 Рангово-бисериальный коэффициент корреляции\nЕсли у нас не количественная, а ранговая шкала, то применяется рангово-бисериальный коэффициент:\n\\[\nr = \\frac{2(\\bar x_1 - \\bar x_2)}{N},\n\\] где \\(\\bar x_1\\) — средний ранг в группе \\(x_1\\), \\(\\bar x_2\\) — средний ранг в группе \\(x_2\\), \\(N\\) — общее количество наблюдений.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "andan-corr.html#andan-cor-fisher-transform",
    "href": "andan-corr.html#andan-cor-fisher-transform",
    "title": "27  Корреляционный анализ",
    "section": "27.6 Преобразование Фишера",
    "text": "27.6 Преобразование Фишера\n\\[\nz_i = \\frac{1}{2} \\ln \\frac{1 + r_i}{1 - r_i} = \\mathop{\\mathrm{artanh}}(r_i)\n\\]\n\\[\nr_P = \\dfrac{e^{2z_P} - 1}{e^{2z_P} + 1} = \\tanh(z_P)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Корреляционный анализ</span>"
    ]
  },
  {
    "objectID": "andan-simplelinear.html#вычисление-коэффициентов-линейной-регрессии",
    "href": "andan-simplelinear.html#вычисление-коэффициентов-линейной-регрессии",
    "title": "29  Общие линейные модели. Простая линейная регрессия",
    "section": "29.1 Вычисление коэффициентов линейной регрессии",
    "text": "29.1 Вычисление коэффициентов линейной регрессии\n\n\n\n\n29.1.1 Матричное вычисление коэффициентов\n\n\n29.1.2 Аналитическое вычисление коэффициентов\n\\[\nf(b_0, b_1) = \\sum (y_i - \\hat y_i)^2 = \\sum (y_i - b_0 - b_1x_i)^2 \\rightarrow \\min_{b_0, b_1}\n\\]\n\\[\nf(b_0, b_1) = \\sum (y_i - b_0 - b_1x_i) (y_i - b_0 - b_1x_i)\n\\]\n\\[\nf(b_0, b_1) =\n\\sum (y_i^2 - b_0 y_i - b_1 x_i y_i - b_0 y_i - b_1 x_i y_i + b_0 b_1 x_i + b_1^2 x_i^2 + b_0^2 + b_0 b_1 x_i)\n\\]\n\\[\nf(b_0, b_1) =\n\\sum(y_i^2 - 2 b_1 x_i y_i - 2 y_i b_0 + x_i^2 b_1^2 + b_0^2 + 2 x_i b_1 b_0)\n\\]\n\\[\n\\frac{f(b_0, b_1)}{\\partial b_0} = \\sum (-2y_i + 2b_0 + 2x_ib_1) =\n-2 \\sum \\big( y_i - (b_0 + b_1 x_i) \\big)\n\\]\n\\[\n\\frac{f(b_0, b_1)}{\\partial b_1} = \\sum (-2 x_i y_i + 2 x_i^2 b_1 + 2 x_i b_0) = -2 \\sum \\big( y_i - (b_0 + b_1 x_i) \\big) x_i\n\\]\n\\[\n\\cases {\n-2 \\sum \\big( y_i - (b_0 + b_1 x_i) \\big) = 0 \\\\\n-2 \\sum \\big( y_i - (b_0 + b_1 x_i) \\big) x_i = 0\n}\n\\]\n\\[\n\\cases{\n\\sum \\big( y_i - (b_0 + b_1 x_i) \\big) = 0 \\\\\n\\sum \\big( y_i - (b_0 + b_1 x_i) \\big) x_i = 0\n}\n\\]\n\\[\n\\cases{\n\\sum y_i - \\sum b_0 + \\sum b_1 x_i = 0 \\\\\n\\sum y_i x_i - \\sum b_0 x_i + \\sum b_1 x^2_i = 0\n}\n\\]\n\\[\n\\cases{\n\\sum b_0 + \\sum b_1 x_i = \\sum y_i \\\\\n\\sum b_0 x_i + \\sum b_1 x_i^2 = \\sum y_i x_i\n}\n\\]\n\\[\n\\cases{\nb1 \\sum x_i + n b_0 = \\sum y_i \\\\\nb1 \\sum x^2_i + b_0 \\sum x_i = \\sum y_i x_i\n}\n\\]\n\\[\nb_0 = \\frac{\\sum y_i}{n} - b_1 \\frac{\\sum x_i}{n} = \\bar y - b_1 \\bar x\n\\]\n\\[\nb1 \\sum x_i^2 + (\\bar y - b_1 \\bar x) \\sum x_i = \\sum x_i y_i\n\\]\n\\[\n\\underline{b_1 \\sum x_i^2} + \\bar y \\sum x_i - \\underline{b_1 \\bar x \\sum x_i} = \\sum x_i y_i\n\\]\n\\[\nb_1 \\Big( \\sum x_i^2 - \\bar x \\sum x_i \\Big) =\n\\sum x_i y_i - \\bar y \\sum x_i\n\\]\n\\[\nb_1 = \\frac{\\sum x_i y_i - \\bar y \\sum x_i}{\\sum x_i^2 - \\bar x \\sum x_i} =\n\\frac{(\\sum x_i y_i - \\bar y \\sum x_i) \\times n}{(\\sum x_i^2 - \\bar x \\sum x_i) \\times n}\n\\]\n\\[\nb_1 = \\frac{\\overline{xy} - \\bar x \\bar y}{\\overline{x^2} - \\bar x^2} =\n\\frac{\\overline{xy} - \\bar x \\bar y}{\\sigma_X^2}\n\\]\n\n\n29.1.3 Угловой коэффициент и коэффициент корреляции\n\n\n\n\\[\nb_1 = r_{X,Y} \\times \\frac{s_Y}{s_X}\n\\]\n\\[\nr_{X,Y} = \\frac{\\sum (x_i - \\bar x)(y_i - \\bar y)}{\\sqrt{\\sum(x_i - \\bar x)^2 \\sum(y_i - \\bar y)^2}}\n\\]\n\\[\nb_1 = \\frac{\\sum (x_i - \\bar x)(y_i - \\bar y)}{\\sqrt{\\sum(x_i - \\bar x)^2 \\sum(y_i - \\bar y)^2}}\n\\times\n\\frac{s_Y}{s_X}\n\\]\n\\[\nb_1 = \\frac{\\sum (x_i - \\bar x)(y_i - \\bar y)}{\\sqrt{\\sum(x_i - \\bar x)^2 \\sum(y_i - \\bar y)^2}}\n\\times\n\\frac{\\sqrt{\\sum (y_i - \\bar y)^2 / (n-1)}}{\\sqrt{\\sum (x_i - \\bar x)^2 / (n-1)}}\n\\]\nСократим дробь:\n\\[\n\\frac{\\sqrt{(y_i - \\bar y)^2 / (n-1)}}{\\sqrt{(x_i - \\bar x)^2 / (n-1)}} =\n\\frac{\\sqrt{(y_i - \\bar y)^2} \\times \\sqrt{(n-1)}}{\\sqrt{(x_i - \\bar x)^2} \\times \\sqrt{(n-1)}} = \\frac{\\sqrt{(y_i - \\bar y)^2}}{\\sqrt{(x_i - \\bar x)^2}}\n\\]\n\\[\nb_1 = \\frac{\\sum (x_i - \\bar x)(y_i - \\bar y)}{\\sqrt{\\sum(x_i - \\bar x)^2} \\times \\color{green}{\\sqrt{\\sum(y_i - \\bar y)^2}}}\n\\times\n\\frac{\\color{green}{\\sqrt{\\sum(y_i - \\bar y)^2}}}{\\sqrt{\\sum(x_i - \\bar x)^2}}\n\\]\n\\[\nb_1 = \\frac{\\sum (x_i - \\bar x)(y_i - \\bar y)}{\\sum(x_i - \\bar x)^2}\n\\]\nСумма отклонений от среднего равна нулю:\n\\[\n\\sum(x_i - \\bar x) = \\sum (y_i - \\bar y) = 0\n\\]\nРазложим числитель:\n\\[\n\\sum (x_i - \\bar x) (y_i - \\bar y) =\n\\sum x_i (y_i - \\bar y) - \\sum \\bar x (y_i - \\bar y) =\n\\sum x_i (y_i - \\bar y)\n\\]\n\\[\nb_1 = \\frac{\\sum x_i (y_i - \\bar y)}{\\sum (x_i - \\bar x)^2} =\n\\frac{\\Big( \\sum x_i (y_i - \\bar y) \\Big) \\times n}{\\Big( \\sum (x_i - \\bar x)^2 \\Big) \\times n} = \\frac{\\overline{xy} - \\bar x \\bar y}{\\sigma_X^2}\n\\]\n\n\n29.1.4 TSS = ESS + RSS?\n\n\n\n\\[\n\\mathrm{TSS} =\n\\sum (y_i - \\bar y)^2 =\n\\sum (y_i - \\hat y + \\hat y - \\bar y)^2 =\n\\sum \\big( (y_i - \\hat y_i) + (\\hat y_i - \\bar y) \\big)^2 =\n\\]\n\\[\n= \\sum (y_i - \\hat y_i) = \\sum (\\hat y_i - \\bar y) + 2 \\sum (y_i - \\hat y_i)(\\hat y_i - \\bar y) =\n\\]\n\\[\n= \\mathrm{RSS} + \\mathrm{ESS} + 2 \\sum (y_i - \\hat y_i)(\\hat y_i - \\bar y)\n\\]\nОкей, осталось доказать, что \\(2 \\sum (y_i - \\hat y_i)(\\hat y_i - \\bar y) = 0\\), и все будет найс.\nТак как \\(b_0 = \\bar y - b_1 x\\).\n\\[\n\\sum (y_i - \\hat y_i)(\\hat y_i - \\bar y) = \\sum (y_i - b_0 - b_1 x_i) (b_0 + b_1 x_i - \\bar y) =\n\\]\n\\[\n= \\sum (y_i - \\bar y + b_1 \\bar x - b_1x_i) (\\bar y - b_1 \\bar x + b_1 x_i - \\bar y) =\n\\]\n\\[\n= \\sum \\big( (y_i - \\bar y) - b_1(x_i - \\bar x) \\big) \\times b_1 (x_i - \\bar x) =\n\\sum \\big( b_1 (x_i - \\bar x) (y_i - \\bar y) - b_1^2 (x_i - \\bar x)^2 \\big) =\n\\]\n\\[\n= b_1 \\sum (x_i - \\bar x) (y_i - \\bar y) - b_1^2 \\sum (x_i - \\bar x)\n\\]\nТак как \\(b_1 = \\frac{\\sum(x_i - \\bar x)(y_i - \\bar y)}{\\sum (x_i - \\bar x)^2}\\), получается, что:\n\\[\n\\frac{\\Big( \\sum (x_i - \\bar x) (y_i - \\bar y) \\Big)^2}{\\sum (x_i - \\bar x)^2} - \\frac{\\Big( \\sum (x_i - \\bar x) (y_i - \\bar y) \\Big)^2 \\times \\sum (x_i - \\bar x)^2}{\\Big( \\sum (x_i - \\bar x)^2\\Big)^2} =\n\\]\n\\[\n= \\frac{\\Big( \\sum (x_i - \\bar x) (y_i - \\bar y) \\Big)^2}{\\sum (x_i - \\bar x)^2} - \\frac{\\Big( \\sum (x_i - \\bar x) (y_i - \\bar y) \\Big)^2}{\\sum (x_i - \\bar x)^2} = 0\n\\]",
    "crumbs": [
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Общие линейные модели. Простая линейная регрессия</span>"
    ]
  },
  {
    "objectID": "andan-logreg.html#линеаризация-логистической-кривой-посредством-logit-преобразования",
    "href": "andan-logreg.html#линеаризация-логистической-кривой-посредством-logit-преобразования",
    "title": "40  Обобщенные линейные модели. Логистическая регрессия",
    "section": "40.1 Линеаризация логистической кривой посредством logit-преобразования",
    "text": "40.1 Линеаризация логистической кривой посредством logit-преобразования\n\\[\np = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\n\\]\nПроизведем замену:\n\\[\n\\beta_0 + \\beta_1 x = t\n\\]\n\\[\n\\mathrm{logit}(p) = \\ln \\Big( \\frac{p}{1-p} \\Big)\n\\]\nЕсли logit-преобразование действительно линеризует логистическую кривую, что должно выполняться равенство:\n\\[\n\\mathrm{logit}(p) = t \\Rightarrow \\ln \\Big( \\frac{p}{1-p} \\Big) = t\n\\]\nДокажем это.\n\\[\n\\ln \\Big( \\frac{p}{1-p} \\Big) =\n\\ln \\Bigg( \\frac{\\frac{e^t}{1 + e^t}}{1 - \\frac{e^t}{1 + e^t}} \\Bigg) =\n\\ln \\Big( \\frac{e^t}{1 + e^t} \\Big) - \\ln \\Big(1 - \\frac{e^t}{1 + e^t} \\Big) =\n\\]\n\\[\n= \\ln \\Big( \\frac{e^t}{1 + e^t} \\Big) - \\ln \\Big(\\frac{1 + e^t - e^t}{1 + e^t} \\Big) =\n\\ln \\Big( \\frac{e^t}{1 + e^t} \\Big) - \\ln \\Big( \\frac{1}{1 + e^t} \\Big) =\n\\]\n\\[\n= \\ln (e^t) - \\ln (1 + e^t) - \\big(\\ln (1) - \\ln (1+e^t)\\big) =\n\\ln (e^t) - \\ln (1) =\n\\ln (e^t) = t\n\\]",
    "crumbs": [
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Обобщенные линейные модели. Логистическая регрессия</span>"
    ]
  },
  {
    "objectID": "andan-cluster.html",
    "href": "andan-cluster.html",
    "title": "48  Кластерный анализ",
    "section": "",
    "text": "Такое расстояние называют мантэттеновским, потому что улицы Манхэттена устроены очень похоже:\n\n\n\n\n\n\nИли майкопским — тут вообще рай обожателя геометрии:",
    "crumbs": [
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Кластерный анализ</span>"
    ]
  },
  {
    "objectID": "appendix-numbers.html#integer_iron",
    "href": "appendix-numbers.html#integer_iron",
    "title": "92  Числа и железо",
    "section": "92.1 Целые числа",
    "text": "92.1 Целые числа\n\n92.1.1 Системы счисления\n\n\n92.1.2 Перевод числа из одной системы счисления в другую\n\n\n92.1.3 Целочисленные типы",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Числа и железо</span>"
    ]
  },
  {
    "objectID": "appendix-numbers.html#negative_iron",
    "href": "appendix-numbers.html#negative_iron",
    "title": "92  Числа и железо",
    "section": "92.2 Отрицательные числа",
    "text": "92.2 Отрицательные числа\n\n92.2.1 Знаковые и беззнаковые типы",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Числа и железо</span>"
    ]
  },
  {
    "objectID": "appendix-numbers.html#float_iron",
    "href": "appendix-numbers.html#float_iron",
    "title": "92  Числа и железо",
    "section": "92.3 Числа с плавающей точкой",
    "text": "92.3 Числа с плавающей точкой\n\n92.3.1 Экспоненциальная запись числа\n\n\n92.3.2 Типы чисел c плавающей точкой",
    "crumbs": [
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>Числа и железо</span>"
    ]
  },
  {
    "objectID": "appendix-git.html#концепция-git",
    "href": "appendix-git.html#концепция-git",
    "title": "97  Git",
    "section": "97.1 Концепция Git",
    "text": "97.1 Концепция Git",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "appendix-git.html#source-tree",
    "href": "appendix-git.html#source-tree",
    "title": "97  Git",
    "section": "97.2 Source Tree",
    "text": "97.2 Source Tree",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "appendix-git.html#cli",
    "href": "appendix-git.html#cli",
    "title": "97  Git",
    "section": "97.3 CLI",
    "text": "97.3 CLI\n\n97.3.1 git clone\n\n\n97.3.2 git status\n\n\n97.3.3 git diff\n\n\n97.3.4 git log\n\n\n97.3.5 git add\n\n\n97.3.6 git commit\n\n\n97.3.7 git push\n\n\n97.3.8 git pull\n\n\n97.3.9 git branch\n\n\n97.3.10 git checkout\n\n\n97.3.11 git config",
    "crumbs": [
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>Git</span>"
    ]
  },
  {
    "objectID": "appendix-formulas.html#formulas_power",
    "href": "appendix-formulas.html#formulas_power",
    "title": "98  Формулы",
    "section": "98.1 Степени и корни",
    "text": "98.1 Степени и корни\n\n98.1.1 Определения\n\\[\na^b \\overset{\\text{def}}{=}\\prod_{i=1}^b a_i\n\\]\n\\[\n\\sqrt[n]a \\overset{\\text{def}}{=}b \\Leftrightarrow b^n = a\n\\]\n\n\n98.1.2 Свойства\n\n\n\n\\[a^n \\cdot a^m = a^{n+m}\\] \\[\\frac{a^n}{a^m} = a^{n-m}\\] \\[(a^n)^m = a^{nm}\\] \\[a^0 = 1\\] \\[a^{-n}=\\frac{1}{a^n}\\] \\[(a \\cdot b)^n = a^n \\cdot b^n\\] \\[\\Big(\\frac{a}{b}\\Big)^n = \\frac{a^n}{b^n}\\]\n\n\n\\[a^{\\frac{1}{n}}=\\sqrt[n]{a}\\] \\[a^{\\frac{m}{n}}=\\sqrt[n]{a^m}\\] \\[\\sqrt[n]{a \\cdot b} = \\sqrt[n]{a} \\cdot \\sqrt[n]{b}\\] \\[(a \\cdot b)^{\\frac{1}{n}} = a^{\\frac{1}{n}} \\cdot b^{\\frac{1}{n}}\\] \\[\\sqrt[n]{\\frac{a}{b}} = \\frac{\\sqrt[n]{a}}{\\sqrt[n]{b}}\\] \\[\\Big(\\frac{a}{b}\\Big)^{\\frac{1}{n}} = \\frac{a^{\\frac{1}{n}}}{b^{\\frac{1}{n}}}\\]\n\n\n\n\\[\nx^n = a \\Rightarrow x =\n\\begin{cases}\n\\pm \\sqrt[n]{a}, &\\quad x \\mod 2 = 0 \\\\\n\\sqrt[n]{a}, &\\quad x \\mod 2 = 1\n\\end{cases}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Формулы</span>"
    ]
  },
  {
    "objectID": "appendix-formulas.html#formulas_log",
    "href": "appendix-formulas.html#formulas_log",
    "title": "98  Формулы",
    "section": "98.2 Логарифмы",
    "text": "98.2 Логарифмы\n\n98.2.1 Определение\n\\[\\log_b a \\overset{\\text{def}}{=}c \\Leftrightarrow a^c = b, a &gt; 0, b &gt; 0, b \\neq 1\\]\n\n\n98.2.2 Свойства\n\n\n\n\\[\\log_a a = 1\\] \\[\\log_c(ab) = \\log_c a + \\log_c b\\] \\[\\log_c\\Big(\\frac{a}{b}\\Big) = \\log_c a - \\log_c b\\]\n\n\n\\[\\log_c 1 = 0\\] \\[\\log_c a^b = b \\log_c a\\] \\[\\log_{c^b} a = \\frac{1}{b} \\log_c a\\]",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Формулы</span>"
    ]
  },
  {
    "objectID": "appendix-formulas.html#formulas_abs",
    "href": "appendix-formulas.html#formulas_abs",
    "title": "98  Формулы",
    "section": "98.3 Модуль",
    "text": "98.3 Модуль\n\\[\n|a| =\n\\begin{cases}\na, &a \\geq 0 \\\\\n-a, &a &lt; 0\n\\end{cases}\n\\]\n\\[|x| \\leq a \\Rightarrow -a \\leq x \\leq a \\Leftrightarrow x \\in [-a, a]\\]\n\\[|x| \\geq a \\Rightarrow x \\leq -a \\wedge x \\geq a \\Leftrightarrow x \\in (-\\infty, -a] \\cup [a, +\\infty)\\]",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Формулы</span>"
    ]
  },
  {
    "objectID": "appendix-formulas.html#formulas_trig",
    "href": "appendix-formulas.html#formulas_trig",
    "title": "98  Формулы",
    "section": "98.4 Тригонометрия",
    "text": "98.4 Тригонометрия\n\n\n\n\n\n\n\n\n\n\n98.4.1 Производные тригонометрические функции\n\\[\n\\sec \\alpha = \\frac{1}{\\cos \\alpha} \\qquad\n\\csc \\alpha = \\frac{1}{\\sin \\alpha} \\qquad\n\\tan \\alpha = \\frac{\\sin \\alpha}{\\cos \\alpha} \\qquad\n\\cot \\alpha = \\frac{\\cos \\alpha}{\\sin \\alpha}\n\\]\n\n\n98.4.2 Основное тригонометрическое тождество\n\\[\n\\sin^2 \\alpha + \\cos^2 \\alpha = 1\n\\]\n\\[\n\\sin \\alpha = \\pm \\sqrt{1 - \\cos^2 \\alpha} \\qquad \\cos \\alpha = \\pm \\sqrt{1 - \\sin^2 \\alpha}\n\\]\n\\[\n1 + \\cot^2 \\alpha = \\csc^2 \\alpha \\qquad\n\\tan^2 \\alpha + 1 = \\sec ^2 \\alpha\n\\]\n\\[\n\\sec^2 \\alpha + \\csc^2 \\alpha = \\sec^2 \\alpha \\cdot \\csc^2 \\alpha\n\\]\n\n\n98.4.3 Отражения\n\n\n\n\n\n\n\n\n\n\n98.4.3.1 Относительно \\(\\varphi = 0\\)\nВыражает свойство чётности функции1.\n\n\n\n\\[\\sin (-\\alpha) = -\\sin \\alpha\\] \\[\\tan (-\\alpha) = -\\tan \\alpha\\] \\[\\sec (-\\alpha) = \\sec \\alpha\\]\n\n\n\\[\\cos (-\\alpha) = \\cos \\alpha\\] \\[\\cot (-\\alpha) = -\\cot \\alpha\\] \\[\\csc (-\\alpha) = -\\csc \\alpha\\]\n\n\n\n\n\n98.4.3.2 Относительно \\(\\varphi = \\frac{\\pi}{4}\\)\n\n\n\n\\[\\sin (\\frac{\\pi}{2}-\\alpha) = \\cos \\alpha\\] \\[\\tan \\Big(\\frac{\\pi}{2}-\\alpha\\Big) = \\cot \\alpha\\] \\[\\sec \\Big(\\frac{\\pi}{2}-\\alpha\\Big) = \\csc \\alpha\\]\n\n\n\\[\\cos \\Big(\\frac{\\pi}{2}-\\alpha\\Big) = \\sin \\alpha\\] \\[\\cot \\Big(\\frac{\\pi}{2}-\\alpha\\Big) = \\tan \\alpha\\] \\[\\csc \\Big(\\frac{\\pi}{2}-\\alpha\\Big) = \\sec \\alpha\\]\n\n\n\n\n\n98.4.3.3 Относительно \\(\\varphi = \\frac{\\pi}{2}\\)\n\n\n\n\\[\\sin (\\pi-\\alpha) = \\sin \\alpha\\] \\[\\tan (\\pi-\\alpha) = -\\tan \\alpha\\] \\[\\sec (\\pi-\\alpha) = -\\sec \\alpha\\]\n\n\n\\[\\cos (\\pi-\\alpha) = -\\cos \\alpha\\] \\[\\cot (\\pi-\\alpha) = -\\cot \\alpha\\] \\[\\csc (\\pi-\\alpha) = \\csc \\alpha\\]\n\n\n\n\n\n98.4.3.4 Относительно \\(\\varphi = \\frac{3\\pi}{4}\\)\n\n\n\n\\[\\sin \\Big(\\frac{3\\pi}{2}-\\alpha\\Big) = -\\cos \\alpha\\] \\[\\tan \\Big(\\frac{3\\pi}{2}-\\alpha\\Big) = \\cot \\alpha\\] \\[\\sec \\Big(\\frac{3\\pi}{2}-\\alpha\\Big) = -\\csc \\alpha\\]\n\n\n\\[\\cos \\Big(\\frac{3\\pi}{2}-\\alpha\\Big) = -\\sin \\alpha\\] \\[\\cot \\Big(\\frac{3\\pi}{2}-\\alpha\\Big) = \\tan \\alpha\\] \\[\\csc \\Big(\\frac{3\\pi}{2}-\\alpha\\Big) = -\\sec \\alpha\\]\n\n\n\n\n\n98.4.3.5 Относительно \\(\\varphi = \\pi\\)\n\n\n\n\\[\\sin (2\\pi - \\alpha) = -\\sin \\alpha = \\sin (-\\alpha)\\] \\[\\tan (2\\pi - \\alpha) = -\\tan \\alpha = \\tan (-\\alpha)\\] \\[\\sec (2\\pi - \\alpha) = \\sec \\alpha = \\sec (-\\alpha)\\]\n\n\n\\[\\cos (2\\pi - \\alpha) = \\cos \\alpha = \\cos (-\\alpha)\\] \\[\\cot (2\\pi - \\alpha) = -\\cot \\alpha = \\cot (-\\alpha)\\] \\[\\csc (2\\pi - \\alpha) = -\\csc \\alpha = \\csc (-\\alpha)\\]\n\n\n\n\n\n\n98.4.4 Сдвиг\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nТак как все тригонометрические функции периодические, результат сдвига функции определяется её периодом. Для функций \\(\\sin, \\cos, \\sec\\) и \\(\\csc\\) период равен \\(2\\pi\\). Для \\(\\tan\\) и \\(\\cot\\) он составляет \\(\\pi\\).\n\n98.4.4.1 На четверть периода\n\n\n\n\\[\\sin \\Big(\\alpha \\pm \\frac{\\pi}{2}\\Big) = \\pm\\cos \\alpha\\] \\[\\tan \\Big(\\alpha \\pm \\frac{\\pi}{4}\\Big) = \\frac{\\tan \\alpha \\pm 1}{1 \\mp \\tan \\alpha}\\] \\[\\sec \\Big(\\alpha \\pm \\frac{\\pi}{2}\\Big) = \\mp \\csc \\alpha\\]\n\n\n\\[\\cos \\Big(\\alpha \\pm \\frac{\\pi}{2}\\Big) = \\mp \\sin \\alpha\\] \\[\\cot \\Big(\\alpha \\pm \\frac{\\pi}{4}\\Big) = \\frac{\\cot \\alpha \\mp 1}{1 \\pm \\cot \\alpha}\\] \\[\\csc \\Big(\\alpha \\pm \\frac{\\pi}{2}\\Big) = \\pm \\sec \\alpha\\]\n\n\n\n\n\n98.4.4.2 На половину периода\n\n\n\n\\[\\sin (\\alpha + \\pi) = -\\sin \\alpha\\] \\[\\tan \\Big(\\alpha + \\frac{\\pi}{2}\\Big) = -\\cot \\alpha\\] \\[\\sec (\\alpha + \\pi) = -\\sec \\alpha\\]\n\n\n\\[\\cos (\\alpha + \\pi) = -\\cos \\alpha\\] \\[\\cot \\Big(\\alpha + \\frac{\\pi}{2}\\Big) = -\\tan \\alpha\\] \\[\\csc (\\alpha + \\pi) = -\\csc \\alpha\\]\n\n\n\n\n\n98.4.4.3 На полный период\n\n\n\n\\[\\sin (\\alpha + 2\\pi) = \\sin \\alpha\\] \\[\\tan (\\alpha + \\pi) = \\tan \\alpha\\] \\[\\sec (\\alpha + 2\\pi) = \\sec \\alpha\\]\n\n\n\\[\\cos (\\alpha + 2\\pi) = \\cos \\alpha\\] \\[\\cot (\\alpha + \\pi) = \\cot \\alpha\\] \\[\\csc (\\alpha + 2\\pi) = \\csc \\alpha\\]\n\n\n\n\n\n\n98.4.5 Соотношение знаков\n\\[\n\\text{sgn}\\sin \\alpha = \\text{sgn}\\csc \\alpha \\qquad \\text{sgn}\\cos \\alpha = \\text{sgn}\\sec \\alpha\n\\] \\[\n\\text{sgn}\\tan \\alpha = \\text{sgn}\\cot \\alpha\n\\]\n\n\n98.4.6 Функции суммы с разности аргументов\n\\[\n\\sin (\\alpha \\pm \\beta) = \\sin \\alpha \\cos \\beta \\pm \\cos \\alpha \\sin \\beta\n\\]\n\\[\n\\cos (\\alpha \\pm \\beta) = \\cos \\alpha \\cos \\beta \\mp \\sin \\alpha \\sin \\beta\n\\]\n\\[\n\\tan (\\alpha \\pm \\beta) = \\frac{\\tan \\alpha \\pm \\tan \\beta}{1 \\mp \\tan \\alpha \\tan \\beta}\n\\]\n\\[\n\\cot (\\alpha \\pm \\beta) = \\frac{\\cot \\alpha \\cot \\beta \\mp 1}{\\cot \\beta \\pm \\cot \\alpha}\n\\] \\[\n\\sec (\\alpha \\pm \\beta) = \\frac{\\sec \\alpha \\sec \\beta \\csc \\alpha \\csc \\beta}{\\csc \\alpha \\csc \\beta \\mp \\sec \\alpha \\sec \\beta}\n\\]\n\\[\n\\csc (\\alpha \\pm \\beta) = \\frac{\\sec \\alpha \\sec \\beta \\csc \\alpha \\csc \\beta}{\\sec \\alpha \\csc \\beta \\pm \\sec \\alpha \\sec \\beta}\n\\]\n\n\n98.4.7 Формулы двойного аргумента\n\\[\n\\sin 2\\alpha = 2\\sin \\alpha \\cos \\alpha = (\\sin \\alpha + \\cos \\alpha)^2 -1 = \\frac{2\\tan \\alpha}{1 + \\tan^2 \\alpha}\n\\]\n\\[\n\\cos 2\\alpha = \\cos^2 \\alpha - \\sin^2 \\alpha = 2\\cos^2 \\alpha - 1 = 1 - 2\\sin^2 \\alpha = \\frac{1 - \\tan^2 \\alpha}{1 + \\tan^2 \\alpha}\n\\]\n\\[\n\\tan 2\\alpha = \\frac{2\\tan \\alpha}{1 - \\tan^2 \\alpha}\n\\]\n\\[\n\\cot 2\\alpha = \\frac{\\cot^2 \\alpha - 1}{2 \\cot \\alpha} = \\frac{1 - \\tan^2 \\alpha}{2 \\tan \\alpha}\n\\]\n\\[\n\\sec 2\\alpha = \\frac{\\sec^2 \\alpha}{2 - \\sec^2 \\alpha} = \\frac{1 + \\tan^2 \\alpha}{1 - \\tan^2 \\alpha}\n\\]\n\\[\n\\csc 2\\alpha = \\frac{\\sec \\alpha \\csc \\alpha}{2} = \\frac{1 + \\tan^2 \\alpha}{2 \\tan \\alpha}\n\\]\n\n\n98.4.8 Формулы тройного аргумента\n\\[\n\\sin 3\\alpha = 3\\sin \\alpha - 4 \\sin^3 \\alpha = 4\\sin \\alpha \\sin \\Big( \\frac{\\pi}{3} - \\alpha\\Big) \\sin \\Big( \\frac{\\pi}{3} + \\alpha \\Big)\n\\]\n\\[\n\\cos 3\\alpha = 4\\cos^3 \\alpha - 3 \\cos \\alpha = 4\\cos \\alpha \\cos \\Big(\\frac{\\pi}{3} - \\alpha\\Big) \\cos \\Big(\\frac{\\pi}{3} + \\alpha\\Big)\n\\]\n\\[\n\\tan 3\\alpha = \\frac{3\\tan \\alpha - \\tan^3 \\alpha}{1 - 3\\tan^2 \\alpha} = \\tan \\alpha \\tan \\Big( \\frac{\\pi}{3} - \\alpha \\Big) \\tan \\Big(\\frac{\\pi}{3} + \\alpha \\Big)\n\\]\n\\[\n\\cot 3\\alpha = \\frac{3 \\cot \\alpha - \\cot^3 \\alpha}{1 - 3\\cot^2 \\alpha}\n\\]\n\\[\n\\sec 3\\alpha = \\frac{\\sec^3 \\alpha}{4 - 3\\sec^2 \\alpha}\n\\]\n\\[\n\\csc 3\\alpha = \\frac{\\csc^3 \\alpha}{3\\csc^2 \\alpha -4}\n\\]\n\n\n98.4.9 Формулы половинного аргумента\n\\[\n\\sin \\frac{\\alpha}{2} = \\text{sgn}\\Big(\\sin \\frac{\\alpha}{2} \\Big) \\sqrt{\\frac{1 - \\cos \\alpha}{2}}\n\\]\n\\[\n\\cos \\frac{\\alpha}{2} = \\text{sgn}\\Big( \\cos \\frac{\\alpha}{2} \\Big) \\sqrt{\\frac{1 + \\cos \\alpha}{2}}\n\\]\n\\[\n\\tan \\frac{\\alpha}{2} = \\frac{1 - \\cos\\alpha}{\\sin \\alpha} = \\frac{\\sin \\alpha}{1 + \\cos \\alpha} = \\csc \\alpha - \\cot \\alpha = \\frac{\\tan \\alpha}{1 + \\sec \\alpha} = \\text{sgn}(\\sin \\alpha) \\sqrt{\\frac{1 - \\cos \\alpha}{1 + \\cos \\alpha}}\n\\]\n\\[\n\\cot \\frac{\\alpha}{2} = \\frac{1 + \\cos \\alpha}{\\sin \\alpha} = \\frac{\\sin \\alpha}{1 - \\cos \\alpha} = \\csc \\alpha + \\cot \\alpha = \\text{sgn}(\\sin \\alpha) \\sqrt{\\frac{1 + \\cos \\alpha}{1 - \\cos{\\alpha}}}\n\\]\n\\[\n\\sec \\frac{\\alpha}{2} = \\text{sgn}\\Big( \\cos \\frac{\\alpha}{2} \\Big) \\sqrt{\\frac{2}{1 + \\cos \\alpha}}\n\\]\n\\[\n\\csc \\frac{\\alpha}{2} = \\text{sgn}\\Big( \\sin \\frac{\\alpha}{2}  \\Big) \\sqrt{\\frac{2}{1 - \\cos \\alpha}}\n\\]\n\n\n98.4.10 Формулы понижения степени\n\n\n\n\\[\n\\sin^2 \\alpha = \\frac{1 - \\cos 2\\alpha}{2}\n\\]\n\n\n\\[\n\\cos^2 \\alpha = \\frac{1 + \\cos 2\\alpha}{2}\n\\]\n\n\n\n\n\\[\n\\tan^2 \\alpha = \\frac{1 - \\cos 2\\alpha}{1 + \\cos 2\\alpha}\n\\]\n\n\n\\[\n\\cot^2 \\alpha = \\frac{1 + \\cos 2\\alpha}{1 - \\cos 2\\alpha}\n\\]\n\n\n\n\n\\[\n\\sec^2 \\alpha = \\frac{2}{1 + \\cos 2\\alpha}\n\\]\n\n\n\\[\n\\csc^2 \\alpha = \\frac{2}{1 - \\cos 2\\alpha}\n\\]\n\n\n\n\n\n\n98.4.11 Преобразование произведения в сумму\n\\[\n\\cos \\alpha \\cos \\beta = \\frac{\\cos (\\alpha - \\beta) + \\cos (\\alpha + \\beta)}{2}\n\\]\n\\[\n\\sin \\alpha \\sin \\beta = \\frac{\\cos (\\alpha - \\beta) - \\cos (\\alpha + \\beta)}{2}\n\\]\n\\[\n\\sin \\alpha \\cos \\beta = \\frac{\\sin (\\alpha + \\beta) + \\sin (\\alpha - \\beta)}{2}\n\\]\n\\[\n\\cos \\alpha \\sin \\beta = \\frac{\\sin (\\alpha + \\beta) - \\sin (\\alpha - \\beta)}{2}\n\\]\n\\[\n\\tan \\alpha \\tan \\beta = \\frac{\\cos (\\alpha - \\beta) - \\cos (\\alpha + \\beta)}{\\cos (\\alpha - \\beta) + \\cos (\\alpha + \\beta)}\n\\]\n\\[\n\\tan \\alpha \\cot \\beta = \\frac{\\sin (\\alpha + \\beta) + \\sin (\\alpha - \\beta)}{\\sin (\\alpha + \\beta) - \\sin (\\alpha - \\beta)}\n\\]\n\n\n98.4.12 Преобразование суммы в произведение\n\\[\n\\sin \\alpha \\pm \\sin \\beta = 2 \\sin \\Big(\\frac{\\alpha \\pm \\beta}{2}\\Big) \\cos \\Big(\\frac{\\alpha \\mp \\beta}{2}\\Big)\n\\]\n\\[\n\\cos \\alpha + \\cos \\beta = 2 \\cos \\Big(\\frac{\\alpha + \\beta}{2}\\Big) \\cos \\Big(\\frac{\\alpha - \\beta}{2}\\Big)\n\\]\n\\[\n\\cos \\alpha - \\cos \\beta = -2 \\sin \\Big(\\frac{\\alpha + \\beta}{2}\\Big) \\sin \\Big(\\frac{\\alpha - \\beta}{2}\\Big)\n\\]\n\\[\n\\tan \\alpha \\pm \\tan \\beta = \\frac{sin(\\alpha + \\beta)}{\\cos \\alpha \\cos \\beta}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>Формулы</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Источники",
    "section": "",
    "text": "Shepard, Roger N., and Jacqueline Metzler. 1971. “Mental Rotation\nof Three-Dimensional Objects.” Science 171 (3972):\n701–3. https://doi.org/10.1126/science.171.3972.701.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80. https://doi.org/10.1126/science.103.2684.677.",
    "crumbs": [
      "Источники"
    ]
  }
]