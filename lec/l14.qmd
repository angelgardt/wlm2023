# L14 // Анализ главных компонент и эксплораторный факторный анализ

{{< include ../other/_symbols.qmd >}}

```{r opts, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE)
```

```{r pkgs}
library(tidyverse)
theme_set(theme_bw())
library(ggforce)
```

## Проклятие размерности (curse of dimensionality)

С данными могут случиться две проблемы: либо их слишком мало, либо их слишком много. С первой проблемой многие так или иначе сталкивались --- во-первых, не достигается требуемая статистическая мощность анализа (мало наблюдений), во-вторых, упускаем какие-либо закономерности (мало переменных). А в чем может быть проблема, если данных много?

### Геометрическая вероятность

Возьмем квадрат, в который вписан круг:

```{r geomprob-square-14}
ggplot() +
  geom_rect(aes(xmin = -.5, xmax = .5, ymin = -.5, ymax = .5), color = "black", alpha = 0, linewidth = 1) +
  geom_circle(aes(x0 = 0, y0 = 0, r = .5)) +
  guides(x = "none", y = "none") +
  coord_fixed()
```

Будем бросать в этот квадрат точки случайным образом:

```{r geomprob-dots}
tibble(x = runif(100, -.5, .5),
       y = runif(100, -.5, .5)) %>% 
ggplot() +
  geom_rect(aes(xmin = -.5, xmax = .5, ymin = -.5, ymax = .5), color = "black", alpha = 0, linewidth = 1) +
  geom_circle(aes(x0 = 0, y0 = 0, r = .5)) +
  geom_point(aes(x, y)) +
  guides(x = "none", y = "none") +
  labs(x = "", y = "") +
  coord_fixed()
```

Вопрос: **какова вероятность, что случайно брошенная точка попадет в круг (событие $A$)?**

Мы можем воспользоваться статистическим подходом к вероятности, набросать побольше точек и посчитать, сколько из них попало в пределы круга:

```{r geomprob-incircle}
tibble(x = runif(1000, -.5, .5),
       y = runif(1000, -.5, .5),
       d = sqrt(x^2 + y^2),
       incirc = ifelse(d <= .5, TRUE, FALSE)) %>% 
ggplot() +
  geom_rect(aes(xmin = -.5, xmax = .5, ymin = -.5, ymax = .5), color = "black", alpha = 0, linewidth = 1) +
  geom_circle(aes(x0 = 0, y0 = 0, r = .5)) +
  geom_point(aes(x, y, color = incirc)) +
  guides(x = "none", y = "none", color = "none") +
  labs(x = "", y = "") +
  coord_fixed() +
  scale_color_manual(values = c(`TRUE` = "black", `FALSE` = "gray70"))
```

Из визуализации видно, что в конечном итоге при очень большом количестве бросаний точек они заполнят всю площадь квадрата, а значит, и всю площадь круга, поэтому вероятность попадания случайно прошенной точки в круг равняется отношению площади круга к площади квадрата, то есть:

$$
\prob (A) = \lim_{N \rightarrow \infty} \frac{n}{N} = \frac{S_\text{circle}}{S_\text{square}}
$$

Можно расписать точнее, если вспомнить геометрические формулы:

$$
\prob (A) = \frac{S_\text{circle}}{S_\text{square}} = \frac{\pi r^2}{a^2} = \frac{\pi \big(\frac{1}{2}a\big)^2}{a^2} = \frac{1}{4}\pi \approx 0.785
$$

Окей, это для $\setR^2$, то есть для пространства размерности два (оно же --- плоскость). Теперь рассмотрим ситуацию для $\setR^3$, то есть для пространства размерности три.

$$
\prob (A) = \frac{V_\text{ball}}{V_\text{cube}} = \frac{\frac{4}{3}\pi r^3}{a^3} = \frac{\frac{4}{3}\pi \big(\frac{1}{2}a\big)^3}{a^3} \approx 0.523
$$

В общем случае, для $\setR^k$ объем шара равен:

$$
\begin{split}
k = 2n &, V = \frac{\pi^2}{n!}r^{2n} \\
k = 2n+1 &, V = \frac{2 \cdot (2\pi)^n}{(2n+1)!!} r^{2n+1}
\end{split}
$$

Можно аналитически доказать, что при $k \to \infty : V \to 0$.

***

То есть, говоря более осязаемо, места вне шара становится больше. Набирая переменных, мы делаем расстояния между точками «больше», пространство становится более «разряжённым», и из-за этого объем выборки должен расти экспоненциально, чтобы сохранялась адекватная точность оценки параметров. Это и называется **проклятием размерности**.

Что делать? Снижать размерность.


## Факторный анализ vs анализ главных компонент

Так сложилось, что под термином «факторный анализ» часто смешиваются две вещи: анализ главных компонент и собственно факторный анализ. Кроме того, собственно факторный анализ также подразделяется на эксплораторный --- когда мы ищем факторы --- и конфирматорный --- когда у нас есть гипотеза о структуре факторов, и нам необходимо ее подтвердить.

Анализ главных компонент и факторный анализ различаются в своей математической модели, своей изначальной идеологии, но дают сходные результаты и решают сходные задачи.


## Задачи факторного анализа и анализа главных компонент

- Сокращение числа переменных
- Измерение неизмеримого (построение новых обобзеных показателей)
- Наглядное представление многомерных наблюдений
- Описание структуры взаимных связей между переменными
- Преодоление мультиколлинеарности (в регрессионном анализе)
- Заполнение пропущенных значений (при работе с разряженными матрицами)

и т.д.


## Анализ главных компонент (Principal Component Analysis)

### Математическая модель анализа главных компонент

Рассмотрим случайный вектор (матрицу) $(\vm X_1, \vm X_2, \dots, \vm X_k)$ ($\vm X_i$ --- некоторый столбец [числовых] данных). Мы хотим найти такую линейную комбинацию наших данных, у которой дисперсия максимальна. При это мы хотим домножать матрицу данных на вектор коэффициентов единичной длины, чтобы искусственно не раздувать дисперсию.

Формально,

$$
\begin{split}
&\vm Y_1 = a_{11} \vm X_1 + a_{12} \vm X_2 + \dots + a_{1k} \vm X_k \\
&\var(\vm Y_1) \to \max \\
&\vm a_1 \vm a_1^\top = 1, \, \vm a_1 = 
\begin{pmatrix} a_{11} & a_{12}  & \dots & a_{1k} \end{pmatrix}
\end{split}
$$

Далее, находим вторую линейную комбинацию наших данных с максимальной дисперсией, независимую (некоррелированную) с первой. Требования к вектору коэффициентов оставляем те же.

$$
\begin{split}
&\vm Y_2 = a_{21} \vm X_1 + a_{22} \vm X_2 + \dots + a_{2k} \vm X_k \\
&\var(\vm Y_2) \to \max \\
&\vm a_2 \vm a_2^\top = 1, \, \vm a_2 = 
\begin{pmatrix} a_{21} & a_{22} & \dots & a_{2k} \end{pmatrix} \\
&\cor(\vm Y_2, \vm Y_1) = 0
\end{split}
$$

Далее, находим третью линейную комбинацию, аналогичную предыдущим и некоррелированную с ними.

$$
\begin{split}
&\vm Y_3 = a_{31} \vm X_1 + a_{32} \vm X_2 + \dots + a_{3k} \vm X_k \\
&\var(\vm Y_3) \to \max \\
&\vm a_3 \vm a_3^\top = 1, \, \vm a_3 = 
\begin{pmatrix} a_{31} & a_{32} & \dots & a_{3k} \end{pmatrix} \\
&\cor(\vm Y_3, \vm Y_1) = 0, \, \cor(\vm Y_2, \vm Y_1) = 0
\end{split}
$$

Так как у нас всего $k$ переменных, то мы можем найти $k$ таких линейных комбинаций.

В общем виде, они будут выглядеть следующим образом:

$$
\begin{split}
&\vm Y_k = a_{k1} \vm X_1 + a_{k2} \vm X_2 + \dots + a_{kk} \vm X_k \\
&\var(\vm Y_k) \to \max \\
&\vm a_k \vm a_k^\top = 1, \, \vm a_k = 
\begin{pmatrix} a_{k1} & a_{k2} \dots a_{kk} \end{pmatrix} \\
&\cor(\vm Y_k, \vm Y_i) = 0, \, i = 1, 2, \ldots, k
\end{split}
$$

Полученные $Y_i$ и будут искомые **главные компоненты (principal components)** --- наши новые оси, с помощью которых мы будем смотреть на данные и описывать их.

Если мы в качестве информативности компоненты используем её дисперсию, то можно сказать, что мы ищем наиболее информативные линейные комбинации. То есть, мы ищем некоторую «правильную систему координат».


### Поиск главных компонент геометрически

Чтобы лучше понять, что все это значит, посмотрим на картинку.

<center>
<figure>
<img src="pics/pca-1.jpg">
</figure>
</center>

Рассмотрим самый простой случай. Пусть у нас есть две переменные ($\vm X_1$, $\vm X_2$) и мы подбираем их линейную комбинацию с наибольшей дисперсией. Графически это то же самое, что провести прямую $Y_1$, то есть такую прямую, по которой наши данные будут максимально «растянуты».

<center>
<figure>
<img src="pics/pca-2.jpg">
</figure>
</center>

Далее ищем вторую линейную комбинацию. Прицнип тот же: необходима прямая, по которой данные максимально растянуты. Однако эта прямая должна быть перпендикулярная $Y_1$, чтобы выполнялось условие $\cor (Y_1, Y_2) = 0$ (в двумерном пространстве отсутствие корреляции эквивалентно ортогональности, то есть пересечению прямых под прямым углом).

<center>
<figure>
<img src="pics/pca-3.jpg">
</figure>
</center>

Получаем что-то такое. Красные оси и есть наши главные компоненты. Их столько же, сколько изначально было переменных. Теперь мы можем рассматривать наши наблюдения относительно новой системы координат $Y_1 O^′ Y_2$.


### Сокращение размерности признакового пространства

Итак, мы нашли новые координаты, но что-то переменных у нас меньше не стало --- мы нашли $k$ новых переменных взамен $k$ старых. Мы же хотели, чтобы осей, то есть измерений признакового пространства, стало меньше.

Теперь нам пригодится следующая идея лаваша.

<center>
<figure>
<img src="pics/lavash.jpg">
</figure>
</center>


Лаваш, это, безусловно, трехмерный объект, однако практически вся его «изменчивость» сосредоточена по двум измерениям:

<center>
<figure>
<img src="pics/lavash_dim.jpg">
</figure>
</center>

Толщина лаваша практически «неинформативна» для нас[^lavash], поэтому мы можем игнорировать его изменчивость по третьей оси.

[^lavash]: Хотя это, бесспорно, определяется видом лаваша --- их популяция весьма разнообразна.

Таким образом, чтобы снизить размерность, мы волей, данной нам нами, выбрасываем малоинформативные главные компоненты.


### Информативность компонент

Что же такое малоинформативные компоненты? И как нам решить, какие из компонент конкретного решения малоинформативны?

Вновь рассмотрим картинку. В данном случае мы имеем дело с ситуацией, когда все наблюдения расположены более-менее вокруг одной прямой --- первой главной компоненты $\text{PC1}$:

<center>
<figure>
<img src="pics/pc1.jpg">
</figure>
</center>

Таким образом, изменчивостью по второй главной компоненте $\text{PC2}$ можно пренебречь.

Каждая из главных компонент объясняет какую-то часть дисперсии данных, значит мы можем воспользоваться следующей идеей: **если новая переменная (главная компонента) объясняет меньше дисперсии, чем исходная переменная, то она не информативна.** Дисперсия исходных переменных может быть какой угодно, поэтому перед проведением анализа переменные стандартизируют --- их дисперсия становится равной единице. Это значит, что **если дисперсия главной компоненты меньше единицы, то можно считать её неинформативной**.

Кроме того, есть ещё один способ отбора главных компонент. Можно вычислить, какую долю дисперсии данных объясняет каждая компонента, и упорядочить компоненты по возрастанию доли объясняемой дисперсии. Затем, рассчитав **кумулятивную долю объясняемой дисперсии**, выяснить, какого количества главных компонент достаточно, чтобы объяснить **не менее 80% дисперсии данных**. Это некий конвенциональный порог, поэтому его можно изменять в зависимости от решаемой задачи.

Результаты обычно представляются в виде таблицы:

|                        |  PC1  |  PC2  |  PC3  | ... |  PCk  |
|------------------------|:-----:|:-----:|:-----:|:---:|:-----:|
| Standard Deviation     | 2.214 | 1.501 | 0.622 | ... | 0.003 |
| Proportion of Variance | 0.596 | 0.327 | 0.059 | ... | 0.000 |
| Cumulative Proportion  | 0.596 | 0.923 | 0.982 | ... | 1.000 |


### Интерпретация главных компонент

В принципе, анализ главных компонент может решать чисто техническую задачу сокращения размерности признакового пространства, и нас может и не интересовать содержательная интерпретация новых переменных.

Однако решая аналитические задачи, мы часто заинтересованы в том, чтобы хотя бы как-то понимать, что же мы в итоге получили. Для этого воспользуемся **матрицой нагрузок (matrix of variable loadings)**.

|     |  PC1 |  PC2  |  PC3  | ... |  PCk  |
|-----|:----:|:-----:|:-----:|:---:|:-----:|
| X1  | 0.06 | −0.62 |  0.42 | ... | −0.42 |
| X2  | 0.38 | −0.27 | −0.74 | ... | −0.27 |
| X3  | 0.44 | −0.24 |  0.19 | ... | −0.38 |
| ... |  ... |  ...  |  ...  | ... | −0.05 |
| Xk  | 0.24 |  0.57 | −0.11 | ... | −0.77 |


Нагрузки --- это не что иное как коэффициенты $a_{ij}$ из математической модели. Чем они больше, тем сильнее исходные переменные связаны с главными компонентами. Обобщив связи, можно попробовать придумать адекватную исследовательской области интерпретацию.



## Эксплораторный факторный анализ (Exploratory Factor Analysis)

### Математическая модель эксплораторного факторного анализа

У нас есть все та же матрица наблюдений $X^\top = \pmatrix{X_1 & X_2 & \ldots X_k}$, только мы ее транспонировали для будущего удобства. Мы предполагаем, что под нашими наблюдениями спрятаны некие **факторы** в количестве $p$ штук, $p < k$. Их мы также можем оформить в матрицу $F^\top = \pmatrix{F_1 & F_2 & \ldots & F_p}$. Эти факторы объясняют имеющиеся переменные. Делают это они следующим образом:

$$
\begin{split}
&\vm X_i = a_{i1} \vm F_1 + a_{i2} \vm F_2 + \dots + a_{ip} \vm F_p + \vm U_i, \, i = 1, 2, \ldots, k \\
& \vm X = \vm A \vm F + \vm U, \\
& \vm A = (a_{ij}), \, i = 1,2,\ldots,k, \, j = 1,2,\dots,p \\
& \vm U^\top = \pmatrix{\vm U_1 & \vm U_2 & \dots & \vm U_k}
\end{split}
$$

Здесь $\vm U$ --- то, что не удалось объяснить факторами (**остатки, уникальность, uniqueness**).

Также выдвигаем ряд дополнительных предположения для формализации требований и упрощения вычислений:

1. $\expect \vm X = 0$.
    * Так сделать можно, так как мы все равно будем стандартизировать переменные, и математическое ожидание обратиться в ноль.
2. $\cor (\vm F_j, \vm F_t) = 0, \, \forall j \forall t, \, j \neq t, j = 1,2,\ldots,p, \, t = 1,2,\ldots,p$.
    * То есть хотим, чтобы факторы были независимы (некоррелированы).
3. $\var (\vm F) = \vm I$.
    * Если «истинная» дисперсия факторов будет отличаться от единицы, то разница уйдет в матрицу $\vm A$.
4. $\cor (\vm U_i, \vm U_r) = 0, \, \cor (\vm U_i, \vm F_j) = 0, \, \forall i \forall r \forall j, i \neq r, \, i = 1,2,\ldots,k, \, r = 1,2,\ldots,k, \, j = 1,2,\ldots,p$.
    * Ну, они же все-таки уникальности.

Элементы матрицы $\vm A$ называются **факторными нагрузками (factor loadings)**. Элементы вектора $\vm U$ называются **уникальными факторами (specific variates)**.

Теперь об информативности. Аналогично PCA, возьмем в качестве меры информативности дисперсию. Мы хотим узнать, насколько хорошо факторы объясняют исходные переменные. Дисперсия переменных будет складываться из следующего:

$$
\var \vm X_i = \sum_{j=1}^p a^2_{ij} + \var \vm U_i
$$

То есть, чем больше **уникальность (uniqueness)** --- часть дисперсии переменной, объясненной уникальными факторами --- тем хуже наши факторы объясняют переменную. Что делать? Либо подобрать другую модель (изменить количество факторов), либо не брать такую переменную в факторный анализ…

Однако в факторном анализе есть еще одна интересная деталь. Мы работаем с выборочными корреляциями. По этой причине мы не можем подобрать единственно верное решение задачи факторного анализа. Матрица факторных нагрузок определена с точностью до ортогонального преобразования (по-русски, вращения). То есть у нас есть множество решений, которые отличаются друг от друга поворотом, и мы можем выбирать самое «симпатишное».

> За такую неопределенность факторный анализа часто критикуют. Да и вообще, как вы могли заметить, в факторном анализе субъективность чуть ли не на каждом шагу.


### Вращение факторов (factor rotation)


Вращать вообще-то не обязательно. Если хочется, чтобы все было похоже на PCA, то можно ввести некоторое дополнительное условие и получить единственное решение.

Однако вращение факторов позволяет получить иногда более интересные результаты. Самые используемые вращения таковы:

* **varimax** --- находит наиболее «контрастное» решение, еще более «изолируя» друг от друга факторы
* **quartimax** --- минимизирует количество факторов, необходимых для объяснения каждой переменной
* **equamax** --- количество переменных, сильно нагружающих фактор, и количество факторов для объяснения переменных минимальны
* **promax** --- наклонное вращение, позволяющее коррелировать факторы
* **oblimin** --- косоугольное вращение, позволяющее коррелировать факторы
